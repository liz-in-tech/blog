<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.0" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.13" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="alternate" hreflang="en-us" href="https://liz-in-tech.github.io/blog/posts/llm/026_distribution_and_parallelism_2.html"><meta property="og:url" content="https://liz-in-tech.github.io/blog/zh/posts/llm/026_distribution_and_parallelism_2.html"><meta property="og:site_name" content="Liz"><meta property="og:title" content="分布式训练之三：数据并行 Data Parallelism"><meta property="og:description" content="分布式训练之三：数据并行 Data Parallelism"><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:locale:alternate" content="en-US"><meta property="og:updated_time" content="2025-03-30T13:07:08.000Z"><meta property="article:author" content="Liz"><meta property="article:tag" content="分布式"><meta property="article:tag" content="并行"><meta property="article:published_time" content="2025-03-02T00:00:00.000Z"><meta property="article:modified_time" content="2025-03-30T13:07:08.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"分布式训练之三：数据并行 Data Parallelism","image":[""],"datePublished":"2025-03-02T00:00:00.000Z","dateModified":"2025-03-30T13:07:08.000Z","author":[{"@type":"Person","name":"Liz","url":"https://github.com/liz-in-tech"}]}</script><link rel="icon" herf="/blogger.png"><link rel="icon" href="/blog/blogger.png"><title>分布式训练之三：数据并行 Data Parallelism | Liz</title><meta name="description" content="分布式训练之三：数据并行 Data Parallelism">
    <link rel="preload" href="/blog/assets/style-m_obra2h.css" as="style"><link rel="stylesheet" href="/blog/assets/style-m_obra2h.css">
    <link rel="modulepreload" href="/blog/assets/app-v5bcDFbF.js"><link rel="modulepreload" href="/blog/assets/026_distribution_and_parallelism_2.html-lTYQr6p_.js"><link rel="modulepreload" href="/blog/assets/026_distribution_and_parallelism_2.html-99kVKQLr.js"><link rel="modulepreload" href="/blog/assets/026_zero3_3-skr0K_0Q.js"><link rel="modulepreload" href="/blog/assets/024_mixed_precision_training_list-dG4PWcKt.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-x3n3nnut.js">
    <link rel="prefetch" href="/blog/assets/index.html-YbPtte5_.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-CGfhr1vY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FUMOuem4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--TTjrkIy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g4Nfr7z1.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-bitGHKd2.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-nrisQopy.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-9XtwFAwc.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-oji9upQP.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-xrin91s2.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-RcwJewgQ.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-c628DmZb.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-4RcS3hxb.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-_8stdYVV.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-eluz3bTT.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-Ft0RQWf3.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-htZNHy9b.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-Q27DlfZz.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-6CxOIR84.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-GIu-oGwK.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html-Gj-zyjT6.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-xa8M4Wu4.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html-dm178NRn.js" as="script"><link rel="prefetch" href="/blog/assets/018_huggingface.html-WI0c55vB.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-kLkdC4dl.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-P-Llr3CJ.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-0d0frUhq.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-RhkTd_Zr.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-5MCDM_Sd.js" as="script"><link rel="prefetch" href="/blog/assets/024_distribution_and_parallelism.html-01hi9eD-.js" as="script"><link rel="prefetch" href="/blog/assets/025_distribution_and_parallelism_1.html-32IzAErP.js" as="script"><link rel="prefetch" href="/blog/assets/026_distribution_and_parallelism_2.html-rsWgIQO4.js" as="script"><link rel="prefetch" href="/blog/assets/027_distribution_and_parallelism_3.html-aP_ykF_V.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-Kub1JEXd.js" as="script"><link rel="prefetch" href="/blog/assets/029_unsloth_grpo.html-zgQtWVj-.js" as="script"><link rel="prefetch" href="/blog/assets/030_wandb.html-QwJ87Q-u.js" as="script"><link rel="prefetch" href="/blog/assets/031_qlora.html-JOYVOncP.js" as="script"><link rel="prefetch" href="/blog/assets/032_sft_trainer_sourcecode_prepare_model.html-6NUsa_LT.js" as="script"><link rel="prefetch" href="/blog/assets/033_sft_trainer_sourcecode_prepare_dataset.html-znW51XOY.js" as="script"><link rel="prefetch" href="/blog/assets/034_sft_trainer_sourcecode_prepare_trainer.html-jLWECXBF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZDCSnlc1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZYw6WxxA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OavE9BET.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-hE_T0u_5.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-iH0mq6XB.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-WyFhRqF6.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-wy9Toayl.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-Igb8VTzY.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-WHPR-17-.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-hz6Q-DAA.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-3KMCwhrh.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-7gxKMp_3.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-KndRvZAj.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-DtFi92ig.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-ZdCqy2Fu.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-W-CdR_ck.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-mimwqA-t.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html--JjBpVvQ.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-hiqs-a6X.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html-bS1SRdf4.js" as="script"><link rel="prefetch" href="/blog/assets/018_huggingface.html-Si3T7PB7.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-V5tcXCC4.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-ytaIU5xV.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-kvlLpO3f.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-nS8_ZZy5.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-VZsN2kO8.js" as="script"><link rel="prefetch" href="/blog/assets/024_distribution_and_parallelism.html-oCd9cE9Q.js" as="script"><link rel="prefetch" href="/blog/assets/025_distribution_and_parallelism_1.html-ZR9kuXUl.js" as="script"><link rel="prefetch" href="/blog/assets/027_distribution_and_parallelism_3.html-j2B_wJ9U.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-38Gbieji.js" as="script"><link rel="prefetch" href="/blog/assets/029_unsloth_grpo.html-MLBLxBKQ.js" as="script"><link rel="prefetch" href="/blog/assets/030_wandb.html-nzAhc8i2.js" as="script"><link rel="prefetch" href="/blog/assets/031_qlora.html-vUUfqpgA.js" as="script"><link rel="prefetch" href="/blog/assets/032_sft_trainer_sourcecode_prepare_model.html-TUbRRh-O.js" as="script"><link rel="prefetch" href="/blog/assets/033_sft_trainer_sourcecode_prepare_dataset.html-RGXXyFkO.js" as="script"><link rel="prefetch" href="/blog/assets/034_sft_trainer_sourcecode_prepare_trainer.html-h3f0tH03.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wh_dBtOR.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-cxLWDy2T.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kf4JCRaf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-RkA-insV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4kI_oqSd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-WhuidxNt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OqGkeUA_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5GeN-sdD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-c4Rf4yh1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2r0jUs7o.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BSKRXRQc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-KjTsJ0Hg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TteIwMx3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g00XXzrL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-YxbJgo4L.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-3T79Cy0i.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-A5tlQHan.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-04ff5e0O.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-vgZ6rfFh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OmipPplE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-E1KrJL6a.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oPH9QkTj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cHRqZSs8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-74SU9ZTn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--iJiA8oX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oZPWb_Fc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FUIWSLsp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5ERWyusD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Zjn0JNqd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jcvPTrgB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9t2TsyuQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CcLVFNIv.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DVoYOOaL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-V52ipvRm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9If_KW0o.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-df9Mrf2R.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-f9bWoKcO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GYH0QUoo.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-JVTfeijx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_C1QVNqX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-L_IXFmna.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nZWHmXY7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-x4gPgqE4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bWnVyuyA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--yTU23ka.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-tRnpyzfw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7aypos-L.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-1BXcbV1R.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-pu478WKz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DFi1VhMA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7uaSljkV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UeJoj0gS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BULhD4SJ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-i7RIA9-O.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cVdKTxAl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-LHukpLS7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-howjHe2f.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DeN_iOWx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cnlzR0a7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0P_c_pcU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-dy_CcFmq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FzFytZ_p.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2KSwV7xp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-F1coElwg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ffflqCb9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-F8ZuLYgH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-HeYWaFeL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xKYeJEc5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xB-iS7Ql.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OEUPqfTV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-EE4iQI9m.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g1PUF_BG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nUBcs85a.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9Y3la5Sf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wufIFDPM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bXZQIxRE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wd6ZkEHi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Xlk0AXmC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Uv7c7pYa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FfZqC9tZ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mSPhZxqB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-51HoKD5A.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Q5_K6Vux.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jkWPo860.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kTEqch5G.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-y4iBqBqc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ibHhI9SI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-3hk_s27_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-QJ9j3Zl2.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-QvWFHL99.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Pefl6i_g.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-dNHrcQzx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-tITHrBRq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-l-UCqoWK.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-r_IMIQdZ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FFF0RGO4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jWI8KaSx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ErByaVwg.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-i6L7adZK.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-EcsmugCa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-XWhg2wST.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hGblY0m5.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-XwiYvbIK.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-GeYfHopu.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-paWNKBSc.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-T76zUd_T.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-bac1SKe4.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-xSapr9te.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-1r_2Xc4o.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-JiAq1EGf.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-f4WQ58r6.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-Ku6AJafC.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-J76tsHEg.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-oEvrlY2o.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html--WmmOiOZ.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-jbc_FMd7.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-jYryOp3z.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html-hOu-S7SI.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-k16i4rfI.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html-JQVQkktq.js" as="script"><link rel="prefetch" href="/blog/assets/018_huggingface.html-jzjkw5cs.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-YYh65X8S.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-Oo-EF862.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-5ZIO5m03.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-92Y9XPXc.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-YUwLuN7l.js" as="script"><link rel="prefetch" href="/blog/assets/024_distribution_and_parallelism.html-0udqPsy-.js" as="script"><link rel="prefetch" href="/blog/assets/025_distribution_and_parallelism_1.html-wjtZF3Ey.js" as="script"><link rel="prefetch" href="/blog/assets/026_distribution_and_parallelism_2.html-EgYFIN8X.js" as="script"><link rel="prefetch" href="/blog/assets/027_distribution_and_parallelism_3.html-TDLmZOfN.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-m_lCatvv.js" as="script"><link rel="prefetch" href="/blog/assets/029_unsloth_grpo.html-mdMZZDiZ.js" as="script"><link rel="prefetch" href="/blog/assets/030_wandb.html-sXLsNyui.js" as="script"><link rel="prefetch" href="/blog/assets/031_qlora.html-zrhw_Nz5.js" as="script"><link rel="prefetch" href="/blog/assets/032_sft_trainer_sourcecode_prepare_model.html-00NJoJyb.js" as="script"><link rel="prefetch" href="/blog/assets/033_sft_trainer_sourcecode_prepare_dataset.html-V4JDlJWM.js" as="script"><link rel="prefetch" href="/blog/assets/034_sft_trainer_sourcecode_prepare_trainer.html-xtKFO81R.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-U65HNMax.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-h_iLVcMr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-gAzgX_Y8.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-qVgYj_8n.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-ahmHFu1l.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-IO54_tgV.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-rsSoIlKH.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-E4b3xtmI.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-Io7DUBa5.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-XXFflABF.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-JqVc-n7x.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-3uX6AwKv.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-zRnfxmsd.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-Q1iY5Uj4.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-I9nefwrz.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-KLB-VG4U.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-nZVtnUPv.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html-PCT5rGE0.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-YYSHpU0u.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html-1MyeLcEX.js" as="script"><link rel="prefetch" href="/blog/assets/018_huggingface.html-NVIzrEOC.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-B6gcA5-w.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-AzEW6-fH.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-dqqv9B9q.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-4YSLyRj2.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-q6oiIFur.js" as="script"><link rel="prefetch" href="/blog/assets/024_distribution_and_parallelism.html-YUJUICFH.js" as="script"><link rel="prefetch" href="/blog/assets/025_distribution_and_parallelism_1.html-rqJW4LIr.js" as="script"><link rel="prefetch" href="/blog/assets/027_distribution_and_parallelism_3.html-i3xmpf9k.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-rSvt4BiT.js" as="script"><link rel="prefetch" href="/blog/assets/029_unsloth_grpo.html-p3AHWjev.js" as="script"><link rel="prefetch" href="/blog/assets/030_wandb.html-P113Ufas.js" as="script"><link rel="prefetch" href="/blog/assets/031_qlora.html-5nko2JaU.js" as="script"><link rel="prefetch" href="/blog/assets/032_sft_trainer_sourcecode_prepare_model.html-yjZC4_RP.js" as="script"><link rel="prefetch" href="/blog/assets/033_sft_trainer_sourcecode_prepare_dataset.html-gxAJDbCu.js" as="script"><link rel="prefetch" href="/blog/assets/034_sft_trainer_sourcecode_prepare_trainer.html-Ned8InaI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-dszj4xZ6.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-SvzvhyaF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9JsptYhF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Ezwlo9JL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-KCAfw_8o.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-3rdHsD_R.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-b2xowSWT.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rPjzPsy3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-d4q5_OjH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rgZXamH3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9D5rz3jx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kr5ysTxR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FjgJHG03.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-v7_KHgMD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-LbrL1opU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TIxjCoIX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TSL0K1Y3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-pey5puwk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2zirtd8_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SW20Oz7Z.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ubN7mM4P.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xVNPfcwn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-l5Dz77VI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TsaR-7n2.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-16Mnz8fy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-vj-OAkvH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-srO5XcGP.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-uuKDebeZ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-otdJ4BTm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4XauGXIh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-s33vMsvv.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-vM_Wa4eu.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-KxvqrxFy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-HoK77Wcm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-6e79wm8i.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DfPGKvx-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-v0_52JLV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OX9qZZJz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-X9seX8Dg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hBdp6RMr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-L7lhNgy-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4iq44NwH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-S0gd-d9z.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-O5DoB959.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--6YLCYUA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Ta8TJrWI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Pd5d5VDQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--UnxRw8M.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Vv7JBMJo.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UEWUR72h.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-13A1owT0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-sv-OzP_G.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-m0t0LOra.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-fbbYlZeb.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-lAnyfZ9Q.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TCfEa3Gl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PpzSDfDV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0rXCichf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-fO6izxrB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-J0zpKUS0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-3UjYX_62.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-qno38Psk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ULjnqer6.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FnMfu-6u.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-poyw1RqG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bxNiZfsg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-iyY6UMUk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UvplFO08.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Sm7wQcUk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4sm21bru.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GBXoWu1D.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rNiSyx6S.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bi9HQHU5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-E_EqbmVz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-IgefXqAi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-pvfHxO2Y.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TADG-VKz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9eBAyAZt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jrWuH7qI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9Q0K7ju_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--jlC3EcV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-KwAzqYf3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SYcjPjF4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-VPgbnH_9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-83w8yP9j.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-JZsf5MBH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-gqv1i1Hl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-r6kseor5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-KnV8X9Yk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-sxJ0BMlW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7Zdy8I7q.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-i-XTqsEU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xx5skesc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-XzDalOSj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-yfPYpr9E.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PuMxUPBL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Z-QcTam5.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-08_zHRDQ.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><!--[--><div class="theme-container no-sidebar has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/blog/zh/"><img class="vp-nav-logo" src="/blog/blogger.png" alt><!----><span class="vp-site-name hide-in-pad">Liz</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="首页" class="vp-link nav-link nav-link" href="/blog/zh/"><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span>首页<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="项目" class="vp-link nav-link nav-link" href="/blog/zh/demo/"><span class="font-icon icon fa-fw fa-sm fas fa-star" style=""></span>项目<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><div class="nav-item"><div class="dropdown-wrapper i18n-dropdown"><button type="button" class="dropdown-title" aria-label="选择语言"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="i18n icon" style="width:1rem;height:1rem;vertical-align:middle;"><path d="M379.392 460.8 494.08 575.488l-42.496 102.4L307.2 532.48 138.24 701.44l-71.68-72.704L234.496 460.8l-45.056-45.056c-27.136-27.136-51.2-66.56-66.56-108.544h112.64c7.68 14.336 16.896 27.136 26.112 35.84l45.568 46.08 45.056-45.056C382.976 312.32 409.6 247.808 409.6 204.8H0V102.4h256V0h102.4v102.4h256v102.4H512c0 70.144-37.888 161.28-87.04 210.944L378.88 460.8zM576 870.4 512 1024H409.6l256-614.4H768l256 614.4H921.6l-64-153.6H576zM618.496 768h196.608L716.8 532.48 618.496 768z"></path></svg><!--]--><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="English" class="vp-link nav-link nav-link" href="/blog/posts/llm/026_distribution_and_parallelism_2.html"><!---->English<!----></a></li><li class="dropdown-item"><a aria-label="简体中文" class="vp-link nav-link active nav-link active" href="/blog/zh/posts/llm/026_distribution_and_parallelism_2.html"><!---->简体中文<!----></a></li></ul></button></div></div><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/liz-in-tech" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" placeholder="搜索" autocomplete="off" spellcheck="false" value><!----></form><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>分布式训练之三：数据并行 Data Parallelism</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/liz-in-tech" target="_blank" rel="noopener noreferrer">Liz</a></span><span property="author" content="Liz"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2025-03-02T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 12 分钟</span><meta property="timeRequired" content="PT12M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category6 clickable" role="navigation">LLM</span><!--]--><meta property="articleSection" content="LLM"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag4 clickable" role="navigation">分布式</span><span class="page-tag-item tag3 clickable" role="navigation">并行</span><!--]--><meta property="keywords" content="分布式,并行"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_1-dp概览">1. DP概览</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_2-data-parallelism-dp">2. Data Parallelism (DP)</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_2-1-朴素dp-naive-ddp-approach">2.1. 朴素DP（naive DDP approach）</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_2-2-dp优化">2.2. DP优化</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_2-3-db实践">2.3. DB实践</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_2-4-采用dp的表现">2.4. 采用DP的表现</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_3-deepspeed-zero">3. DeepSpeed ZeRO</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-1-zero">3.1. ZeRO</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-2-混合精度训练回顾">3.2. 混合精度训练回顾</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-3-zero-1">3.3. ZeRO-1</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-4-zero-2">3.4. ZeRO-2</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-5-zero-3">3.5. ZeRO-3</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_4-扩展链接">4. 扩展链接</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_5-代码实现">5. 代码实现</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-1-naive-dp-implementation-with-overlap-in-picotron">5.1. Naive DP implementation with overlap in Picotron</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-2-bucket-dp-implementation-in-picotron">5.2. Bucket DP implementation in Picotron</a></li><!----><!--]--></ul></li><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h1 id="分布式训练之三-数据并行-data-parallelism" tabindex="-1"><a class="header-anchor" href="#分布式训练之三-数据并行-data-parallelism" aria-hidden="true">#</a> 分布式训练之三：数据并行 Data Parallelism</h1><!-- more --><h2 id="_1-dp概览" tabindex="-1"><a class="header-anchor" href="#_1-dp概览" aria-hidden="true">#</a> 1. DP概览</h2><p>Data parallelism (DP)</p><ul><li>朴素DP（naive DDP approach）</li><li>DP优化一：为每个参数附加一个all-reduce的钩子函数</li><li>DP优化二：分桶 Bucketing gradients</li><li>ZeRO (Zero Redundancy Optimizer) <ul><li>ZeRO-1: optimizer state partitioning</li><li>ZeRO-2: optimizer state + gradient partitioning</li><li>ZeRO-3 / FSDP (Fully-Sharded Data Parallelism): optimizer state + gradient + parameter partitioning</li></ul></li></ul><h2 id="_2-data-parallelism-dp" tabindex="-1"><a class="header-anchor" href="#_2-data-parallelism-dp" aria-hidden="true">#</a> 2. Data Parallelism (DP)</h2><p>不同微批次在不同的GPU上并行处理（micro batch上只进行梯度计算，每个global batch更新一次模型参数）</p><ul><li>每个GPU上复制一份模型实例</li><li>多个GPU并行计算微批次的前向传播和反向传播，然后计算出每个微批次的梯度</li><li>将微批次的梯度进行all-reduce操作求平均值</li><li>优化器用平均梯度更新模型参数</li></ul><figure><img src="/blog/assets/026_dp-EXtwUza9.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>涉及到的分布式通信 distributed communication</p><ul><li>all-reduce：此处用于将不同GPU上计算的梯度求平均值，并把每个GPU上的梯度都更新为平均梯度，从而进行梯度同步</li></ul><p>梯度计算与梯度同步</p><ul><li>梯度计算（GPU计算）：前向传播和反向传播的梯度计算</li><li>梯度同步（GPU通信）：触发分布式通信all-reduce操作进行梯度同步</li></ul><h3 id="_2-1-朴素dp-naive-ddp-approach" tabindex="-1"><a class="header-anchor" href="#_2-1-朴素dp-naive-ddp-approach" aria-hidden="true">#</a> 2.1. 朴素DP（naive DDP approach）</h3><p>原理：GPU计算完成后进行GPU通信，再等GPU通信完成后进行GPU计算</p><ul><li>等待每个GPU计算梯度完成后，触发一次分布式通信all-reduce操作进行梯度同步，然后等待梯度同步完成后，优化器进行更新参数</li></ul><figure><img src="/blog/assets/026_naive_dp-E_HNnfVP.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>改进空间：在通信进行时，GPU处于空闲状态，而我们应该尽量让通信和计算重叠，尽可能使它们同时进行</p><h3 id="_2-2-dp优化" tabindex="-1"><a class="header-anchor" href="#_2-2-dp优化" aria-hidden="true">#</a> 2.2. DP优化</h3><p>将梯度计算的反向传播和梯度同步重叠</p><ul><li>DP优化一：多个GPU上的某个参数都计算出梯度了，就开始对这个参数的梯度进行all-reduce操作</li><li>DP优化二：多个GPU上的某个层都计算出梯度了，就开始对这个层的梯度进行all-reduce操作</li></ul><h4 id="_2-2-1-dp优化一-为每个参数附加一个all-reduce的钩子函数" tabindex="-1"><a class="header-anchor" href="#_2-2-1-dp优化一-为每个参数附加一个all-reduce的钩子函数" aria-hidden="true">#</a> 2.2.1. DP优化一：为每个参数附加一个all-reduce的钩子函数</h4><p>原理：不等所有层的反向传播都计算完成了才开始梯度同步，而是在计算出部分层的梯度后，就开始同步这些计算出的层的梯度同步，从而显著加速数据并行，减少等待整个模型梯度同步的时间（例如:llama在计算出第32层的梯度后,GPU还在计算第31层的梯度时，就开始同步第32层的梯度）</p><p>Note: 优化一甚至不等一个解码层的梯度完全计算完成才开始梯度同步，而是只要每个参数在各个GPU上计算出梯度就开始同步这个参数的梯度</p><figure><img src="/blog/assets/026_dp_opt1-c6SNtqfy.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>PyTorch实现：为每个参数附加一个all-reduce的钩子函数，一旦某个参数的梯度准备就绪，就会立即触发all-reduce操作，而此时其他参数的梯度可能仍在计算中</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">register_backward_hook</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hook<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Registers a backward hook for all parameters of the model that 
    require gradients.
    &quot;&quot;&quot;</span>
    <span class="token keyword">for</span> p <span class="token keyword">in</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> p<span class="token punctuation">.</span>requires_grad <span class="token keyword">is</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
            p<span class="token punctuation">.</span>register_post_accumulate_grad_hook<span class="token punctuation">(</span>hook<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-2-2-dp优化二-分桶-bucketing-gradients" tabindex="-1"><a class="header-anchor" href="#_2-2-2-dp优化二-分桶-bucketing-gradients" aria-hidden="true">#</a> 2.2.2. DP优化二：分桶 Bucketing gradients</h4><p>前置理论：GPU 操作在处理大张量时通常比对许多小张量执行多次操作更高效。这一点对通信操作同样适用。</p><p>原理：将梯度分桶（例如，按层分桶，每个解码层作为一个整体），对同一桶的所有梯度启动一次all-reduce操作，而不是对每个参数的梯度单独执行all-reduce操作。通过对每个桶执行一次all-reduce操作，我们可以显著减少通信开销并加速通信过程。</p><figure><img src="/blog/assets/026_dp_opt2-iYKI8SGn.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_2-3-db实践" tabindex="-1"><a class="header-anchor" href="#_2-3-db实践" aria-hidden="true">#</a> 2.3. DB实践</h3><p>术语</p><ul><li>bs: batch size 批次大小</li><li>mbs: micro batch size 微批次大小</li><li>gbs: global batch size 全局批次大小</li><li>grad_acc: the number of gradient accumulation step 梯度累积步骤数 (指在没法并行的情况下，在一个GPU上连续串行计算几个mbs，把串行计算的这几个累积起来)</li><li>dp: the number of parallel instances used for data parallelism 数据并行的并行实例数</li><li>gbst: global batch size tokens 全局批次大小Token数</li></ul><p>公式</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>bs = gbs = mbs * grad_acc * dp
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>gbst = batch_size * sequence_length 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>数据并行初步方案</p><ul><li>1.确定最佳的全局批次大小Token数 global batch size tokens (gbst) <ul><li>通过查阅文献或运行实验（测量模型收敛性）来确定</li></ul></li><li>2.选择训练的序列长度 <ul><li>同样可以通过查阅文献或实验来决定</li><li>通常，2-8k 个 token 对于我们今天的评估来说效果可靠</li></ul></li><li>3.找到最大本地批次大小 micro batch size (mbs) <ul><li>可以通过不断增加单 GPU 上的本地批次大小（mbs），直到内存耗尽，来找到最大本地批次大小</li></ul></li><li>4.确定目标数据并行（DP）可用的GPU数量 <ul><li>gbs/dp 的值将告诉我们，为了达到所需的 gbs，还需要多少梯度累积步骤。</li></ul></li></ul><p>具体案例</p><ul><li>gbst（token数）= 4M tokens</li><li>sequence_length=4K tokens</li><li>bs=1024 samples</li><li>observe assume: a single GPU can only fit mbs=2 in memory (也就是单个GPU单位时间只能容纳序列长度为4k tokens的2个样本), mbs需要小于等于2</li><li>we have 128 GPUs available for training</li><li>This means with 4 gradient accumulation steps we’ll achieve our goal of 1024 samples or 4M tokens per training step. <ul><li>grad_acc = bs / (dp * mbs) = 1024 / (128 * 2) = 4</li></ul></li><li>Now what if we suddenly have 512 GPUs available? We can achieve the same GBS and thus identical training by keeping MBS=2 and setting gradient accumulation steps to 1 and achieve faster training! <ul><li>grad_acc = bs / (dp * mbs) = 1024 / (512 * 2) = 1</li></ul></li></ul><h3 id="_2-4-采用dp的表现" tabindex="-1"><a class="header-anchor" href="#_2-4-采用dp的表现" aria-hidden="true">#</a> 2.4. 采用DP的表现</h3><figure><img src="/blog/assets/026_throughput_and_memory-SnE7PVSE.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>随着扩充DP并行数量</p><ul><li>吞吐量(throughput)下降</li><li>内存占用保持稳定</li></ul><p>采用这个的前提是mbs至少是1，也就是一个GPU至少能支持一个输入样本的前向传播，但这个不总是成立，甚至采用了激活值重新计算的情况下</p><figure><img src="/blog/assets/026_memory_usage-pj4jdpOl.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Tip: 快速目测模型参数所需的最小内存：模型参数 * 2（e.g. 70B → 140GB (=133GiB)）</p><p>对于超大的模型或较大的批次token数，我们还有其他可选项吗？</p><ul><li>有两种主要的拆分方法：并行性 parallelism（张量并行、上下文并行或流水线并行）和分片 sharding（DeepSpeed Zero 或 PyTorch FSDP）。这两种方法在某种程度上是相互独立的，实际上可以结合使用！</li></ul><h2 id="_3-deepspeed-zero" tabindex="-1"><a class="header-anchor" href="#_3-deepspeed-zero" aria-hidden="true">#</a> 3. DeepSpeed ZeRO</h2><h3 id="_3-1-zero" tabindex="-1"><a class="header-anchor" href="#_3-1-zero" aria-hidden="true">#</a> 3.1. ZeRO</h3><p>DeepSpeed ZeRO官方文档：<a href="https://www.deepspeed.ai/tutorials/zero/" target="_blank" rel="noopener noreferrer">https://www.deepspeed.ai/tutorials/zero/<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>ZeRO</p><ul><li>(<strong>Ze</strong>ro <strong>R</strong>edundancy <strong>O</strong>ptimizer)</li><li>一种内存优化技术，旨在减少LLM训练中的内存冗余</li><li>ZeRO通过在<strong>数据并行DP维度</strong>上将optimizer states, gradients, and parameters进行分片来减少内存冗余，这有时需要更多GPU通信，这些通信也许能和GPU计算重叠也许不能</li><li>当我们提到“分区”时，是指沿着 DP 轴进行分区，因为 ZeRO 是数据并行的一部分</li></ul><p>ZeRO的三种优化策略</p><ul><li>ZeRO-1: optimizer state partitioning</li><li>ZeRO-2: optimizer state + gradient partitioning</li><li>ZeRO-3 / FSDP (also called FSDP for “Fully-Sharded Data Parallelism”): optimizer state + gradient + parameter partitioning</li></ul><figure><img src="/blog/assets/026_zero-6AWHVtA1.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Note：k=12 for Adam （混合精度训练情况下：全精度的模型参数+全精度的2个Adam优化器参数）</p><p>Note: 激活值activations怎么不进行分片？ 因为每个DP接收不同的微批次micro-batch，其计算的激活值自然也不同，它本身就没有冗余，本来就不是共享或重复的数据，所以不需要同步，不需要分片，并且已经被用于计算DP rank各自的梯度，它的主要任务就完成了，后面已经用不到了</p><p>DP不同策略的内存占用：</p><figure><img src="/blog/assets/026_zero_memory_usage-fGTznOTt.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>ZeRO的限制：</p><p>数据并行（DP）只有在模型的单层能够适应单个 GPU 的情况下才能正常工作，而 ZeRO 只能对参数、梯度和优化器状态进行分区，无法对激活值内存进行分区！我们从之前的激活值内存讨论中回忆，激活值内存的这一部分会随着序列长度和批次大小（batch size）而线性增加。自然地，我们可以通过限制序列长度和批次大小来应对，但实际上，我们不希望因硬件限制而只能使用较短的序列长度进行训练。</p><h3 id="_3-2-混合精度训练回顾" tabindex="-1"><a class="header-anchor" href="#_3-2-混合精度训练回顾" aria-hidden="true">#</a> 3.2. 混合精度训练回顾</h3><p>混合精度训练的已知方法汇总：</p><figure><img src="/blog/assets/024_mixed_precision_training_list-zWnwhm6Z.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>参数量 num_parameters : Ψ</p><ul><li>BF16+FP32混合精度baseline：2Ψ + 6Ψ + 12Ψ = 20Ψ <ul><li>模型参数（半精度）：2 bytes</li><li>梯度（半精度） + FP32的梯度（以FP32的精度进行梯度累积）：2 + 4 = 6 bytes</li><li>FP32的模型参数和优化器状态：4 + （4 + 4）= 12 bytes</li></ul></li><li>去掉FP32梯度的BF16+FP32混合精度：2Ψ + 2Ψ + 12Ψ = 16Ψ <ul><li>模型参数（半精度）：2 bytes</li><li>梯度（半精度）：2 bytes</li><li>FP32的模型参数和优化器状态：4 + （4 + 4）= 12 bytes</li></ul></li></ul><h3 id="_3-3-zero-1" tabindex="-1"><a class="header-anchor" href="#_3-3-zero-1" aria-hidden="true">#</a> 3.3. ZeRO-1</h3><figure><img src="/blog/assets/026_zero1_1-chKLWwve.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/blog/assets/026_zero1_2-nDtu1GKB.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>单个训练步的执行过程</p><ul><li>前向传播：每个副本持相同且完整的模型参数（BF16），但处理不同的微批次micro_batch（尽管参数相同，之后计算出的激活值和梯度会因微批次不同而不同）</li><li>反向传播：每个副本计算出完整的梯度，但由于微批次不同，每个副本的梯度也不同</li><li>对梯度进行reduce-scatter操作：每个副本只把优化器状态分片对应的部分梯度进行累积，其他部分的梯度保持不变</li><li>本地优化：每个副本在其本地优化器状态上执行更新参数的步骤，只更新优化器状态分片对应的部分参数（从 FP32 格式更新后转换回 BF16），其他参数保持不变</li><li>对参数进行all-gather操作：将每个副本更新的参数（BF16）部分进行收集，使每个副本都有完整的更新后的参数</li></ul><p>和朴素DP相比</p><ul><li>梯度累积从朴素DP的all-reduce操作转变为reduce-scatter操作（reduce-scatter is 2 times faster than all-reduce! ）</li><li>在优化器步骤后添加了all-gather操作</li></ul><p>ZeRO的进一步优化: 将all-gather与前后操作重叠</p><ul><li>all-gather重叠优化器步骤：优化器更新部分参数后，就开启all-gather操作</li><li>all-gather重叠前向传播：all-gather执行完一层的参数收集，就开启这层的前向传播</li></ul><h3 id="_3-4-zero-2" tabindex="-1"><a class="header-anchor" href="#_3-4-zero-2" aria-hidden="true">#</a> 3.4. ZeRO-2</h3><figure><img src="/blog/assets/026_zero2_1-ltGxlP93.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/blog/assets/026_zero2_2-jKSycbBf.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_3-5-zero-3" tabindex="-1"><a class="header-anchor" href="#_3-5-zero-3" aria-hidden="true">#</a> 3.5. ZeRO-3</h3><p>ZeRO-3在PyTorch的实现中叫FSDP(Fully Shared Data Parallelism)</p><figure><img src="/blog/assets/026_zero3_1-Cnk6_7Bf.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/blog/assets/026_zero3_2-P1IgyB-r.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/blog/assets/026_zero3_3-kj-z2o8V.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>只有在模型参数用到的时候按需收集它们</p><ul><li>每一层前向传播之前采用all-gather操作收集所有参数，每一层前向传播之后，将不需要的参数从内存中刷掉（从第1层到第32层进行32次）</li><li>每一层反向传播之前采用all-gather操作收集所有参数，每一层反向传播计算出梯度之后，将不需要的参数从内存中刷掉（从第32层到第1层进行32次）</li><li>优化：第32层的前向传播和反向传播合在一起，在第32层前向传播之前采用all-gather操作收集所有参数，在第32层反向传播计算出梯度之后，将不需要的参数从内存中刷掉</li><li>所以执行all-gather操作和将不需要的参数从内存刷掉的操作总共执行 num_layers + num_layers - 1 = 32 + 32 - 1 = 63 次，如第3个图所示有一点延迟开销</li><li>ZeRO-3重度依赖参数通信</li></ul><h2 id="_4-扩展链接" tabindex="-1"><a class="header-anchor" href="#_4-扩展链接" aria-hidden="true">#</a> 4. 扩展链接</h2><p><a href="https://siboehm.com/articles/22/data-parallel-training" target="_blank" rel="noopener noreferrer">https://siboehm.com/articles/22/data-parallel-training<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><a href="https://www.harmdevries.com/post/context-length/" target="_blank" rel="noopener noreferrer">https://www.harmdevries.com/post/context-length/<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><h2 id="_5-代码实现" tabindex="-1"><a class="header-anchor" href="#_5-代码实现" aria-hidden="true">#</a> 5. 代码实现</h2><h3 id="_5-1-naive-dp-implementation-with-overlap-in-picotron" tabindex="-1"><a class="header-anchor" href="#_5-1-naive-dp-implementation-with-overlap-in-picotron" aria-hidden="true">#</a> 5.1. Naive DP implementation with overlap in Picotron</h3><p>完成代码</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">DataParallelNaive</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Naive Data Parallelism. Not used in practice. But it is a good starting point to understand how data parallelism works.
    It implements a simple all-reduce operation to synchronize gradients across multiple processes.
    And `no_sync` context manager to disable gradient synchronization.
    &quot;&quot;&quot;</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> module<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Initializes the DataParallel wrapper for a given module.

        Args:
            module (nn.Module): The model to be wrapped for data parallelism.
            process_group (torch.distributed.ProcessGroup): The process group used for gradient synchronization. 
                                                            It could be a data parallel or context parallel group.
        &quot;&quot;&quot;</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>module <span class="token operator">=</span> module
        self<span class="token punctuation">.</span>require_backward_grad_sync <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token comment"># whether to synchronize gradients during backward pass. Set to False when using gradient accumulation</span>
        self<span class="token punctuation">.</span>register_backward_hook<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_allreduce_grads<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>module<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">register_backward_hook</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hook<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Registers a backward hook for all parameters of the model that require gradients.    
        &quot;&quot;&quot;</span>
        <span class="token keyword">for</span> p <span class="token keyword">in</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> p<span class="token punctuation">.</span>requires_grad <span class="token keyword">is</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
                p<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span>hook<span class="token punctuation">)</span>
                
    <span class="token keyword">def</span> <span class="token function">_allreduce_grads</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> grad<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Performs an all-reduce operation to synchronize gradients across multiple processes.    
        &quot;&quot;&quot;</span>
        <span class="token comment"># No synchronization needed during gradient accumulation, except at the final accumulation step.</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>require_backward_grad_sync<span class="token punctuation">:</span>
            dist<span class="token punctuation">.</span>all_reduce<span class="token punctuation">(</span>grad<span class="token punctuation">,</span> op<span class="token operator">=</span>dist<span class="token punctuation">.</span>ReduceOp<span class="token punctuation">.</span>SUM<span class="token punctuation">,</span> group<span class="token operator">=</span>pgm<span class="token punctuation">.</span>process_group_manager<span class="token punctuation">.</span>cp_dp_group<span class="token punctuation">)</span>
            grad <span class="token operator">/=</span> pgm<span class="token punctuation">.</span>process_group_manager<span class="token punctuation">.</span>cp_dp_world_size
        <span class="token keyword">return</span> grad 
    
    <span class="token decorator annotation punctuation">@contextlib<span class="token punctuation">.</span>contextmanager</span>
    <span class="token keyword">def</span> <span class="token function">no_sync</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        A context manager to temporarily disable gradient synchronization. 
        This is useful for performing multiple backward passes during gradient accumulation without synchronizing 
        gradients in between.
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>require_backward_grad_sync <span class="token operator">=</span> <span class="token boolean">False</span>
        <span class="token keyword">yield</span>
        self<span class="token punctuation">.</span>require_backward_grad_sync <span class="token operator">=</span> <span class="token boolean">True</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_5-2-bucket-dp-implementation-in-picotron" tabindex="-1"><a class="header-anchor" href="#_5-2-bucket-dp-implementation-in-picotron" aria-hidden="true">#</a> 5.2. Bucket DP implementation in Picotron</h3><p>完整代码</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">DataParallelBucket</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Data Parallelism with gradient grouped into buckets to reduce the communication overhead.
    &quot;&quot;&quot;</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> module<span class="token punctuation">,</span> bucket_cap_mb<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">,</span> grad_type <span class="token operator">=</span> torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Initialize the DataParallelBucket module.
        
        Args:
            module (nn.Module): The model to be parallelized.
            process_group: The process group for gradient synchronization, which can be either 
                           a data parallel group or a context parallel group.
            bucket_cap_mb (int, optional): The maximum size of each gradient synchronization bucket in megabytes. 
                                           Defaults to 25 MB.
            grad_type (torch.dtype, optional): The data type of gradients, defaulting to float32.
        &quot;&quot;&quot;</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>module <span class="token operator">=</span> module
        self<span class="token punctuation">.</span>require_backward_grad_sync <span class="token operator">=</span> <span class="token boolean">True</span> <span class="token comment"># whether to synchronize gradients during backward pass. Set to False when using gradient accumulation</span>
        grad_size <span class="token operator">=</span> <span class="token number">2</span> <span class="token keyword">if</span> grad_type <span class="token operator">==</span> torch<span class="token punctuation">.</span>bfloat16 <span class="token keyword">else</span> <span class="token number">4</span> <span class="token comment"># float32 gradient: 4 bytes</span>
        bucket_size <span class="token operator">=</span> bucket_cap_mb <span class="token operator">*</span> <span class="token number">1024</span> <span class="token operator">*</span> <span class="token number">1024</span> <span class="token operator">//</span> grad_size <span class="token comment"># number of gradients in one bucket</span>
        self<span class="token punctuation">.</span>bucket_manager <span class="token operator">=</span> BucketManager<span class="token punctuation">(</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> pgm<span class="token punctuation">.</span>process_group_manager<span class="token punctuation">.</span>cp_dp_group<span class="token punctuation">,</span> bucket_size<span class="token punctuation">,</span> grad_type<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_backward_hook<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_post_backward_callback_set <span class="token operator">=</span> <span class="token boolean">False</span> <span class="token comment"># whether the callback for wait gradient synchronization is set</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>module<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_tensor<span class="token punctuation">,</span> output_tensor<span class="token punctuation">,</span> output_tensor_grad<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> output_tensor<span class="token punctuation">,</span> output_tensor_grad<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">register_backward_hook</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Registers a backward hook to manually accumulate and synchronize gradients.
        
        This hook serves two main purposes:
        1. PyTorch does not natively support gradient accumulation with mixed precision.
        2. After gradient accumulation, it flags parameters as ready for synchronization.
        
        The gradient accumulation functions are stored to prevent them from going out of scope.
        
        References:
        - https://github.com/NVIDIA/Megatron-LM/issues/690
        - https://pytorch.org/docs/stable/generated/torch.autograd.graph.Node.register_hook.html
        - https://arxiv.org/abs/2006.15704 (page 5)
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>grad_accs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad<span class="token punctuation">:</span>
                <span class="token comment"># Expand so we get access to grad_fn.</span>
                param_tmp <span class="token operator">=</span> param<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>param<span class="token punctuation">)</span>
                <span class="token comment"># Get the gradient accumulator function.</span>
                grad_acc_fn <span class="token operator">=</span> param_tmp<span class="token punctuation">.</span>grad_fn<span class="token punctuation">.</span>next_functions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
                grad_acc_fn<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_make_param_hook<span class="token punctuation">(</span>param<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bucket_manager<span class="token punctuation">)</span><span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>grad_accs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>grad_acc_fn<span class="token punctuation">)</span>
                
    <span class="token keyword">def</span> <span class="token function">_make_param_hook</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> param<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">,</span>bucket_manager<span class="token punctuation">:</span> BucketManager<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Creates the a hook for each parameter to handle gradient accumulation and synchronization.
        &quot;&quot;&quot;</span>
        <span class="token keyword">def</span> <span class="token function">param_hook</span><span class="token punctuation">(</span><span class="token operator">*</span>unused<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token triple-quoted-string string">&quot;&quot;&quot;
            The hook called after the gradient is ready. It performs the following:
            1. Accumulates the gradient into the main gradient.
            2. Adds a post-backward callback to wait for gradient synchronization completion.
            3. Marks the parameter as ready for synchronization.
            &quot;&quot;&quot;</span>
            <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad<span class="token punctuation">:</span>
                <span class="token keyword">assert</span> param<span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
                param<span class="token punctuation">.</span>main_grad<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>param<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">)</span> <span class="token comment"># accumulate the gradients</span>
                param<span class="token punctuation">.</span>grad <span class="token operator">=</span> <span class="token boolean">None</span>
                
                <span class="token comment"># skip the gradient synchronization (gradient accumulation/PP micro batches)</span>
                <span class="token keyword">if</span> self<span class="token punctuation">.</span>require_backward_grad_sync<span class="token punctuation">:</span>
                    <span class="token comment"># Add a callback to wait for gradient synchronization. Ensures the callback is added only once.</span>
                    <span class="token comment"># Callback is executed after the backward pass. It should be added per backward pass.</span>
                    <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>_post_backward_callback_set<span class="token punctuation">:</span>
                        Variable<span class="token punctuation">.</span>_execution_engine<span class="token punctuation">.</span>queue_callback<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_post_backward<span class="token punctuation">)</span>
                        self<span class="token punctuation">.</span>_post_backward_callback_set <span class="token operator">=</span> <span class="token boolean">True</span>
                        
                    <span class="token comment"># mark the parameter as ready for gradient synchronization. </span>
                    bucket_manager<span class="token punctuation">.</span>mark_param_as_ready<span class="token punctuation">(</span>param<span class="token punctuation">)</span> 
        <span class="token keyword">return</span> param_hook
    
    <span class="token decorator annotation punctuation">@contextlib<span class="token punctuation">.</span>contextmanager</span>
    <span class="token keyword">def</span> <span class="token function">no_sync</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;A context manager to disable gradient synchronization.&quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>require_backward_grad_sync <span class="token operator">=</span> <span class="token boolean">False</span>
        <span class="token keyword">yield</span>
        self<span class="token punctuation">.</span>require_backward_grad_sync <span class="token operator">=</span> <span class="token boolean">True</span>
        
    <span class="token keyword">def</span> <span class="token function">_post_backward</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        A post-backward callback that waits for gradient synchronization to finish, then copies 
        the synchronized gradients back to the parameters&#39; grad attribute.
        
        This method is called after the backward pass and before the optimizer step.
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>bucket_manager<span class="token punctuation">.</span>wait<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_post_backward_callback_set <span class="token operator">=</span> <span class="token boolean">False</span>
        <span class="token comment"># copy to params.grad so we can use the optimizer to update the parameters</span>
        <span class="token keyword">for</span> p <span class="token keyword">in</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> p<span class="token punctuation">.</span>requires_grad<span class="token punctuation">:</span>
                p<span class="token punctuation">.</span>grad <span class="token operator">=</span> p<span class="token punctuation">.</span>main_grad<span class="token punctuation">.</span>to<span class="token punctuation">(</span>p<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span> <span class="token comment"># In PyTorch, you cannot assign a gradient with one data type to a tensor of another data type.</span>

    <span class="token keyword">def</span> <span class="token function">reset</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Reset the bucket manager and zero out gradients in the model
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>bucket_manager<span class="token punctuation">.</span>reset<span class="token punctuation">(</span><span class="token punctuation">)</span> 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><!--[--><!----><!--]--><footer class="page-meta"><!----><div class="meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a aria-label="分布式训练之四：并行策略" class="vp-link nav-link prev nav-link prev" href="/blog/zh/posts/llm/027_distribution_and_parallelism_3.html"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>分布式训练之四：并行策略</div></a><a aria-label="分布式训练之二：并行编程 Parallel Programming" class="vp-link nav-link next nav-link next" href="/blog/zh/posts/llm/025_distribution_and_parallelism_1.html"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">分布式训练之二：并行编程 Parallel Programming<span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span></div></a></nav><!----><!--[--><!----><!--]--><!--]--></main><!--]--><!----></div><!--]--><!--]--><!----><!--]--></div>
    <script type="module" src="/blog/assets/app-v5bcDFbF.js" defer></script>
  </body>
</html>
