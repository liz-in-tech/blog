<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.0" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.13" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="alternate" hreflang="zh-cn" href="https://liz-in-tech.github.io/blog/zh/posts/llm/024_distribution_and_parallelism.html"><meta property="og:url" content="https://liz-in-tech.github.io/blog/posts/llm/024_distribution_and_parallelism.html"><meta property="og:site_name" content="Liz"><meta property="og:title" content="Distributed Training Part 1: Memory Usage in Model Training"><meta property="og:description" content="Distributed Training Part 1: Memory Usage in Model Training"><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:locale:alternate" content="zh-CN"><meta property="og:updated_time" content="2025-03-08T14:32:06.000Z"><meta property="article:author" content="Liz"><meta property="article:tag" content="Distributed"><meta property="article:tag" content="Parallel"><meta property="article:published_time" content="2025-02-26T00:00:00.000Z"><meta property="article:modified_time" content="2025-03-08T14:32:06.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Distributed Training Part 1: Memory Usage in Model Training","image":[""],"datePublished":"2025-02-26T00:00:00.000Z","dateModified":"2025-03-08T14:32:06.000Z","author":[{"@type":"Person","name":"Liz","url":"https://github.com/liz-in-tech"}]}</script><link rel="icon" herf="/blogger.png"><link rel="icon" href="/blog/blogger.png"><title>Distributed Training Part 1: Memory Usage in Model Training | Liz</title><meta name="description" content="Distributed Training Part 1: Memory Usage in Model Training">
    <link rel="preload" href="/blog/assets/style-m_obra2h.css" as="style"><link rel="stylesheet" href="/blog/assets/style-m_obra2h.css">
    <link rel="modulepreload" href="/blog/assets/app-dpf1czaz.js"><link rel="modulepreload" href="/blog/assets/024_distribution_and_parallelism.html-QSMoIwEI.js"><link rel="modulepreload" href="/blog/assets/024_scaling_experiments1-sDd0n9MZ.js"><link rel="modulepreload" href="/blog/assets/024_mixed_precision_training_list-dG4PWcKt.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-x3n3nnut.js"><link rel="modulepreload" href="/blog/assets/024_distribution_and_parallelism.html-uCefT9T7.js">
    <link rel="prefetch" href="/blog/assets/index.html-YbPtte5_.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-CGfhr1vY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FUMOuem4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--TTjrkIy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g4Nfr7z1.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-bitGHKd2.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-nrisQopy.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-9XtwFAwc.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-oji9upQP.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-xrin91s2.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-q7LEGqjL.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-c628DmZb.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-4RcS3hxb.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-_8stdYVV.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-eluz3bTT.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-Ft0RQWf3.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-CImz0KSx.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-1CqW55t5.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-6CxOIR84.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-l_kCYUI1.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html-JrbwEhT3.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-XtD4OMNh.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html-VmsqVzZn.js" as="script"><link rel="prefetch" href="/blog/assets/018_autorag.html-azoAgFGI.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-8h4F60kB.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-61m3-6IM.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-0d0frUhq.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-FazlUPBT.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-5MCDM_Sd.js" as="script"><link rel="prefetch" href="/blog/assets/025_distribution_and_parallelism_1.html-32IzAErP.js" as="script"><link rel="prefetch" href="/blog/assets/026_distribution_and_parallelism_2.html-MEZ1JY2b.js" as="script"><link rel="prefetch" href="/blog/assets/027_distribution_and_parallelism_3.html-IYByZhbZ.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-Kub1JEXd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZDCSnlc1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZYw6WxxA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OavE9BET.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-hE_T0u_5.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-iH0mq6XB.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-WyFhRqF6.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-YaT0PR6o.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-x2qtCXhJ.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-WHPR-17-.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-hz6Q-DAA.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-3KMCwhrh.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-7gxKMp_3.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-KndRvZAj.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-Akmj-Ub_.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-bXz0YlpJ.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-W-CdR_ck.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-aq7YkQ-T.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html-V8hUrR0F.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-DCPUUxWe.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html--6PmCjyW.js" as="script"><link rel="prefetch" href="/blog/assets/018_autorag.html-QupCTUgH.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-6hLqoXkg.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-qFyQHNwt.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-kvlLpO3f.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-AiRwjg9s.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-VZsN2kO8.js" as="script"><link rel="prefetch" href="/blog/assets/024_distribution_and_parallelism.html-MO4kg9bC.js" as="script"><link rel="prefetch" href="/blog/assets/025_distribution_and_parallelism_1.html-ZR9kuXUl.js" as="script"><link rel="prefetch" href="/blog/assets/026_distribution_and_parallelism_2.html-GkjDPoeT.js" as="script"><link rel="prefetch" href="/blog/assets/027_distribution_and_parallelism_3.html-5kFJGreK.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-38Gbieji.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wh_dBtOR.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-cxLWDy2T.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kf4JCRaf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-RkA-insV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4kI_oqSd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-WhuidxNt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OqGkeUA_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5GeN-sdD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-c4Rf4yh1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2r0jUs7o.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BSKRXRQc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-KjTsJ0Hg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TteIwMx3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g00XXzrL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-YxbJgo4L.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-3T79Cy0i.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-A5tlQHan.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-04ff5e0O.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-vgZ6rfFh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OmipPplE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-E1KrJL6a.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oPH9QkTj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cHRqZSs8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-74SU9ZTn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--iJiA8oX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oZPWb_Fc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FUIWSLsp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5ERWyusD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Zjn0JNqd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jcvPTrgB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9t2TsyuQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CcLVFNIv.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DVoYOOaL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-V52ipvRm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9If_KW0o.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kUk1kAMU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GYH0QUoo.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-JVTfeijx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_C1QVNqX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-L_IXFmna.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nZWHmXY7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-x4gPgqE4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bWnVyuyA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-LHukpLS7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-howjHe2f.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DeN_iOWx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cnlzR0a7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0P_c_pcU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-dy_CcFmq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FzFytZ_p.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2KSwV7xp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-F1coElwg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ffflqCb9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-F8ZuLYgH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-HeYWaFeL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xKYeJEc5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xB-iS7Ql.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OEUPqfTV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-EE4iQI9m.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g1PUF_BG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nUBcs85a.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9Y3la5Sf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wufIFDPM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bXZQIxRE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wd6ZkEHi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Xlk0AXmC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bGhIpy0C.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mSPhZxqB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-51HoKD5A.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Q5_K6Vux.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jkWPo860.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kTEqch5G.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-y4iBqBqc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mDjwyff2.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-rlPv-U40.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Xozbilpx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rQSkUpmB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5mHlpcIA.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-MDqcApPy.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-svn8VaJE.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-t1ZKIaFi.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-svNDcpfY.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-ibPgsQe-.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-12Wld-D8.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-gg6HeTG1.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-_9L9rGol.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-afDMONW6.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-OYo1zSVw.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-eukxPDTz.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-5ahBfiJs.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-h7f4lbQ3.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-iRQW1OIW.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-1KF03aF2.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html-O9tsguRd.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-z6HAoRZE.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html-IfCt6cQN.js" as="script"><link rel="prefetch" href="/blog/assets/018_autorag.html-QHkLO_Uc.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-A8rSneDc.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-lewZvDbY.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-pDAOS03B.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-5gR9Z1Ha.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-nVD6mEoW.js" as="script"><link rel="prefetch" href="/blog/assets/025_distribution_and_parallelism_1.html-_9D7bYHB.js" as="script"><link rel="prefetch" href="/blog/assets/026_distribution_and_parallelism_2.html-pqRrl5fs.js" as="script"><link rel="prefetch" href="/blog/assets/027_distribution_and_parallelism_3.html-wKs_50DK.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-yEsL0eWX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-LspvVCg2.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7RypPZGx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--XgiAiTK.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-KLXLK5Z1.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html--JWFw4f5.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-XYAeXNBe.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-tlV0s4Tp.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html--4ncz0Yn.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-jeKMYERG.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-McEa8nYA.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-t6Z5Tyoq.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-SFq9dk7k.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-4nGBR1tH.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-n8KhvvmX.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html--vhenbJs.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-eJIzxi-r.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-0_z0N4cn.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html-asxEThTF.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-v8Tf8NbJ.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html-hKis9eJ-.js" as="script"><link rel="prefetch" href="/blog/assets/018_autorag.html-w3pHD7fm.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-a_fmLOJF.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-Z9QN3I5k.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-lO9Yktq1.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-zD3oU5qx.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-6npfcKl0.js" as="script"><link rel="prefetch" href="/blog/assets/024_distribution_and_parallelism.html-cjWEF7OA.js" as="script"><link rel="prefetch" href="/blog/assets/025_distribution_and_parallelism_1.html-z5CSb0LW.js" as="script"><link rel="prefetch" href="/blog/assets/026_distribution_and_parallelism_2.html-BtwN9BTM.js" as="script"><link rel="prefetch" href="/blog/assets/027_distribution_and_parallelism_3.html-7_Y86UGw.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-oOKqU8GM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SUPOf1zR.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-GyHXtMAi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-is8-d0Ed.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-zjp-8UWR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-6XBW8Ov3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ReszkuNd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-I1KAZoJS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-tGnEaYkR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9JOjCKuG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-p096FpRm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--QWlevSp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-KXM4afpz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-64vzsvy1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CNPm0ezS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-iXzmtIjD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-w8OoKm6o.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Fiq11jHe.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Z7h9D0Jf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oREYj-a-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-budu_xBX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-If0V6QyO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-RWY55qw7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UtL__lw1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jvBihG4-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ExwAseF_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kaGWF7Cx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-paG2kzYt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--Llj_cs6.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9IPCc-JI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-gKNfe4Gs.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PBrmCCvX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-VMt1Fqv4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wnICfMXi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OOQkvOmw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oTXANWTI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PSeY-y79.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-j2ygMYLh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_V7D3_v_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-KjISYaQI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jmjguAy1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-erodNSeh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2TE--uqe.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-V9gGDnkt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Biwg39Wn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UweT067v.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-piHHnfzq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2E9LwQB9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-co_ZGbxH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-IU3fDVGW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FEpI73em.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PdvgQbUn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C1kxVAHO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wS8WdvrO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-XVQ3rLg7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-WtPaqBmM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-VIOkao96.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-LY77Ty5E.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-uJnLyt6M.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-V_28uAsN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-fsr-C8y0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-am3MbFaJ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UF_ZR-BQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jdjsnYbZ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xM6KW1wy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xMx166s4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5bylOvyD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Mv5ChhRw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OGmfB8EP.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Gl7UPVqn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Im-IDi1W.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nlTIfczW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-RlYU_OE5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xLNxKPlX.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-08_zHRDQ.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><!--[--><div class="theme-container no-sidebar has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/blog/"><img class="vp-nav-logo" src="/blog/blogger.png" alt><!----><span class="vp-site-name hide-in-pad">Liz</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="Home" class="vp-link nav-link nav-link" href="/blog/"><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span>Home<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Project" class="vp-link nav-link nav-link" href="/blog/demo/"><span class="font-icon icon fa-fw fa-sm fas fa-star" style=""></span>Project<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><div class="nav-item"><div class="dropdown-wrapper i18n-dropdown"><button type="button" class="dropdown-title" aria-label="Select language"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="i18n icon" style="width:1rem;height:1rem;vertical-align:middle;"><path d="M379.392 460.8 494.08 575.488l-42.496 102.4L307.2 532.48 138.24 701.44l-71.68-72.704L234.496 460.8l-45.056-45.056c-27.136-27.136-51.2-66.56-66.56-108.544h112.64c7.68 14.336 16.896 27.136 26.112 35.84l45.568 46.08 45.056-45.056C382.976 312.32 409.6 247.808 409.6 204.8H0V102.4h256V0h102.4v102.4h256v102.4H512c0 70.144-37.888 161.28-87.04 210.944L378.88 460.8zM576 870.4 512 1024H409.6l256-614.4H768l256 614.4H921.6l-64-153.6H576zM618.496 768h196.608L716.8 532.48 618.496 768z"></path></svg><!--]--><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="English" class="vp-link nav-link active nav-link active" href="/blog/posts/llm/024_distribution_and_parallelism.html"><!---->English<!----></a></li><li class="dropdown-item"><a aria-label="简体中文" class="vp-link nav-link nav-link" href="/blog/zh/posts/llm/024_distribution_and_parallelism.html"><!---->简体中文<!----></a></li></ul></button></div></div><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/liz-in-tech" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Distributed Training Part 1: Memory Usage in Model Training</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/liz-in-tech" target="_blank" rel="noopener noreferrer">Liz</a></span><span property="author" content="Liz"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2025-02-26T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 9 min</span><meta property="timeRequired" content="PT9M"></span><span class="page-category-info" aria-label="Category🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category6 clickable" role="navigation">LLM</span><!--]--><meta property="articleSection" content="LLM"></span><span class="page-tag-info" aria-label="Tag🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag3 clickable" role="navigation">Distributed</span><span class="page-tag-item tag2 clickable" role="navigation">Parallel</span><!--]--><meta property="keywords" content="Distributed,Parallel"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_1-metrics">1. Metrics</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_2-three-key-challenges">2. Three Key Challenges</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_3-basics-of-model-training">3. Basics of Model Training</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-1-model-training-process">3.1. Model Training Process</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-2-important-hyperparameter-batch-size">3.2. Important Hyperparameter -- Batch Size</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_4-memory-usage-in-model-training">4. Memory Usage in Model Training</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_5-mixed-precision-training">5. Mixed Precision Training</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-1-numerical-range-and-precision-of-floating-point-numbers">5.1. Numerical Range and Precision of Floating-Point Numbers</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-2-concept-of-mixed-precision-training">5.2. Concept of Mixed Precision Training</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-3-summary-of-known-methods-for-mixed-precision-training">5.3. Summary of Known Methods for Mixed Precision Training</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-4-fp16-and-bf16-training">5.4. FP16 and BF16 Training</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-5-fp8-training">5.5. FP8 Training</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_6-activation-recomputation-gradient-checkpointing-rematerialization">6. Activation Recomputation / Gradient Checkpointing / Rematerialization</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_7-gradient-accumulation">7. Gradient Accumulation</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_8-tools">8. Tools</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_8-1-memory-usage-calculation-tool-predict-memory">8.1. Memory Usage Calculation Tool: Predict Memory</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_8-2-distributed-training-tool-for-visualizing-gpu-compute-and-communication-costs-profiler">8.2. Distributed Training Tool for Visualizing GPU Compute and Communication Costs: Profiler</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_9-reference-ultrascale-playbook">9. Reference: Ultrascale Playbook</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_9-1-overview">9.1. Overview</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_9-2-prerequisite-knowledge">9.2. Prerequisite Knowledge</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_9-3-scaling-experiments">9.3. Scaling Experiments</a></li><!----><!--]--></ul></li><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h1 id="distributed-training-part-1-memory-usage-in-model-training" tabindex="-1"><a class="header-anchor" href="#distributed-training-part-1-memory-usage-in-model-training" aria-hidden="true">#</a> Distributed Training Part 1: Memory Usage in Model Training</h1><!-- more --><h2 id="_1-metrics" tabindex="-1"><a class="header-anchor" href="#_1-metrics" aria-hidden="true">#</a> 1. Metrics</h2><p>Objective: Fully utilize the expensive hardware of GPUs</p><ul><li>Throughput</li><li>GPU Utilization</li><li>Training Time</li></ul><h2 id="_2-three-key-challenges" tabindex="-1"><a class="header-anchor" href="#_2-three-key-challenges" aria-hidden="true">#</a> 2. Three Key Challenges</h2><ul><li>Memory Usage <ul><li>It&#39;s a hard limitation - if a training step doesn&#39;t fit in memory, training cannot proceed</li><li>Out-of-Memory (OOM) issues</li></ul></li><li>Compute Efficiency <ul><li>We want our hardware to spend most time computing, so we need to reduce time spent on data transfers or waiting for other GPUs to perform work.</li></ul></li><li>Communication Overhead <ul><li>We want to minimize communication overhead as it keeps GPUs idle. To achieve this, we will try to make the best use of intra-node (fast) and inter-node (slower) bandwidths as well as overlap communication with compute as much as possible.</li></ul></li></ul><h2 id="_3-basics-of-model-training" tabindex="-1"><a class="header-anchor" href="#_3-basics-of-model-training" aria-hidden="true">#</a> 3. Basics of Model Training</h2><h3 id="_3-1-model-training-process" tabindex="-1"><a class="header-anchor" href="#_3-1-model-training-process" aria-hidden="true">#</a> 3.1. Model Training Process</h3><p>Model training consists of three steps:</p><ul><li>Forward Pass: Inputs are passed to the model to produce outputs</li><li>Backward Pass: Gradients are computed</li><li>Optimization Step: The optimizer uses gradients to update model parameters</li></ul><figure><img src="/blog/assets/024_model_train_process-h1blFZuC.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_3-2-important-hyperparameter-batch-size" tabindex="-1"><a class="header-anchor" href="#_3-2-important-hyperparameter-batch-size" aria-hidden="true">#</a> 3.2. Important Hyperparameter -- Batch Size</h3><h4 id="_3-2-1-batch-size-bs" tabindex="-1"><a class="header-anchor" href="#_3-2-1-batch-size-bs" aria-hidden="true">#</a> 3.2.1. Batch Size (bs)</h4><p>Affects both model convergence and throughput</p><p>A small batch size can be useful early in training to quickly move along the training landscape reaching an optimal learning point. However, further along the model training, small batch sizes will keep gradients noisy and the model may not be able to converge to the most optimal final performances. At the other extreme, a large batch size while giving very accurate gradient estimations will tend to make less use of each training token rendering convergence slower and potentially wasting compute.</p><p>Batch size also affects the time it takes to train on a given text dataset: a small batch size will require more optimizer steps to train on the same amount of samples. Optimizer steps are costly (in compute time) and the total time to train will thus increase compared to using a larger batch size. This being said, note that the batch size can often be adjusted quite largely around the optimal batch size without major impact to the performance of the model, i.e. the sensitivity of final model performances to the exact batch size value is usually rather low around the optimal batch size.</p><p>Batch size extension links:</p><ul><li>OpenAI Paper: https://arxiv.org/pdf/1812.06162</li><li>MiniMax-01 Paper: https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf</li></ul><h4 id="_3-2-2-batch-size-tokens-bst" tabindex="-1"><a class="header-anchor" href="#_3-2-2-batch-size-tokens-bst" aria-hidden="true">#</a> 3.2.2. Batch Size Tokens (bst)</h4><p>In the field of LLM pre-training, batch size is often reported in terms of tokens rather than the number of samples (bst = Batch Size Tokens). This approach makes the amount of training data independent of the specific input sequence length used during training.</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>bst = bs * seq
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>where seq is the model input sequence length</p><ul><li>Ideal Batch Size: 4-60 million tokens</li><li>Llama 1: For 1.4 trillion tokens, batch size is about 4 million tokens</li><li>DeepSeek: For 14 trillion tokens, batch size is about 60 million tokens</li></ul><p>A sweet spot for recent LLM training is typically on the order of 4-60 million tokens per batch. The batch size as well as the training corpus have been steadily increasing over the years: Llama 1 was trained with a batch size of ~4M tokens for 1.4 trillion tokens while DeepSeek was trained with a batch size of ~60M tokens for 14 trillion tokens.</p><h2 id="_4-memory-usage-in-model-training" tabindex="-1"><a class="header-anchor" href="#_4-memory-usage-in-model-training" aria-hidden="true">#</a> 4. Memory Usage in Model Training</h2><p>Four components:</p><ul><li>Model Parameters (weights &amp; Biases) <ul><li>Purpose: Determine the model&#39;s performance; training the model involves updating these parameters</li><li>Memory Usage: Determined by the number of parameters, each being a floating-point number, depending on the precision used</li><li>Variation: Fixed during training</li></ul></li><li>Optimizer States <ul><li>Purpose: Assist in parameter updates to minimize the loss function</li><li>Memory Usage: Different optimizers occupy different amounts of memory. Many optimizers (like Adam, RMSprop) store additional states for each parameter (like momentum, squared gradients). For example, the Adam optimizer stores two additional variables for each parameter: momentum and the average of squared gradients. This means each parameter occupies additional memory, usually twice its parameter memory usage.</li><li>Variation: Fixed during training</li></ul></li><li>Activations <ul><li>Purpose: Outputs of each layer during the forward pass, used to compute gradients during the backward pass</li><li>Memory Usage: Related to batch size, sequence length, and model architecture. Typically large, especially in deep networks or large batch training.</li><li>Variation: Stored after each forward pass and used during the backward pass, varies with batch size and input data, dynamically changes during training</li></ul></li><li>Gradients <ul><li>Purpose: Used in the backward pass to compute the direction and magnitude of parameter updates</li><li>Memory Usage: During the backward pass, a gradient matrix of the same dimension is stored for each parameter (each parameter corresponds to a gradient), occupying memory equal to the size of model parameters</li><li>Variation: Dynamically changes during training, computed and stored during the backward pass, released after parameter updates</li></ul></li></ul><p>These four components are stored as tensors with different shapes and precisions.</p><p>Hyperparameters determining shapes:</p><ul><li>Batch size</li><li>Sequence length</li><li>Model hidden dimensions</li><li>Attention heads</li><li>Vocabulary size</li><li>Model sharding</li></ul><p>Common precisions:</p><ul><li>FP32 (full precision) -&gt; 4 bytes</li><li>BF16 -&gt; 2 bytes</li><li>FP8 -&gt; 1 byte</li></ul><p>During training, memory usage is continuously changing rather than static.</p><ul><li>Initialization <ul><li>Initialize model parameters (neural network models usually randomly initialize weights, with some methods like Xavier initialization, He initialization helping to avoid gradient vanishing or explosion issues)</li></ul></li><li>Iterative Loop <ul><li>Forward Pass <ul><li>Compute model outputs, store activations (hidden_state)</li></ul></li><li>Compute Loss <ul><li>Compute loss value (usually a scalar, small memory usage)</li></ul></li><li>Backward Pass <ul><li>Compute and store gradients</li></ul></li><li>Parameter Update <ul><li>Optimizer updates model parameters based on gradients and maintains optimizer states</li></ul></li></ul></li></ul><figure><img src="/blog/assets/024_memory_profile-l_mjFy4T.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Memory Optimization Suggestions:</p><ul><li>Activation Recomputation: Reduce memory usage of activations by storing only part of them and recomputing when needed to save memory</li><li>Gradient Accumulation: Accumulate gradients from multiple small batches before updating parameters to reduce memory usage</li><li>Mixed Precision Training: Reduce memory usage of parameters and gradients by using half-precision floating-point numbers (float16) instead of single-precision (float32)</li><li>Distributed Training: Distribute memory pressure by spreading the model or data across multiple GPUs to reduce memory usage on a single GPU</li><li>Reduce Batch Size: Reducing batch size decreases memory usage but may affect training speed</li><li>Reduce Model Size: Use smaller models or model compression techniques</li></ul><p>Extension Link: https://zdevito.github.io/2022/08/04/cuda-caching-allocator.html</p><h2 id="_5-mixed-precision-training" tabindex="-1"><a class="header-anchor" href="#_5-mixed-precision-training" aria-hidden="true">#</a> 5. Mixed Precision Training</h2><h3 id="_5-1-numerical-range-and-precision-of-floating-point-numbers" tabindex="-1"><a class="header-anchor" href="#_5-1-numerical-range-and-precision-of-floating-point-numbers" aria-hidden="true">#</a> 5.1. Numerical Range and Precision of Floating-Point Numbers</h3><p>Default numerical precision of PyTorch tensors: single-precision floating-point format, also known as FP32 or float32</p><ul><li>This means each stored number occupies 32 bits (i.e., 4 bytes)</li></ul><p>The available bits are divided into three parts to represent a number (scientific notation):</p><ul><li>Sign Bit: The first bit determines whether the number is positive or negative</li><li>Exponent: Controls the range of the number</li><li>Mantissa: Determines the significant digits of the number</li></ul><figure><img src="/blog/assets/024_float_point-6dd9mggT.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>List of floating-point formats provided by PyTorch:</p><ul><li>FP32 / float32 / 32-bit Floating Point</li><li>FP16 / float16 / 16-bit Floating Point</li><li>BF16 / bfloat16 / 16-bit Brain Floating Point</li><li>FP8 / float8 / 8-bit Floating Point</li></ul><figure><img src="/blog/assets/024_float_point_list-16OE-Y0d.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Note:</p><ul><li>bfloat16 was proposed by Google Brain, where &#39;b&#39; stands for &quot;brain&quot;</li><li>Two types of float8 are named based on exponent and mantissa (e4m3 and e5m2)</li></ul><p>We focus on two aspects of floating-point numbers: precision and numerical range</p><ul><li>Precision: The fineness of the numbers that can be represented (i.e., the gap between two adjacent representable numbers)</li><li>Numerical Range: The maximum and minimum values that can be represented</li></ul><p>Numerical range of different floating-point numbers:</p><figure><img src="/blog/assets/024_float_point_list-16OE-Y0d.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>From the chart (look at the width, the wider the range, the larger the numerical range):</p><ul><li>float32 and bfloat16 have the same and relatively large numerical range</li><li>float16 and float8_e5m2 have the same numerical range, which is relatively small</li><li>float8_e4m3 has the smallest numerical range</li></ul><p>Precision of different floating-point numbers: <img src="/blog/assets/024_float_point_precision-Ddm83ksq.png" alt="" loading="lazy"></p><p>From the chart (look at the spacing of the vertical lines, the smaller the spacing, the greater the precision):</p><ul><li>bfloat16 has lower precision than float32 and float16</li></ul><h3 id="_5-2-concept-of-mixed-precision-training" tabindex="-1"><a class="header-anchor" href="#_5-2-concept-of-mixed-precision-training" aria-hidden="true">#</a> 5.2. Concept of Mixed Precision Training</h3><p>The concept of mixed precision training is to use lower precision formats to reduce computational and memory requirements while maintaining performance comparable to full precision (float32) training.</p><p>However, completely abandoning float32 is impractical because certain critical parts require higher precision to avoid numerical instability. Therefore, in practice, a mix of high and low precision formats is often used, a method known as &quot;mixed precision training.&quot;</p><h3 id="_5-3-summary-of-known-methods-for-mixed-precision-training" tabindex="-1"><a class="header-anchor" href="#_5-3-summary-of-known-methods-for-mixed-precision-training" aria-hidden="true">#</a> 5.3. Summary of Known Methods for Mixed Precision Training</h3><figure><img src="/blog/assets/024_mixed_precision_training_list-zWnwhm6Z.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Number of Parameters: Ψ</p><ul><li>BF16+FP32 Mixed Precision Baseline: 2Ψ + 6Ψ + 12Ψ = 20Ψ <ul><li>Model Parameters (half precision): 2 bytes</li><li>Gradients (half precision) + FP32 gradients (accumulated in FP32 precision): 2 + 4 = 6 bytes</li><li>FP32 Model Parameters and Optimizer States: 4 + (4 + 4) = 12 bytes</li></ul></li><li>BF16+FP32 Mixed Precision without FP32 Gradients: 2Ψ + 2Ψ + 12Ψ = 16Ψ <ul><li>Model Parameters (half precision): 2 bytes</li><li>Gradients (half precision): 2 bytes</li><li>FP32 Model Parameters and Optimizer States: 4 + (4 + 4) = 12 bytes</li></ul></li></ul><h3 id="_5-4-fp16-and-bf16-training" tabindex="-1"><a class="header-anchor" href="#_5-4-fp16-and-bf16-training" aria-hidden="true">#</a> 5.4. FP16 and BF16 Training</h3><p>Simply switching all tensors and operations to float16 format usually doesn&#39;t work, often resulting in divergent loss values. However, the initial mixed precision training paper proposed three techniques to maintain the performance of float32 training:</p><ul><li>FP32 Copy of Weights: <ul><li>Using float16 weights can encounter two issues. During training, some weights may become very small and be rounded to 0. Even if the weights themselves are not close to 0, the magnitude difference may cause weights to underflow in addition operations if the update amount is very small. Once weights become 0, they will remain 0 in subsequent training because no gradient signal is passed.</li></ul></li><li>Loss Scaling: <ul><li>Gradients face similar issues because they are often much smaller than 1 and prone to underflow. A simple but effective strategy is to scale (amplify) the loss before backpropagation and then reverse scale (reduce) the gradients after backpropagation. This ensures no underflow occurs during backpropagation, and the scaling operation doesn&#39;t affect training because we reverse scale before further processing gradients (e.g., clipping) and optimization steps.</li></ul></li><li>Accumulation: <ul><li>Performing certain arithmetic operations (e.g., averaging or summing) in 16-bit precision may also encounter underflow or overflow issues. The solution is to use float32 precision to accumulate intermediate results during operations and only convert the result back to 16-bit precision at the end.</li></ul></li></ul><p>The core goal of these three techniques is to leverage the computational acceleration brought by low precision while ensuring training stability by introducing high precision (e.g., float32) operations to avoid numerical instability issues (e.g., gradient or weight underflow). Through these techniques, we can benefit from faster low precision arithmetic operations while maintaining training stability, resulting in higher throughput.</p><h3 id="_5-5-fp8-training" tabindex="-1"><a class="header-anchor" href="#_5-5-fp8-training" aria-hidden="true">#</a> 5.5. FP8 Training</h3><ul><li>FP8 precision and numerical range are very limited, prone to numerical instability and divergent loss, especially in high learning rate scenarios.</li><li>The main advantage of FP8 is its ability to significantly enhance computational efficiency (e.g., on NVIDIA H100 GPUs, FP8 matrix multiplication performance is twice that of bfloat16), making it attractive for training that seeks high throughput and low energy consumption.</li></ul><figure><img src="/blog/assets/024_divergent_loss-nJoB-ECp.png" alt="Divergent Loss" tabindex="0" loading="lazy"><figcaption>Divergent Loss</figcaption></figure><h4 id="_5-5-1-fp8-mixed-precision-training-in-deepseek-v3" tabindex="-1"><a class="header-anchor" href="#_5-5-1-fp8-mixed-precision-training-in-deepseek-v3" aria-hidden="true">#</a> 5.5.1. FP8 Mixed Precision Training in DeepSeek-V3</h4><ul><li>The first successful, very large-scale FP8 mixed precision training was publicly reported in DeepSeek-V3.</li><li>The authors carefully analyzed each operation in the forward pass (Fprop) and activations (Dgrad) and weights (Wgrad) operations in the backward pass.</li><li>To address numerical instability caused by FP8&#39;s low precision, they adopted strategies similar to BF16 mixed precision training: keeping critical parts (e.g., aggregation operations and main weights) at higher precision (possibly float32 or bfloat16) while delegating compute-intensive operations (e.g., matrix multiplication) to FP8, thus ensuring stability while fully leveraging FP8&#39;s high-performance advantages.</li></ul><figure><img src="/blog/assets/024_deepseek_v3_fp8-j18TkbxW.png" alt="FP8 Mixed Precision Training Framework in DeepSeek-V3" tabindex="0" loading="lazy"><figcaption>FP8 Mixed Precision Training Framework in DeepSeek-V3</figcaption></figure><p>DeepSeek-V3 Paper: http://arxiv.org/pdf/2412.19437</p><h2 id="_6-activation-recomputation-gradient-checkpointing-rematerialization" tabindex="-1"><a class="header-anchor" href="#_6-activation-recomputation-gradient-checkpointing-rematerialization" aria-hidden="true">#</a> 6. Activation Recomputation / Gradient Checkpointing / Rematerialization</h2><p>Trading time for space, computation for memory: discard some activations computed during the forward pass to save memory and spend extra computation during the backward pass to dynamically recompute activations.</p><p>Activation Storage Content:</p><ul><li>Without Recomputation: Store every hidden state between two learnable operations (e.g., feedforward network, layer normalization) for use in computing gradients during the backward pass.</li><li>With Recomputation: Store activations only at a few key points in the model architecture, discard the rest, and dynamically recompute them during the backward pass starting from the nearest saved activation, essentially re-executing a subpart of the forward pass.</li></ul><figure><img src="/blog/assets/024_activation_recomputation_process-tNDxYgfa.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Activation Recomputation Strategies:</p><ul><li>Full Recomputation <ul><li>Perform a complete forward pass during the backward pass</li><li>Memory Usage: Activations occupy almost no memory</li><li>Computational Cost: Increases computational cost and time by up to 30-40%</li></ul></li><li>Selective Recomputation (Preferred) <ul><li>Discard and recompute the attention part (which causes the largest growth in activations but has the cheapest computational cost)</li><li>Memory Usage: Reduces activation memory usage by 70% (significantly reduces memory access overhead)</li><li>Computational Cost: Increases computational cost by 2.7% (slightly increases the number of FLOPS, where FLOPS stands for Floating point operations per second)</li><li>This trade-off is particularly advantageous on hardware with small, fast memory, such as GPUs, because accessing memory is usually slower than performing computations. Despite involving additional operations, the overall effect is often faster computation with significantly reduced memory usage.</li></ul></li></ul><figure><img src="/blog/assets/024_no_recomputation_8b-F_J5_04-.png" alt="No Recomputation - 8B" tabindex="0" loading="lazy"><figcaption>No Recomputation - 8B</figcaption></figure><figure><img src="/blog/assets/024_full_recomputation_8b-hjGV9SJN.png" alt="Full Recomputation - 8B" tabindex="0" loading="lazy"><figcaption>Full Recomputation - 8B</figcaption></figure><figure><img src="/blog/assets/024_selective_recomputation_8b-G5un0y8u.png" alt="Selective Recomputation - 8B" tabindex="0" loading="lazy"><figcaption>Selective Recomputation - 8B</figcaption></figure><figure><img src="/blog/assets/024_no_recomputation_70b-5yTACmz2.png" alt="No Recomputation - 70B" tabindex="0" loading="lazy"><figcaption>No Recomputation - 70B</figcaption></figure><figure><img src="/blog/assets/024_full_recomputation_70b-N_l20i9U.png" alt="Full Recomputation - 70B" tabindex="0" loading="lazy"><figcaption>Full Recomputation - 70B</figcaption></figure><figure><img src="/blog/assets/024_selective_recomputation_70b-Fnz-Nn2X.png" alt="Selective Recomputation - 70B" tabindex="0" loading="lazy"><figcaption>Selective Recomputation - 70B</figcaption></figure><ul><li>The smaller the model, the larger the proportion of activations</li><li>The longer the sequence, the larger the proportion of activations</li><li>For small models with long sequences, recomputation has a significant impact on memory</li></ul><p>Implementation of Selective Recomputation: FlashAttention</p><p>However, activations still have a linear dependency on batch size, and all the profiles in the bar charts above use a batch size of 1, so this may become an issue again when we move to larger batch sizes. Don&#39;t despair, because we have a second tool—gradient accumulation to save the day!</p><h2 id="_7-gradient-accumulation" tabindex="-1"><a class="header-anchor" href="#_7-gradient-accumulation" aria-hidden="true">#</a> 7. Gradient Accumulation</h2><p>Trading time for space, computation for memory: further divide the batch into micro-batches, perform forward and backward passes for each micro-batch to compute gradients, then accumulate the gradients from each micro-batch (gradient accumulation is actually averaging rather than summing, so it is not affected by the number of micro-batches), and finally execute the optimizer update step.</p><p>Terminology:</p><ul><li>Batch size (bs)</li><li>Micro batch size (mbs)</li><li>Global batch size (gbs)</li><li>grad_acc: the number of gradient accumulation steps</li></ul><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>bs = gbs = mbs * grad_acc
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><figure><img src="/blog/assets/024_gradient_accumulation-qnp3itkV.png" alt="Gradient Accumulation" tabindex="0" loading="lazy"><figcaption>Gradient Accumulation</figcaption></figure><p>Advantages and Disadvantages of Gradient Accumulation:</p><ul><li>Advantages: <ul><li>Allows batch size to be set larger while keeping memory usage stable, reducing memory usage of activations that grow linearly with batch size through micro-batch size</li><li>Compatible with Activation Recomputation, can be used together to reduce memory usage</li><li>Forward and backward computations of multiple micro-batches can be processed in parallel</li></ul></li><li>Disadvantages: <ul><li>Requires computation of multiple forward and backward passes, increasing computational cost</li></ul></li></ul><h2 id="_8-tools" tabindex="-1"><a class="header-anchor" href="#_8-tools" aria-hidden="true">#</a> 8. Tools</h2><h2 id="_8-1-memory-usage-calculation-tool-predict-memory" tabindex="-1"><a class="header-anchor" href="#_8-1-memory-usage-calculation-tool-predict-memory" aria-hidden="true">#</a> 8.1. Memory Usage Calculation Tool: Predict Memory</h2><p>Before diving into code and experiments, we want to understand how each method works at a high level and what its advantages and limits are. You&#39;ll learn about which parts of a language model eat away your memory and when during training it happens. You&#39;ll learn how we can solve memory constraints by parallelizing the models and increase the throughput by scaling up GPUs. As a result, you&#39;ll understand how the following widget to compute the memory breakdown of a transformer model works.</p><figure><img src="/blog/assets/024_memory_usage_widget-GZpHZ-eQ.png" alt="Memory Usage Widget" tabindex="0" loading="lazy"><figcaption>Memory Usage Widget</figcaption></figure><p>Memory Usage Prediction Tool: https://huggingface.co/spaces/nanotron/predict_memory</p><figure><img src="/blog/assets/024_memory_timeline-cE57v0pS.png" alt="Memory Timeline" tabindex="0" loading="lazy"><figcaption>Memory Timeline</figcaption></figure><h2 id="_8-2-distributed-training-tool-for-visualizing-gpu-compute-and-communication-costs-profiler" tabindex="-1"><a class="header-anchor" href="#_8-2-distributed-training-tool-for-visualizing-gpu-compute-and-communication-costs-profiler" aria-hidden="true">#</a> 8.2. Distributed Training Tool for Visualizing GPU Compute and Communication Costs: Profiler</h2><p>Purpose: Understand and verify GPU compute and communication costs, identify bottlenecks</p><p><a href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html" target="_blank" rel="noopener noreferrer">PyTorch&#39;s profiler<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>Code:</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ],
    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),
    on_trace_ready=torch.profiler.tensorboard_trace_handler(&#39;./log/profile&#39;),
    with_stack=True
) as prof:
    for step in range(steps):
        train_step() 
        prof.step()
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="/blog/assets/024_profiler-TUbH0qPy.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>This generates a trace that we can visualize in TensorBoard or Chrome&#39;s trace viewer. The trace shows:</p><ul><li>CPU threads asynchronously launching kernels to the GPU</li><li>Multiple CUDA streams processing compute and communication in parallel</li><li>Kernel execution times and memory allocations For example, the trace shows CPU threads asynchronously launching kernels to the GPU, with compute kernels and communication occurring in parallel on different CUDA streams. The trace helps identify bottlenecks, such as:</li><li>Sequential compute and communication that could have been overlapped</li><li>Idle time where the GPU is waiting for data transfers</li><li>Memory movement between CPU and GPU</li><li>Kernel launch overhead on the CPU</li></ul><h2 id="_9-reference-ultrascale-playbook" tabindex="-1"><a class="header-anchor" href="#_9-reference-ultrascale-playbook" aria-hidden="true">#</a> 9. Reference: Ultrascale Playbook</h2><p><a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook" target="_blank" rel="noopener noreferrer">https://huggingface.co/spaces/nanotron/ultrascale-playbook<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><h3 id="_9-1-overview" tabindex="-1"><a class="header-anchor" href="#_9-1-overview" aria-hidden="true">#</a> 9.1. Overview</h3><figure><img src="/blog/assets/024_preview-AUOpCXxY.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/blog/assets/024_preview1-5sq2ztyI.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/blog/assets/024_preview2-jIG_e765.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_9-2-prerequisite-knowledge" tabindex="-1"><a class="header-anchor" href="#_9-2-prerequisite-knowledge" aria-hidden="true">#</a> 9.2. Prerequisite Knowledge</h3><ul><li>Mainstream LLM Architectures</li><li>Basics of Model Training: How Deep Learning Models are Trained <ul><li>Recommended Quality Educational Resources <ul><li>https://www.deeplearning.ai/</li><li>https://pytorch.org/tutorials/beginner/basics/intro.html</li></ul></li></ul></li></ul><h3 id="_9-3-scaling-experiments" tabindex="-1"><a class="header-anchor" href="#_9-3-scaling-experiments" aria-hidden="true">#</a> 9.3. Scaling Experiments</h3><p>We ran over 4000 scaling experiments on up to 512 GPUs and measured throughput (size of markers) and GPU utilization (color of markers). Note that both are normalized per model size in this visualization.</p><p>We ran over 4100 distributed experiments (over 16k including test runs) with up to 512 GPUs to scan many possible distributed training layouts and model sizes.</p><figure><img src="/blog/assets/024_scaling_experiments-7UafEv_Q.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/blog/assets/024_scaling_experiments1-_V_eQt5b.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure></div><!--[--><!----><!--]--><footer class="page-meta"><!----><div class="meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a aria-label="Distributed Training Part 2: Parallel Programming" class="vp-link nav-link prev nav-link prev" href="/blog/posts/llm/025_distribution_and_parallelism_1.html"><div class="hint"><span class="arrow start"></span>Prev</div><div class="link"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Distributed Training Part 2: Parallel Programming</div></a><a aria-label="Langchain and LlamaIndex Integration" class="vp-link nav-link next nav-link next" href="/blog/posts/llm/023_agent_framework.html"><div class="hint">Next<span class="arrow end"></span></div><div class="link">Langchain and LlamaIndex Integration<span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span></div></a></nav><!----><!--[--><!----><!--]--><!--]--></main><!--]--><!----></div><!--]--><!--]--><!----><!--]--></div>
    <script type="module" src="/blog/assets/app-dpf1czaz.js" defer></script>
  </body>
</html>
