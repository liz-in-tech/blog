<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.0" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.13" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="alternate" hreflang="zh-cn" href="https://liz-in-tech.github.io/blog/zh/posts/llm/025_distribution_and_parallelism_1.html"><meta property="og:url" content="https://liz-in-tech.github.io/blog/posts/llm/025_distribution_and_parallelism_1.html"><meta property="og:site_name" content="Liz"><meta property="og:title" content="Distributed Training Part 2: Parallel Programming"><meta property="og:description" content="Distributed Training Part 2: Parallel Programming"><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:locale:alternate" content="zh-CN"><meta property="og:updated_time" content="2025-03-08T14:32:06.000Z"><meta property="article:author" content="Liz"><meta property="article:tag" content="Distributed"><meta property="article:tag" content="Parallel"><meta property="article:published_time" content="2025-02-28T00:00:00.000Z"><meta property="article:modified_time" content="2025-03-08T14:32:06.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Distributed Training Part 2: Parallel Programming","image":[""],"datePublished":"2025-02-28T00:00:00.000Z","dateModified":"2025-03-08T14:32:06.000Z","author":[{"@type":"Person","name":"Liz","url":"https://github.com/liz-in-tech"}]}</script><link rel="icon" herf="/blogger.png"><link rel="icon" href="/blog/blogger.png"><title>Distributed Training Part 2: Parallel Programming | Liz</title><meta name="description" content="Distributed Training Part 2: Parallel Programming">
    <link rel="preload" href="/blog/assets/style-m_obra2h.css" as="style"><link rel="stylesheet" href="/blog/assets/style-m_obra2h.css">
    <link rel="modulepreload" href="/blog/assets/app-dpf1czaz.js"><link rel="modulepreload" href="/blog/assets/025_distribution_and_parallelism_1.html-_9D7bYHB.js"><link rel="modulepreload" href="/blog/assets/025_barrier-Cle6-zl5.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-x3n3nnut.js"><link rel="modulepreload" href="/blog/assets/025_distribution_and_parallelism_1.html-32IzAErP.js">
    <link rel="prefetch" href="/blog/assets/index.html-YbPtte5_.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-CGfhr1vY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FUMOuem4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--TTjrkIy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g4Nfr7z1.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-bitGHKd2.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-nrisQopy.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-9XtwFAwc.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-oji9upQP.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-xrin91s2.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-q7LEGqjL.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-c628DmZb.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-4RcS3hxb.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-_8stdYVV.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-eluz3bTT.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-Ft0RQWf3.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-CImz0KSx.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-1CqW55t5.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-6CxOIR84.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-l_kCYUI1.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html-JrbwEhT3.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-XtD4OMNh.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html-VmsqVzZn.js" as="script"><link rel="prefetch" href="/blog/assets/018_autorag.html-azoAgFGI.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-8h4F60kB.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-61m3-6IM.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-0d0frUhq.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-FazlUPBT.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-5MCDM_Sd.js" as="script"><link rel="prefetch" href="/blog/assets/024_distribution_and_parallelism.html-uCefT9T7.js" as="script"><link rel="prefetch" href="/blog/assets/026_distribution_and_parallelism_2.html-MEZ1JY2b.js" as="script"><link rel="prefetch" href="/blog/assets/027_distribution_and_parallelism_3.html-IYByZhbZ.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-Kub1JEXd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZDCSnlc1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZYw6WxxA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OavE9BET.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-hE_T0u_5.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-iH0mq6XB.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-WyFhRqF6.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-YaT0PR6o.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-x2qtCXhJ.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-WHPR-17-.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-hz6Q-DAA.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-3KMCwhrh.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-7gxKMp_3.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-KndRvZAj.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-Akmj-Ub_.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-bXz0YlpJ.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-W-CdR_ck.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-aq7YkQ-T.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html-V8hUrR0F.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-DCPUUxWe.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html--6PmCjyW.js" as="script"><link rel="prefetch" href="/blog/assets/018_autorag.html-QupCTUgH.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-6hLqoXkg.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-qFyQHNwt.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-kvlLpO3f.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-AiRwjg9s.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-VZsN2kO8.js" as="script"><link rel="prefetch" href="/blog/assets/024_distribution_and_parallelism.html-MO4kg9bC.js" as="script"><link rel="prefetch" href="/blog/assets/025_distribution_and_parallelism_1.html-ZR9kuXUl.js" as="script"><link rel="prefetch" href="/blog/assets/026_distribution_and_parallelism_2.html-GkjDPoeT.js" as="script"><link rel="prefetch" href="/blog/assets/027_distribution_and_parallelism_3.html-5kFJGreK.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-38Gbieji.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wh_dBtOR.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-cxLWDy2T.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kf4JCRaf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-RkA-insV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4kI_oqSd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-WhuidxNt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OqGkeUA_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5GeN-sdD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-c4Rf4yh1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2r0jUs7o.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BSKRXRQc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-KjTsJ0Hg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TteIwMx3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g00XXzrL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-YxbJgo4L.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-3T79Cy0i.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-A5tlQHan.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-04ff5e0O.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-vgZ6rfFh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OmipPplE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-E1KrJL6a.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oPH9QkTj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cHRqZSs8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-74SU9ZTn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--iJiA8oX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oZPWb_Fc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FUIWSLsp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5ERWyusD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Zjn0JNqd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jcvPTrgB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9t2TsyuQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CcLVFNIv.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DVoYOOaL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-V52ipvRm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9If_KW0o.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kUk1kAMU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GYH0QUoo.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-JVTfeijx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_C1QVNqX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-L_IXFmna.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nZWHmXY7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-x4gPgqE4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bWnVyuyA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-LHukpLS7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-howjHe2f.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DeN_iOWx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cnlzR0a7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0P_c_pcU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-dy_CcFmq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FzFytZ_p.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2KSwV7xp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-F1coElwg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ffflqCb9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-F8ZuLYgH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-HeYWaFeL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xKYeJEc5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xB-iS7Ql.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OEUPqfTV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-EE4iQI9m.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-g1PUF_BG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nUBcs85a.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9Y3la5Sf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wufIFDPM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bXZQIxRE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wd6ZkEHi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Xlk0AXmC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bGhIpy0C.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mSPhZxqB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-51HoKD5A.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Q5_K6Vux.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jkWPo860.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kTEqch5G.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-y4iBqBqc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mDjwyff2.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-rlPv-U40.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Xozbilpx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rQSkUpmB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5mHlpcIA.js" as="script"><link rel="prefetch" href="/blog/assets/intro.html-MDqcApPy.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-svn8VaJE.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html-t1ZKIaFi.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-svNDcpfY.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-ibPgsQe-.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html-12Wld-D8.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-gg6HeTG1.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-_9L9rGol.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-afDMONW6.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-OYo1zSVw.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-eukxPDTz.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-5ahBfiJs.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html-h7f4lbQ3.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-iRQW1OIW.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-1KF03aF2.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html-O9tsguRd.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-z6HAoRZE.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html-IfCt6cQN.js" as="script"><link rel="prefetch" href="/blog/assets/018_autorag.html-QHkLO_Uc.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-A8rSneDc.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-lewZvDbY.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-pDAOS03B.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-5gR9Z1Ha.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-nVD6mEoW.js" as="script"><link rel="prefetch" href="/blog/assets/024_distribution_and_parallelism.html-QSMoIwEI.js" as="script"><link rel="prefetch" href="/blog/assets/026_distribution_and_parallelism_2.html-pqRrl5fs.js" as="script"><link rel="prefetch" href="/blog/assets/027_distribution_and_parallelism_3.html-wKs_50DK.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-yEsL0eWX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-LspvVCg2.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7RypPZGx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--XgiAiTK.js" as="script"><link rel="prefetch" href="/blog/assets/001_langchain.html-KLXLK5Z1.js" as="script"><link rel="prefetch" href="/blog/assets/002_langchain_sourcecode.html--JWFw4f5.js" as="script"><link rel="prefetch" href="/blog/assets/003_streamlit.html-XYAeXNBe.js" as="script"><link rel="prefetch" href="/blog/assets/004_transformer.html-tlV0s4Tp.js" as="script"><link rel="prefetch" href="/blog/assets/005_llama.html--4ncz0Yn.js" as="script"><link rel="prefetch" href="/blog/assets/006_llm_leaderboard.html-jeKMYERG.js" as="script"><link rel="prefetch" href="/blog/assets/007_computer_use.html-McEa8nYA.js" as="script"><link rel="prefetch" href="/blog/assets/008_rag_challenge.html-t6Z5Tyoq.js" as="script"><link rel="prefetch" href="/blog/assets/009_llm_challenge.html-SFq9dk7k.js" as="script"><link rel="prefetch" href="/blog/assets/010_rag_workflow.html-4nGBR1tH.js" as="script"><link rel="prefetch" href="/blog/assets/011_vector_database.html-n8KhvvmX.js" as="script"><link rel="prefetch" href="/blog/assets/012_prompt_engineering.html--vhenbJs.js" as="script"><link rel="prefetch" href="/blog/assets/013_optimizing_llm.html-eJIzxi-r.js" as="script"><link rel="prefetch" href="/blog/assets/014_rag_evaluation.html-0_z0N4cn.js" as="script"><link rel="prefetch" href="/blog/assets/015_fine_tune.html-asxEThTF.js" as="script"><link rel="prefetch" href="/blog/assets/016_multimodal.html-v8Tf8NbJ.js" as="script"><link rel="prefetch" href="/blog/assets/017_agent_and_multiagent.html-hKis9eJ-.js" as="script"><link rel="prefetch" href="/blog/assets/018_autorag.html-w3pHD7fm.js" as="script"><link rel="prefetch" href="/blog/assets/019_ollama.html-a_fmLOJF.js" as="script"><link rel="prefetch" href="/blog/assets/020_neo4j.html-Z9QN3I5k.js" as="script"><link rel="prefetch" href="/blog/assets/021_microsoft_graphrag.html-lO9Yktq1.js" as="script"><link rel="prefetch" href="/blog/assets/022_llamaindex_graphrag.html-zD3oU5qx.js" as="script"><link rel="prefetch" href="/blog/assets/023_agent_framework.html-6npfcKl0.js" as="script"><link rel="prefetch" href="/blog/assets/024_distribution_and_parallelism.html-cjWEF7OA.js" as="script"><link rel="prefetch" href="/blog/assets/025_distribution_and_parallelism_1.html-z5CSb0LW.js" as="script"><link rel="prefetch" href="/blog/assets/026_distribution_and_parallelism_2.html-BtwN9BTM.js" as="script"><link rel="prefetch" href="/blog/assets/027_distribution_and_parallelism_3.html-7_Y86UGw.js" as="script"><link rel="prefetch" href="/blog/assets/028_distribution_and_parallelism_4.html-oOKqU8GM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SUPOf1zR.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-GyHXtMAi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-is8-d0Ed.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-zjp-8UWR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-6XBW8Ov3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ReszkuNd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-I1KAZoJS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-tGnEaYkR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9JOjCKuG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-p096FpRm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--QWlevSp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-KXM4afpz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-64vzsvy1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CNPm0ezS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-iXzmtIjD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-w8OoKm6o.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Fiq11jHe.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Z7h9D0Jf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oREYj-a-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-budu_xBX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-If0V6QyO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-RWY55qw7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UtL__lw1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jvBihG4-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ExwAseF_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kaGWF7Cx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-paG2kzYt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--Llj_cs6.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9IPCc-JI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-gKNfe4Gs.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PBrmCCvX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-VMt1Fqv4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wnICfMXi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OOQkvOmw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oTXANWTI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PSeY-y79.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-j2ygMYLh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_V7D3_v_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-KjISYaQI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jmjguAy1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-erodNSeh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2TE--uqe.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-V9gGDnkt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Biwg39Wn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UweT067v.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-piHHnfzq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2E9LwQB9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-co_ZGbxH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-IU3fDVGW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FEpI73em.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PdvgQbUn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C1kxVAHO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wS8WdvrO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-XVQ3rLg7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-WtPaqBmM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-VIOkao96.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-LY77Ty5E.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-uJnLyt6M.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-V_28uAsN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-fsr-C8y0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-am3MbFaJ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UF_ZR-BQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jdjsnYbZ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xM6KW1wy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xMx166s4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5bylOvyD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Mv5ChhRw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OGmfB8EP.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Gl7UPVqn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Im-IDi1W.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nlTIfczW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-RlYU_OE5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xLNxKPlX.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-08_zHRDQ.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><!--[--><div class="theme-container no-sidebar has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/blog/"><img class="vp-nav-logo" src="/blog/blogger.png" alt><!----><span class="vp-site-name hide-in-pad">Liz</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="Home" class="vp-link nav-link nav-link" href="/blog/"><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span>Home<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Project" class="vp-link nav-link nav-link" href="/blog/demo/"><span class="font-icon icon fa-fw fa-sm fas fa-star" style=""></span>Project<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><div class="nav-item"><div class="dropdown-wrapper i18n-dropdown"><button type="button" class="dropdown-title" aria-label="Select language"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="i18n icon" style="width:1rem;height:1rem;vertical-align:middle;"><path d="M379.392 460.8 494.08 575.488l-42.496 102.4L307.2 532.48 138.24 701.44l-71.68-72.704L234.496 460.8l-45.056-45.056c-27.136-27.136-51.2-66.56-66.56-108.544h112.64c7.68 14.336 16.896 27.136 26.112 35.84l45.568 46.08 45.056-45.056C382.976 312.32 409.6 247.808 409.6 204.8H0V102.4h256V0h102.4v102.4h256v102.4H512c0 70.144-37.888 161.28-87.04 210.944L378.88 460.8zM576 870.4 512 1024H409.6l256-614.4H768l256 614.4H921.6l-64-153.6H576zM618.496 768h196.608L716.8 532.48 618.496 768z"></path></svg><!--]--><span class="arrow"></span><ul class="nav-dropdown"><li class="dropdown-item"><a aria-label="English" class="vp-link nav-link active nav-link active" href="/blog/posts/llm/025_distribution_and_parallelism_1.html"><!---->English<!----></a></li><li class="dropdown-item"><a aria-label="简体中文" class="vp-link nav-link nav-link" href="/blog/zh/posts/llm/025_distribution_and_parallelism_1.html"><!---->简体中文<!----></a></li></ul></button></div></div><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/liz-in-tech" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Distributed Training Part 2: Parallel Programming</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/liz-in-tech" target="_blank" rel="noopener noreferrer">Liz</a></span><span property="author" content="Liz"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2025-02-28T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 5 min</span><meta property="timeRequired" content="PT5M"></span><span class="page-category-info" aria-label="Category🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category6 clickable" role="navigation">LLM</span><!--]--><meta property="articleSection" content="LLM"></span><span class="page-tag-info" aria-label="Tag🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag3 clickable" role="navigation">Distributed</span><span class="page-tag-item tag2 clickable" role="navigation">Parallel</span><!--]--><meta property="keywords" content="Distributed,Parallel"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_1-overview">1. Overview</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_2-broadcast">2. Broadcast</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_3-reduce-allreduce">3. Reduce &amp; AllReduce</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_4-gather-allgather">4. Gather &amp; AllGather</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_5-scatter-reducescatter">5. Scatter &amp; ReduceScatter</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_6-barrier">6. Barrier</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_7-pytorch-code-implementation">7. PyTorch Code Implementation</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_7-1-what-is-nccl">7.1. What is NCCL</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_7-2-broadcast">7.2. Broadcast</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_7-3-reduce">7.3. Reduce</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_7-4-allreduce">7.4. AllReduce</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_7-5-gather">7.5. Gather</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_7-6-allgather">7.6. AllGather</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_7-7-scatter">7.7. Scatter</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_7-8-reducescatter">7.8. ReduceScatter</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_7-9-barrier">7.9. Barrier</a></li><!----><!--]--></ul></li><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h1 id="distributed-training-part-2-parallel-programming" tabindex="-1"><a class="header-anchor" href="#distributed-training-part-2-parallel-programming" aria-hidden="true">#</a> Distributed Training Part 2: Parallel Programming</h1><!-- more --><h2 id="_1-overview" tabindex="-1"><a class="header-anchor" href="#_1-overview" aria-hidden="true">#</a> 1. Overview</h2><ul><li>Broadcast</li><li>Reduce</li><li>AllReduce</li><li>Gather</li><li>AllGather</li><li>Scatter</li><li>ReduceScatter</li><li>Barrier</li></ul><p>Note: The root node acts as a server, serving as the target or source for certain operations.</p><p>Relationship: AllReduce = ReduceScatter + AllGather</p><figure><img src="/blog/assets/025_allreduce_relation-H8NMiCQn.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_2-broadcast" tabindex="-1"><a class="header-anchor" href="#_2-broadcast" aria-hidden="true">#</a> 2. Broadcast</h2><figure><img src="/blog/assets/025_broadcast-uLqUf_wO.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_3-reduce-allreduce" tabindex="-1"><a class="header-anchor" href="#_3-reduce-allreduce" aria-hidden="true">#</a> 3. Reduce &amp; AllReduce</h2><p>Combine values from each node using a function to produce a single value.</p><p>Common functions for f() are SUM or AVG.</p><ul><li>AVG is only available with the NCCL backend.</li></ul><figure><img src="/blog/assets/025_reduce-yXKNy72E.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Reduce: The result is sent only to the root node.</li><li>AllReduce: The result is broadcast to every node (each node has the same value).</li></ul><h2 id="_4-gather-allgather" tabindex="-1"><a class="header-anchor" href="#_4-gather-allgather" aria-hidden="true">#</a> 4. Gather &amp; AllGather</h2><figure><img src="/blog/assets/025_gather-MQZDPrtG.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_5-scatter-reducescatter" tabindex="-1"><a class="header-anchor" href="#_5-scatter-reducescatter" aria-hidden="true">#</a> 5. Scatter &amp; ReduceScatter</h2><p><img src="/blog/assets/025_scatter-0m04J2I3.png" alt="" loading="lazy"> Scatter</p><ul><li>Scatter differs from broadcast in that scatter sends data in pieces, while broadcast sends the entire data.</li><li>Scatter is logically the reverse operation of gather.</li></ul><p>ReduceScatter</p><ul><li>Split data on each node into pieces.</li><li>Perform Reduce on data from each piece across nodes using a function.</li><li>Scatter the result of each piece&#39;s Reduce to each node.</li></ul><h2 id="_6-barrier" tabindex="-1"><a class="header-anchor" href="#_6-barrier" aria-hidden="true">#</a> 6. Barrier</h2><p>A barrier will not be lifted until all nodes reach it. Once all nodes reach the barrier, subsequent computations can proceed, used for synchronizing nodes.</p><figure><img src="/blog/assets/025_barrier-c-EDt2OK.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_7-pytorch-code-implementation" tabindex="-1"><a class="header-anchor" href="#_7-pytorch-code-implementation" aria-hidden="true">#</a> 7. PyTorch Code Implementation</h2><h3 id="_7-1-what-is-nccl" tabindex="-1"><a class="header-anchor" href="#_7-1-what-is-nccl" aria-hidden="true">#</a> 7.1. What is NCCL</h3><p>NCCL</p><ul><li>NVIDIA Collective Communications Library</li><li>Optimized primitives for communication between NVIDIA GPUs</li><li>Designed for efficient GPU-GPU communication</li></ul><h3 id="_7-2-broadcast" tabindex="-1"><a class="header-anchor" href="#_7-2-broadcast" aria-hidden="true">#</a> 7.2. Broadcast</h3><p>Code</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">as</span> dist

<span class="token keyword">def</span> <span class="token function">init_process</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span><span class="token string">&#39;nccl&#39;</span><span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
<span class="token keyword">def</span> <span class="token function">example_broadcast</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Before broadcast on rank </span><span class="token interpolation"><span class="token punctuation">{</span>dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    dist<span class="token punctuation">.</span>broadcast<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> src<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;After broadcast on rank </span><span class="token interpolation"><span class="token punctuation">{</span>dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    
init_process<span class="token punctuation">(</span><span class="token punctuation">)</span>
example_broadcast<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Output</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>Before broadcast on rank 0: tensor([1., 2., 3., 4., 5.], device=&#39;cuda:0&#39;)
Before broadcast on rank 1: tensor([0., 0., 0., 0., 0.], device=&#39;cuda:1&#39;)
Before broadcast on rank 2: tensor([0., 0., 0., 0., 0.], device=&#39;cuda:2&#39;)

After broadcast on rank 0: tensor([1., 2., 3., 4., 5.], device=&#39;cuda:0&#39;)
After broadcast on rank 1: tensor([1., 2., 3., 4., 5.], device=&#39;cuda:1&#39;)
After broadcast on rank 2: tensor([1., 2., 3., 4., 5.], device=&#39;cuda:2&#39;)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_7-3-reduce" tabindex="-1"><a class="header-anchor" href="#_7-3-reduce" aria-hidden="true">#</a> 7.3. Reduce</h3><p>Code</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">example_reduce</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Before reduce on rank </span><span class="token interpolation"><span class="token punctuation">{</span>dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    dist<span class="token punctuation">.</span><span class="token builtin">reduce</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> dst<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> op<span class="token operator">=</span>dist<span class="token punctuation">.</span>ReduceOp<span class="token punctuation">.</span>SUM<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;After reduce on rank </span><span class="token interpolation"><span class="token punctuation">{</span>rank<span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    
init_process<span class="token punctuation">(</span><span class="token punctuation">)</span>
example_reduce<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Output</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>Before reduce on rank 0: tensor([1., 1., 1., 1., 1.], device=&#39;cuda:0&#39;)
Before reduce on rank 1: tensor([2., 2., 2., 2., 2.], device=&#39;cuda:1&#39;)
Before reduce on rank 2: tensor([3., 3., 3., 3., 3.], device=&#39;cuda:2&#39;)

After reduce on rank 0: tensor([6., 6., 6., 6., 6.], device=&#39;cuda:0&#39;)
After reduce on rank 1: tensor([2., 2., 2., 2., 2.], device=&#39;cuda:1&#39;)
After reduce on rank 2: tensor([3., 3., 3., 3., 3.], device=&#39;cuda:2&#39;)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_7-4-allreduce" tabindex="-1"><a class="header-anchor" href="#_7-4-allreduce" aria-hidden="true">#</a> 7.4. AllReduce</h3><p>Code</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">example_all_reduce</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Before all_reduce on rank </span><span class="token interpolation"><span class="token punctuation">{</span>dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    dist<span class="token punctuation">.</span>all_reduce<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> op<span class="token operator">=</span>dist<span class="token punctuation">.</span>ReduceOp<span class="token punctuation">.</span>SUM<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;After all_reduce on rank </span><span class="token interpolation"><span class="token punctuation">{</span>dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    
init_process<span class="token punctuation">(</span><span class="token punctuation">)</span>
example_all_reduce<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Output</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>Before all_reduce on rank 0: tensor([1., 1., 1., 1., 1.], device=&#39;cuda:0&#39;)
Before all_reduce on rank 1: tensor([2., 2., 2., 2., 2.], device=&#39;cuda:1&#39;)
Before all_reduce on rank 2: tensor([3., 3., 3., 3., 3.], device=&#39;cuda:2&#39;)

After all_reduce on rank 0: tensor([6., 6., 6., 6., 6.], device=&#39;cuda:0&#39;)
After all_reduce on rank 1: tensor([6., 6., 6., 6., 6.], device=&#39;cuda:1&#39;)
After all_reduce on rank 2: tensor([6., 6., 6., 6., 6.], device=&#39;cuda:2&#39;)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_7-5-gather" tabindex="-1"><a class="header-anchor" href="#_7-5-gather" aria-hidden="true">#</a> 7.5. Gather</h3><p>Code</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">example_gather</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        gather_list <span class="token operator">=</span> <span class="token punctuation">[</span>
            torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        gather_list <span class="token operator">=</span> <span class="token boolean">None</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Before gather on rank </span><span class="token interpolation"><span class="token punctuation">{</span>dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    dist<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> gather_list<span class="token punctuation">,</span> dst<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;After gather on rank 0: </span><span class="token interpolation"><span class="token punctuation">{</span>gather_list<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    
init_process<span class="token punctuation">(</span><span class="token punctuation">)</span>
example_gather<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Output</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>Before gather on rank 0: tensor([1., 1., 1., 1., 1.], device=&#39;cuda:0&#39;)
Before gather on rank 1: tensor([2., 2., 2., 2., 2.], device=&#39;cuda:1&#39;)
Before gather on rank 2: tensor([3., 3., 3., 3., 3.], device=&#39;cuda:2&#39;)

After gather on rank 0: [tensor([1., 1., 1., 1., 1.], device=&#39;cuda:0&#39;),
                         tensor([2., 2., 2., 2., 2.], device=&#39;cuda:0&#39;),
                         tensor([3., 3., 3., 3., 3.], device=&#39;cuda:0&#39;)]
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_7-6-allgather" tabindex="-1"><a class="header-anchor" href="#_7-6-allgather" aria-hidden="true">#</a> 7.6. AllGather</h3><p>Code</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">example_all_gather</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    gather_list <span class="token operator">=</span> <span class="token punctuation">[</span>
        torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">]</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Before all_gather on rank </span><span class="token interpolation"><span class="token punctuation">{</span>dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    dist<span class="token punctuation">.</span>all_gather<span class="token punctuation">(</span>gather_list<span class="token punctuation">,</span> tensor<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;After all_gather on rank </span><span class="token interpolation"><span class="token punctuation">{</span>dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>gather_list<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    
init_process<span class="token punctuation">(</span><span class="token punctuation">)</span>
example_all_gather<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Output</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>Before all_gather on rank 0: tensor([1., 1., 1., 1., 1.], device=&#39;cuda:0&#39;)
Before all_gather on rank 1: tensor([2., 2., 2., 2., 2.], device=&#39;cuda:1&#39;)
Before all_gather on rank 2: tensor([3., 3., 3., 3., 3.], device=&#39;cuda:2&#39;)

After all_gather on rank 0: [tensor([1., 1., 1., 1., 1.], device=&#39;cuda:0&#39;),
                             tensor([2., 2., 2., 2., 2.], device=&#39;cuda:0&#39;),
                             tensor([3., 3., 3., 3., 3.], device=&#39;cuda:0&#39;)]
After all_gather on rank 1: [tensor([1., 1., 1., 1., 1.], device=&#39;cuda:1&#39;),
                             tensor([2., 2., 2., 2., 2.], device=&#39;cuda:0&#39;),
                             tensor([3., 3., 3., 3., 3.], device=&#39;cuda:0&#39;)]
After all_gather on rank 2: [tensor([1., 1., 1., 1., 1.], device=&#39;cuda:2&#39;),
                             tensor([2., 2., 2., 2., 2.], device=&#39;cuda:2&#39;),
                             tensor([3., 3., 3., 3., 3.], device=&#39;cuda:2&#39;)]
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_7-7-scatter" tabindex="-1"><a class="header-anchor" href="#_7-7-scatter" aria-hidden="true">#</a> 7.7. Scatter</h3><p>Code</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">example_scatter</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        scatter_list <span class="token operator">=</span> <span class="token punctuation">[</span>
            torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token punctuation">]</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Rank 0: Tensor to scatter: </span><span class="token interpolation"><span class="token punctuation">{</span>scatter_list<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        scatter_list <span class="token operator">=</span> <span class="token boolean">None</span>
    tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Before scatter on rank </span><span class="token interpolation"><span class="token punctuation">{</span>dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    dist<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> scatter_list<span class="token punctuation">,</span> src<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;After scatter on rank </span><span class="token interpolation"><span class="token punctuation">{</span>dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>tensor<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    
init_process<span class="token punctuation">(</span><span class="token punctuation">)</span>
example_scatter<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Output</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>Rank 0: Tensor to scatter: [tensor([1., 1., 1., 1., 1.], device=&#39;cuda:0&#39;),
                            tensor([2., 2., 2., 2., 2.], device=&#39;cuda:0&#39;),
                            tensor([3., 3., 3., 3., 3.], device=&#39;cuda:0&#39;)]
Before scatter on rank 0: tensor([0., 0., 0., 0., 0.], device=&#39;cuda:0&#39;)
Before scatter on rank 1: tensor([0., 0., 0., 0., 0.], device=&#39;cuda:1&#39;)
Before scatter on rank 2: tensor([0., 0., 0., 0., 0.], device=&#39;cuda:2&#39;)

After scatter on rank 0: tensor([1., 1., 1., 1., 1.], device=&#39;cuda:0&#39;)
After scatter on rank 1: tensor([2., 2., 2., 2., 2.], device=&#39;cuda:1&#39;)
After scatter on rank 2: tensor([3., 3., 3., 3., 3.], device=&#39;cuda:2&#39;)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_7-8-reducescatter" tabindex="-1"><a class="header-anchor" href="#_7-8-reducescatter" aria-hidden="true">#</a> 7.8. ReduceScatter</h3><p>Code</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">example_reduce_scatter</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    rank <span class="token operator">=</span> dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span>
    world_size <span class="token operator">=</span> dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span>
    input_tensor <span class="token operator">=</span> <span class="token punctuation">[</span>
        torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span>rank <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> i <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">**</span><span class="token punctuation">(</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> 
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>world_size<span class="token punctuation">)</span>
        <span class="token punctuation">]</span>
    output_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Before ReduceScatter on rank </span><span class="token interpolation"><span class="token punctuation">{</span>rank<span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>input_tensor<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    dist<span class="token punctuation">.</span>reduce_scatter<span class="token punctuation">(</span>output_tensor<span class="token punctuation">,</span> input_tensor<span class="token punctuation">,</span> op<span class="token operator">=</span>dist<span class="token punctuation">.</span>ReduceOp<span class="token punctuation">.</span>SUM<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;After ReduceScatter on rank </span><span class="token interpolation"><span class="token punctuation">{</span>rank<span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>output_tensor<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>    
    
init_process<span class="token punctuation">(</span><span class="token punctuation">)</span>
example_reduce_scatter<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Output</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>Before ReduceScatter on rank 0: [tensor([1., 2.], device=&#39;cuda:0&#39;),
                                 tensor([1., 4.], device=&#39;cuda:0&#39;),
                                 tensor([1., 8.], device=&#39;cuda:0&#39;)]
Before ReduceScatter on rank 1: [tensor([2., 4.], device=&#39;cuda:1&#39;),
                                 tensor([ 4., 16.], device=&#39;cuda:1&#39;),
                                 tensor([ 8., 64.], device=&#39;cuda:1&#39;)]
Before ReduceScatter on rank 2: [tensor([3., 6.], device=&#39;cuda:2&#39;),
                                 tensor([ 9., 36.], device=&#39;cuda:2&#39;),
                                 tensor([ 27., 216.], device=&#39;cuda:2&#39;)]

After ReduceScatter on rank 0: tensor([ 6., 12.], device=&#39;cuda:0&#39;)
After ReduceScatter on rank 1: tensor([14., 56.], device=&#39;cuda:1&#39;)
After ReduceScatter on rank 2: tensor([ 36., 288.], device=&#39;cuda:2&#39;)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_7-9-barrier" tabindex="-1"><a class="header-anchor" href="#_7-9-barrier" aria-hidden="true">#</a> 7.9. Barrier</h3><p>Code</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">example_barrier</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    rank <span class="token operator">=</span> dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span>
    t_start <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Rank </span><span class="token interpolation"><span class="token punctuation">{</span>rank<span class="token punctuation">}</span></span><span class="token string"> sleeps </span><span class="token interpolation"><span class="token punctuation">{</span>rank<span class="token punctuation">}</span></span><span class="token string"> seconds.&quot;</span></span><span class="token punctuation">)</span>
    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span>rank<span class="token punctuation">)</span>  <span class="token comment"># Simulate different processing times</span>
    dist<span class="token punctuation">.</span>barrier<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Rank </span><span class="token interpolation"><span class="token punctuation">{</span>rank<span class="token punctuation">}</span></span><span class="token string"> after barrier time delta: </span><span class="token interpolation"><span class="token punctuation">{</span>time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span>t_start<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    
init_process<span class="token punctuation">(</span><span class="token punctuation">)</span>
example_barrier<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Output</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>Rank 0 sleeps 0 seconds.
Rank 1 sleeps 1 seconds.
Rank 2 sleeps 2 seconds.

Rank 0 after barrier time delta: 2.0025
Rank 1 after barrier time delta: 2.0025
Rank 2 after barrier time delta: 2.0024
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><!--[--><!----><!--]--><footer class="page-meta"><!----><div class="meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a aria-label="Distributed Training Part 3: Data Parallelism" class="vp-link nav-link prev nav-link prev" href="/blog/posts/llm/026_distribution_and_parallelism_2.html"><div class="hint"><span class="arrow start"></span>Prev</div><div class="link"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Distributed Training Part 3: Data Parallelism</div></a><a aria-label="Distributed Training Part 1: Memory Usage in Model Training" class="vp-link nav-link next nav-link next" href="/blog/posts/llm/024_distribution_and_parallelism.html"><div class="hint">Next<span class="arrow end"></span></div><div class="link">Distributed Training Part 1: Memory Usage in Model Training<span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span></div></a></nav><!----><!--[--><!----><!--]--><!--]--></main><!--]--><!----></div><!--]--><!--]--><!----><!--]--></div>
    <script type="module" src="/blog/assets/app-dpf1czaz.js" defer></script>
  </body>
</html>
