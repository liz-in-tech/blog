const e=JSON.parse('{"key":"v-2131a5fc","path":"/posts/llm/024_distribution_and_parallelism.html","title":"Distributed Training Part 1: Memory Usage in Model Training","lang":"en-US","frontmatter":{"icon":"lightbulb","sidebar":false,"date":"2025-02-26T00:00:00.000Z","prev":"./025_distribution_and_parallelism_1","next":"./023_agent_framework","category":["LLM"],"tag":["Distributed","Parallel"],"description":"Distributed Training Part 1: Memory Usage in Model Training","head":[["link",{"rel":"alternate","hreflang":"zh-cn","href":"https://liz-in-tech.github.io/blog/zh/posts/llm/024_distribution_and_parallelism.html"}],["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/blog/posts/llm/024_distribution_and_parallelism.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"Distributed Training Part 1: Memory Usage in Model Training"}],["meta",{"property":"og:description","content":"Distributed Training Part 1: Memory Usage in Model Training"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:locale:alternate","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-03-08T14:32:06.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:tag","content":"Distributed"}],["meta",{"property":"article:tag","content":"Parallel"}],["meta",{"property":"article:published_time","content":"2025-02-26T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-03-08T14:32:06.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Distributed Training Part 1: Memory Usage in Model Training\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-02-26T00:00:00.000Z\\",\\"dateModified\\":\\"2025-03-08T14:32:06.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. Metrics","slug":"_1-metrics","link":"#_1-metrics","children":[]},{"level":2,"title":"2. Three Key Challenges","slug":"_2-three-key-challenges","link":"#_2-three-key-challenges","children":[]},{"level":2,"title":"3. Basics of Model Training","slug":"_3-basics-of-model-training","link":"#_3-basics-of-model-training","children":[{"level":3,"title":"3.1. Model Training Process","slug":"_3-1-model-training-process","link":"#_3-1-model-training-process","children":[]},{"level":3,"title":"3.2. Important Hyperparameter -- Batch Size","slug":"_3-2-important-hyperparameter-batch-size","link":"#_3-2-important-hyperparameter-batch-size","children":[]}]},{"level":2,"title":"4. Memory Usage in Model Training","slug":"_4-memory-usage-in-model-training","link":"#_4-memory-usage-in-model-training","children":[]},{"level":2,"title":"5. Mixed Precision Training","slug":"_5-mixed-precision-training","link":"#_5-mixed-precision-training","children":[{"level":3,"title":"5.1. Numerical Range and Precision of Floating-Point Numbers","slug":"_5-1-numerical-range-and-precision-of-floating-point-numbers","link":"#_5-1-numerical-range-and-precision-of-floating-point-numbers","children":[]},{"level":3,"title":"5.2. Concept of Mixed Precision Training","slug":"_5-2-concept-of-mixed-precision-training","link":"#_5-2-concept-of-mixed-precision-training","children":[]},{"level":3,"title":"5.3. Summary of Known Methods for Mixed Precision Training","slug":"_5-3-summary-of-known-methods-for-mixed-precision-training","link":"#_5-3-summary-of-known-methods-for-mixed-precision-training","children":[]},{"level":3,"title":"5.4. FP16 and BF16 Training","slug":"_5-4-fp16-and-bf16-training","link":"#_5-4-fp16-and-bf16-training","children":[]},{"level":3,"title":"5.5. FP8 Training","slug":"_5-5-fp8-training","link":"#_5-5-fp8-training","children":[]}]},{"level":2,"title":"6. Activation Recomputation / Gradient Checkpointing / Rematerialization","slug":"_6-activation-recomputation-gradient-checkpointing-rematerialization","link":"#_6-activation-recomputation-gradient-checkpointing-rematerialization","children":[]},{"level":2,"title":"7. Gradient Accumulation","slug":"_7-gradient-accumulation","link":"#_7-gradient-accumulation","children":[]},{"level":2,"title":"8. Tools","slug":"_8-tools","link":"#_8-tools","children":[]},{"level":2,"title":"8.1. Memory Usage Calculation Tool: Predict Memory","slug":"_8-1-memory-usage-calculation-tool-predict-memory","link":"#_8-1-memory-usage-calculation-tool-predict-memory","children":[]},{"level":2,"title":"8.2. Distributed Training Tool for Visualizing GPU Compute and Communication Costs: Profiler","slug":"_8-2-distributed-training-tool-for-visualizing-gpu-compute-and-communication-costs-profiler","link":"#_8-2-distributed-training-tool-for-visualizing-gpu-compute-and-communication-costs-profiler","children":[]},{"level":2,"title":"9. Reference: Ultrascale Playbook","slug":"_9-reference-ultrascale-playbook","link":"#_9-reference-ultrascale-playbook","children":[{"level":3,"title":"9.1. Overview","slug":"_9-1-overview","link":"#_9-1-overview","children":[]},{"level":3,"title":"9.2. Prerequisite Knowledge","slug":"_9-2-prerequisite-knowledge","link":"#_9-2-prerequisite-knowledge","children":[]},{"level":3,"title":"9.3. Scaling Experiments","slug":"_9-3-scaling-experiments","link":"#_9-3-scaling-experiments","children":[]}]}],"git":{"createdTime":1741444326000,"updatedTime":1741444326000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":9.42,"words":2826},"filePathRelative":"posts/llm/024_distribution_and_parallelism.md","localizedDate":"February 26, 2025","excerpt":"<h1> Distributed Training Part 1: Memory Usage in Model Training</h1>\\n","autoDesc":true}');export{e as data};
