const i=JSON.parse('{"key":"v-2131a5fc","path":"/posts/llm/024_distribution_and_parallelism.html","title":"Distributed Training Part 1: Memory Usage in Model Training","lang":"en-US","frontmatter":{"icon":"lightbulb","sidebar":false,"date":"2025-02-26T00:00:00.000Z","prev":"./025_distribution_and_parallelism_1","next":"./023_agent_framework","category":["LLM"],"tag":["Distributed","Parallel"],"description":"Distributed Training Part 1: Memory Usage in Model Training Model Training Process and Important Hyperparameter Memory Usage in Model Training Memory Optimization Suggestions Activation Recomputation / Gradient Checkpointing Gradient Accumulation Mixed Precision Training","head":[["link",{"rel":"alternate","hreflang":"zh-cn","href":"https://liz-in-tech.github.io/blog/zh/posts/llm/024_distribution_and_parallelism.html"}],["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/blog/posts/llm/024_distribution_and_parallelism.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"Distributed Training Part 1: Memory Usage in Model Training"}],["meta",{"property":"og:description","content":"Distributed Training Part 1: Memory Usage in Model Training Model Training Process and Important Hyperparameter Memory Usage in Model Training Memory Optimization Suggestions Activation Recomputation / Gradient Checkpointing Gradient Accumulation Mixed Precision Training"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:locale:alternate","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-04-10T09:24:41.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:tag","content":"Distributed"}],["meta",{"property":"article:tag","content":"Parallel"}],["meta",{"property":"article:published_time","content":"2025-02-26T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-04-10T09:24:41.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Distributed Training Part 1: Memory Usage in Model Training\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-02-26T00:00:00.000Z\\",\\"dateModified\\":\\"2025-04-10T09:24:41.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. Metrics","slug":"_1-metrics","link":"#_1-metrics","children":[]},{"level":2,"title":"2. Three Key Challenges","slug":"_2-three-key-challenges","link":"#_2-three-key-challenges","children":[]},{"level":2,"title":"3. Basics of Model Training","slug":"_3-basics-of-model-training","link":"#_3-basics-of-model-training","children":[{"level":3,"title":"3.1. Model Training Process","slug":"_3-1-model-training-process","link":"#_3-1-model-training-process","children":[]},{"level":3,"title":"3.2. Important Hyperparameter -- Batch Size","slug":"_3-2-important-hyperparameter-batch-size","link":"#_3-2-important-hyperparameter-batch-size","children":[]}]},{"level":2,"title":"4. Memory Usage in Model Training","slug":"_4-memory-usage-in-model-training","link":"#_4-memory-usage-in-model-training","children":[]},{"level":2,"title":"5. Memory Optimization Suggestions","slug":"_5-memory-optimization-suggestions","link":"#_5-memory-optimization-suggestions","children":[]},{"level":2,"title":"6. Activation Recomputation / Gradient Checkpointing / Rematerialization","slug":"_6-activation-recomputation-gradient-checkpointing-rematerialization","link":"#_6-activation-recomputation-gradient-checkpointing-rematerialization","children":[]},{"level":2,"title":"7. Gradient Accumulation","slug":"_7-gradient-accumulation","link":"#_7-gradient-accumulation","children":[]},{"level":2,"title":"8. Mixed Precision Training","slug":"_8-mixed-precision-training","link":"#_8-mixed-precision-training","children":[{"level":3,"title":"8.1. Numerical Range and Precision of Floating-Point Numbers","slug":"_8-1-numerical-range-and-precision-of-floating-point-numbers","link":"#_8-1-numerical-range-and-precision-of-floating-point-numbers","children":[]},{"level":3,"title":"8.2. Concept of Mixed Precision Training","slug":"_8-2-concept-of-mixed-precision-training","link":"#_8-2-concept-of-mixed-precision-training","children":[]},{"level":3,"title":"8.3. Summary of Known Methods for Mixed Precision Training","slug":"_8-3-summary-of-known-methods-for-mixed-precision-training","link":"#_8-3-summary-of-known-methods-for-mixed-precision-training","children":[]},{"level":3,"title":"8.4. FP16 and BF16 Training","slug":"_8-4-fp16-and-bf16-training","link":"#_8-4-fp16-and-bf16-training","children":[]},{"level":3,"title":"8.5. FP8 Training","slug":"_8-5-fp8-training","link":"#_8-5-fp8-training","children":[]}]},{"level":2,"title":"9. Tools","slug":"_9-tools","link":"#_9-tools","children":[]},{"level":2,"title":"9.1. Memory Usage Calculation Tool: Predict Memory","slug":"_9-1-memory-usage-calculation-tool-predict-memory","link":"#_9-1-memory-usage-calculation-tool-predict-memory","children":[]},{"level":2,"title":"9.2. Distributed Training Tool for Visualizing GPU Compute and Communication Costs: Profiler","slug":"_9-2-distributed-training-tool-for-visualizing-gpu-compute-and-communication-costs-profiler","link":"#_9-2-distributed-training-tool-for-visualizing-gpu-compute-and-communication-costs-profiler","children":[]},{"level":2,"title":"10. Reference: Ultrascale Playbook","slug":"_10-reference-ultrascale-playbook","link":"#_10-reference-ultrascale-playbook","children":[{"level":3,"title":"10.1. Overview","slug":"_10-1-overview","link":"#_10-1-overview","children":[]},{"level":3,"title":"10.2. Prerequisite Knowledge","slug":"_10-2-prerequisite-knowledge","link":"#_10-2-prerequisite-knowledge","children":[]},{"level":3,"title":"10.3. Scaling Experiments","slug":"_10-3-scaling-experiments","link":"#_10-3-scaling-experiments","children":[]}]}],"git":{"createdTime":1741444326000,"updatedTime":1744277081000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro-2.local","commits":1},{"name":"liz","email":"liz@MacBook-Pro-4.local","commits":1},{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":9.5,"words":2851},"filePathRelative":"posts/llm/024_distribution_and_parallelism.md","localizedDate":"February 26, 2025","excerpt":"<h1> Distributed Training Part 1: Memory Usage in Model Training</h1>\\n<ul>\\n<li>Model Training Process and Important Hyperparameter</li>\\n<li>Memory Usage in Model Training</li>\\n<li>Memory Optimization Suggestions\\n<ul>\\n<li>Activation Recomputation / Gradient Checkpointing</li>\\n<li>Gradient Accumulation</li>\\n<li>Mixed Precision Training</li>\\n</ul>\\n</li>\\n</ul>\\n","autoDesc":true}');export{i as data};
