import{_ as t,a as r,b as s,c as o,d,e as c,f as u,g as m}from"./018_huggingface_task-I5iSVkQS.js";import{_ as g}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as p,o as f,c as h,f as b,a as e,b as i,d as n,e as l}from"./app-eQeDSkHB.js";const v={},_=e("h1",{id:"hugging-face-and-transformers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#hugging-face-and-transformers","aria-hidden":"true"},"#"),i(" Hugging Face and Transformers")],-1),y=e("h2",{id:"_1-official-website",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_1-official-website","aria-hidden":"true"},"#"),i(" 1. Official Website")],-1),x={href:"https://huggingface.co/",target:"_blank",rel:"noopener noreferrer"},k=e("p",null,"Introduction: Hugging Face is an open-source platform focused on natural language processing (NLP) and artificial intelligence, offering a wealth of tools and resources. It hosts the most active, popular, and influential LLM community, where the latest and most powerful LLMs are often released and open-sourced.",-1),w=e("h2",{id:"_2-main-features",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-main-features","aria-hidden":"true"},"#"),i(" 2. Main Features")],-1),q=l("<li>Models Library <ul><li>Model download</li><li>Model sharing</li></ul></li><li>Datasets Library <ul><li>Dataset download</li><li>Dataset sharing</li></ul></li><li>Spaces <ul><li>Contains recent interesting and popular applications</li><li>Supports tools like Gradio and Streamlit for quickly building visual interfaces</li><li>Used to showcase model effects, test model performance, or as teaching tools</li></ul></li><li>Trending <ul><li>Displays the most popular Models, Datasets, and Spaces from the last 7 days on the homepage</li></ul></li><li>Docs Library <ul><li>Documentation for Hugging Face&#39;s core libraries and model algorithms</li></ul></li><li>Core Libraries <ul><li>Transformers Library <ul><li>Hugging Face&#39;s core open-source library, providing APIs and tools to easily download and train state-of-the-art pre-trained models, supporting multiple deep learning frameworks (PyTorch, TensorFlow, and JAX), with PyTorch supporting all models and frameworks.</li></ul></li><li>Datasets Library <ul><li>Offers the largest ready-to-use machine learning dataset hub with fast, easy-to-use, and efficient data manipulation tools, providing a unified API to access and manage various machine learning datasets, facilitating the handling of different data sources.</li></ul></li><li>Tokenizers Library <ul><li>A library for creating and using tokenizers, crucial for text processing in natural language processing models, offering high performance and flexibility.</li></ul></li><li>Evaluate Library <ul><li>A library for evaluating and comparing model performance, providing a simple unified API for evaluating machine learning models on different tasks and metrics, facilitating benchmarking and comparison of different models.</li></ul></li><li>PEFT Library <ul><li>Parameter-efficient fine-tuning library, efficiently fine-tuning large language models by updating only a small portion of model parameters, helping adapt to new tasks with limited computational resources.</li></ul></li><li>TRL (Transformer Reinforcement Learning) Library <ul><li>Reinforcement learning library, providing tools to train large language models with reinforcement learning techniques, facilitating supervised fine-tuning, reward modeling, and proximal policy optimization (PPO) for further fine-tuning of large language models.</li></ul></li><li>Accelerate Library <ul><li>Distributed training and inference library, enabling the same code to run on various distributed configurations (e.g., single GPU, multi-node clusters) with minimal modifications, supporting automatic mixed precision and distributed training, enhancing the efficiency of large-scale machine learning tasks.</li></ul></li><li>Optimum Library <ul><li>Optimization acceleration library, using easy-to-use hardware optimization tools to accelerate model inference and training, providing tools to optimize model performance on various hardware, including multi-GPU systems and TPUs, facilitating scaling and accelerating model training and inference.</li></ul></li><li>Gradio Library <ul><li>Used to create user interfaces for machine learning models, allowing developers to quickly build and share interactive machine learning applications, with free hosting of demos through Hugging Face Spaces.</li></ul></li></ul></li><li>Daily Papers <ul><li>Provides the latest research papers in the AI field, allowing users to interact with authors and recommend related papers.</li></ul></li>",7),T={href:"https://huggingface.co/learn",target:"_blank",rel:"noopener noreferrer"},z=e("li",null,"Community Forum",-1),A=l('<h2 id="_3-models" tabindex="-1"><a class="header-anchor" href="#_3-models" aria-hidden="true">#</a> 3. Models</h2><h3 id="_3-1-model-list-page" tabindex="-1"><a class="header-anchor" href="#_3-1-model-list-page" aria-hidden="true">#</a> 3.1. Model list page</h3><ul><li>Covers various tasks, allowing selection of models based on tasks or direct search by model name <ul><li>Multimodal</li><li>Computer Vision</li><li>Natural Language Processing</li><li>Audio</li><li>Tabular</li><li>Reinforcement Learning</li></ul></li><li>Models without a prefix are provided by Hugging Face, while those with a prefix are third-party models</li></ul><figure><img src="'+t+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_3-2-model-detail-page" tabindex="-1"><a class="header-anchor" href="#_3-2-model-detail-page" aria-hidden="true">#</a> 3.2. Model detail page</h3><ul><li>Model card: Model introduction</li><li>File and versions: Model files</li><li>Use this model: Sample code for using the model, usually providing a pipeline method</li><li>Inference Providers: Interface provision</li><li>Model tree: Model variants</li><li>Spaces using model: Spaces using the model</li></ul><figure><img src="'+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_3-3-model-files" tabindex="-1"><a class="header-anchor" href="#_3-3-model-files" aria-hidden="true">#</a> 3.3. Model Files</h3><figure><img src="'+s+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Documentation/Metadata: Provides usage guidelines, licensing, and version control <ul><li>README.md: The model’s documentation, usually containing an introduction, usage instructions, training details, and performance evaluation</li><li>LICENSE: The open-source license agreement, specifying the model’s usage, distribution, and modification permissions</li><li>.gitattributes: Used for Git version control, especially for Git LFS (Large File Storage), typically containing tracking rules for large files</li></ul></li><li>Model Configuration Files: Define model architecture and inference parameters <ul><li>config.json: The model’s primary configuration file, specifying architecture details such as the number of layers, hidden units, and attention heads</li><li>generation_config.json: The default parameter configuration file for text generation, including settings such as maximum length, temperature, and top-k sampling, which control the inference process</li></ul></li><li>Model Weight Files: Store the neural network parameters <ul><li>safetensors files: The model’s main weight files, stored in the safetensors format (safer and faster to load than traditional .bin), managed via Git LFS <ul><li>model-00001-of-000002.safetensors</li><li>model-00002-of-000002.safetensors</li></ul></li><li>model.safetensors.index.json: The index file, mapping model weights across different .safetensors files, enabling sharded model loading</li></ul></li><li>Tokenizer: Used for text preprocessing and postprocessing <ul><li>tokenizer.json: The core tokenizer file, containing all tokens and their corresponding IDs for text encoding and decoding</li><li>tokenizer_config.json: Defines tokenizer-related parameters, such as the use of special tokens and whether it follows BPE/WordPiece tokenization</li></ul></li></ul><h2 id="_4-datasets" tabindex="-1"><a class="header-anchor" href="#_4-datasets" aria-hidden="true">#</a> 4. Datasets</h2><p>Dataset list page</p><figure><img src="'+o+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Dataset detail page</p><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_5-trending" tabindex="-1"><a class="header-anchor" href="#_5-trending" aria-hidden="true">#</a> 5. Trending</h2><p>Trending showcases recent popular models, datasets, and shared spaces</p><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_6-transformers" tabindex="-1"><a class="header-anchor" href="#_6-transformers" aria-hidden="true">#</a> 6. Transformers</h2><h3 id="_6-1-transformers-api" tabindex="-1"><a class="header-anchor" href="#_6-1-transformers-api" aria-hidden="true">#</a> 6.1. Transformers API</h3><p>The API describes all classes and functions:</p><ul><li>MAIN CLASSES: Detailed introduction of the most important classes, such as configuration, model, tokenizer, and pipeline <ul><li>Pipelines: Provides a high-level API to simplify the execution of common tasks</li><li>AutoClasses: Provides a unified API to automatically load models, tokenizers, and configurations</li><li>Configuration: Defines model parameters and structure, ensuring correct model initialization</li><li>Models: Models</li><li>Tokenizer: Tokenizer, preprocessing data, converting text to token sequences and vice versa</li><li>Trainer: Supports model training and fine-tuning, providing a complete PyTorch training API, supporting distributed training and mixed precision</li></ul></li><li>MODELS: Detailed introduction of classes and functions related to each model implemented in the library <ul><li>text models</li><li>vision models</li><li>audio models</li><li>video models</li><li>multimodal models</li><li>reinforcement learning models</li><li>time series models</li><li>graph models</li></ul></li><li>INTERNAL HELPERS: Detailed introduction of utility classes and functions used internally</li></ul><h3 id="_6-2-pipelines" tabindex="-1"><a class="header-anchor" href="#_6-2-pipelines" aria-hidden="true">#</a> 6.2. Pipelines</h3><h4 id="_6-2-1-introduction-to-pipelines" tabindex="-1"><a class="header-anchor" href="#_6-2-1-introduction-to-pipelines" aria-hidden="true">#</a> 6.2.1. Introduction to Pipelines</h4><p>Pipeline is a high-level encapsulation class in the Transformers library, assembling data preprocessing, model invocation, and result post-processing into a pipeline.</p><figure><img src="'+u+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>pipeline() is the simplest and fastest way to perform inference using pre-trained models. It can be used for many tasks in different modes, as shown in the table below:</p><figure><img src="'+m+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="_6-2-2-using-pipelines" tabindex="-1"><a class="header-anchor" href="#_6-2-2-using-pipelines" aria-hidden="true">#</a> 6.2.2. Using Pipelines</h4><p>1 parameter: Specify only the task type</p><ul><li>If no model is specified, the default model and accompanying tokenizer for the target task will be downloaded</li></ul><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>from transformers import pipeline

pipe = pipeline(&quot;text-classification&quot;)
pipe(&quot;This restaurant is awesome&quot;)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>1 parameter: Specify only the model name</p><ul><li>The model has a corresponding task type and accompanying tokenizer</li></ul><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>from transformers import pipeline

pipe = pipeline(model=&quot;FacebookAI/roberta-large-mnli&quot;)
pipe(&quot;This restaurant is awesome&quot;)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>2 parameters: Specify task type and model name</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>from transformers import pipeline

messages = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who are you?&quot;},
]
pipe = pipeline(&quot;text-generation&quot;, model=&quot;Qwen/QwQ-32B&quot;)
pipe(messages)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>3 parameters: Specify task type, model name, and embedding model name</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>from transformers import pipeline

oracle = pipeline(
    &quot;question-answering&quot;, model=&quot;distilbert/distilbert-base-cased-distilled-squad&quot;, tokenizer=&quot;google-bert/bert-base-cased&quot;
)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>3 parameters: Load model first, then create Pipeline</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code># Named entity recognition pipeline, passing in a specific model and tokenizer

from transformers import *

model = AutoModelForTokenClassification.from_pretrained(&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;google-bert/bert-base-cased&quot;)
recognizer = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>Generally, the pipeline will load the model the first time it is executed, and the model will be automatically downloaded locally for direct use</li><li>The way the pipeline loads models and the way models are loaded before creating a Pipeline are consistent, both using the Auto Classes method</li></ul>`,42),P={href:"https://huggingface.co/docs/transformers/main_classes/pipelines#pipelines",target:"_blank",rel:"noopener noreferrer"},M=l(`<h3 id="_6-3-autoclasses" tabindex="-1"><a class="header-anchor" href="#_6-3-autoclasses" aria-hidden="true">#</a> 6.3. AutoClasses</h3><ul><li>AutoClasses can automatically load Model, Tokenizer, and Config by passing parameters to the from_pretrained() method, i.e., by the name or path of the pre-trained model, automatically retrieving the architecture of the pre-trained model.</li><li>Mainly includes AutoConfig, AutoModel, and AutoTokenizer</li></ul><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>from transformers import AutoConfig

config = AutoConfig.from_pretrained(&quot;google-bert/bert-base-uncased&quot;)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>from transformers import AutoModel

model = AutoModel.from_pretrained(&quot;google-bert/bert-base-cased&quot;)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;google-bert/bert-base-uncased&quot;)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>Task-specific AutoModel classes (e.g., AutoModelForSequenceClassification), where the suffix ForSequenceClassification indicates the task type. The reason for task-specific AutoModel classes is that the backbone model can handle multiple tasks: the same model can be used for different tasks by connecting different post-processing modules.</li><li>These classes provide a unified API, making the code more flexible and suitable for different models, and they are very effective in simplifying model usage.</li></ul>`,6),L={href:"https://huggingface.co/docs/transformers/model_doc/auto",target:"_blank",rel:"noopener noreferrer"};function S(C,I){const a=p("ExternalLinkIcon");return f(),h("div",null,[_,b(" more "),y,e("p",null,[i("Official website: "),e("a",x,[i("https://huggingface.co/"),n(a)])]),k,w,e("ul",null,[q,e("li",null,[i("Learn Courses "),e("ul",null,[e("li",null,[e("a",T,[i("https://huggingface.co/learn"),n(a)])])])]),z]),A,e("p",null,[i("Official Pipeline documentation: "),e("a",P,[i("https://huggingface.co/docs/transformers/main_classes/pipelines#pipelines"),n(a)])]),M,e("p",null,[i("Official Auto Classes documentation: "),e("a",L,[i("https://huggingface.co/docs/transformers/model_doc/auto"),n(a)])])])}const N=g(v,[["render",S],["__file","018_huggingface.html.vue"]]);export{N as default};
