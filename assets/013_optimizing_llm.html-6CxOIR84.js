const e=JSON.parse('{"key":"v-672d81ad","path":"/posts/llm/013_optimizing_llm.html","title":"Best Practices for Optimizing LLMs (Prompt Engineering, RAG and Fine-tuning)","lang":"en-US","frontmatter":{"icon":"lightbulb","sidebar":false,"date":"2024-11-02T00:00:00.000Z","prev":"./014_rag_evaluation","next":"./012_prompt_engineering","category":["LLM"],"tag":["LLM","Prompt Engineering","RAG","Fine-tuning"],"description":"Best Practices for Optimizing LLMs (Prompt Engineering, RAG and Fine-tuning) The Optimization Strategies Typical Optimization Pipeline Comparison of Optimisation Approaches OpenAI RAG Use Case","head":[["link",{"rel":"alternate","hreflang":"zh-cn","href":"https://liz-in-tech.github.io/blog/zh/posts/llm/013_optimizing_llm.html"}],["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/blog/posts/llm/013_optimizing_llm.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"Best Practices for Optimizing LLMs (Prompt Engineering, RAG and Fine-tuning)"}],["meta",{"property":"og:description","content":"Best Practices for Optimizing LLMs (Prompt Engineering, RAG and Fine-tuning) The Optimization Strategies Typical Optimization Pipeline Comparison of Optimisation Approaches OpenAI RAG Use Case"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:locale:alternate","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-12-20T00:19:34.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:tag","content":"Prompt Engineering"}],["meta",{"property":"article:tag","content":"RAG"}],["meta",{"property":"article:tag","content":"Fine-tuning"}],["meta",{"property":"article:published_time","content":"2024-11-02T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-12-20T00:19:34.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Best Practices for Optimizing LLMs (Prompt Engineering, RAG and Fine-tuning)\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-11-02T00:00:00.000Z\\",\\"dateModified\\":\\"2024-12-20T00:19:34.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. The Challenges of Optimizing LLMs","slug":"_1-the-challenges-of-optimizing-llms","link":"#_1-the-challenges-of-optimizing-llms","children":[]},{"level":2,"title":"2. The Optimization Strategies","slug":"_2-the-optimization-strategies","link":"#_2-the-optimization-strategies","children":[]},{"level":2,"title":"3. Typical Optimization Pipeline","slug":"_3-typical-optimization-pipeline","link":"#_3-typical-optimization-pipeline","children":[]},{"level":2,"title":"4. Comparison of Optimisation Approaches","slug":"_4-comparison-of-optimisation-approaches","link":"#_4-comparison-of-optimisation-approaches","children":[]},{"level":2,"title":"5. Optimization via Prompt Engineering","slug":"_5-optimization-via-prompt-engineering","link":"#_5-optimization-via-prompt-engineering","children":[]},{"level":2,"title":"6. Benefits of Using RAG","slug":"_6-benefits-of-using-rag","link":"#_6-benefits-of-using-rag","children":[]},{"level":2,"title":"7. Optimization via Fine-Tuning","slug":"_7-optimization-via-fine-tuning","link":"#_7-optimization-via-fine-tuning","children":[]},{"level":2,"title":"8. Best Practices in Fine-Tuning","slug":"_8-best-practices-in-fine-tuning","link":"#_8-best-practices-in-fine-tuning","children":[]},{"level":2,"title":"9. Best Practices in Fine-Tuning + RAG","slug":"_9-best-practices-in-fine-tuning-rag","link":"#_9-best-practices-in-fine-tuning-rag","children":[]},{"level":2,"title":"10. OpenAI RAG Use Case","slug":"_10-openai-rag-use-case","link":"#_10-openai-rag-use-case","children":[{"level":3,"title":"10.1. Experiments that didnâ€™t work","slug":"_10-1-experiments-that-didn-t-work","link":"#_10-1-experiments-that-didn-t-work","children":[]},{"level":3,"title":"10.2. Experiments which worked","slug":"_10-2-experiments-which-worked","link":"#_10-2-experiments-which-worked","children":[]}]},{"level":2,"title":"11. OpenAI Fine-Tuning + RAG Use Case","slug":"_11-openai-fine-tuning-rag-use-case","link":"#_11-openai-fine-tuning-rag-use-case","children":[]},{"level":2,"title":"12. Reference","slug":"_12-reference","link":"#_12-reference","children":[]}],"git":{"createdTime":1730566096000,"updatedTime":1734653974000,"contributors":[{"name":"unknown","email":"15721607377@163.com","commits":4}]},"readingTime":{"minutes":4.5,"words":1351},"filePathRelative":"posts/llm/013_optimizing_llm.md","localizedDate":"November 2, 2024","excerpt":"<h1> Best Practices for Optimizing LLMs (Prompt Engineering, RAG and Fine-tuning)</h1>\\n<ul>\\n<li>The Optimization Strategies</li>\\n<li>Typical Optimization Pipeline</li>\\n<li>Comparison of Optimisation Approaches</li>\\n<li>OpenAI RAG Use Case</li>\\n</ul>\\n","autoDesc":true}');export{e as data};
