import{_ as s,a as n,b as o,c as r,d as u,e as m,f as d,g,h as p,i as c,j as h,k as f,l as _,m as b,n as y,o as x,p as v,q as T}from"./016_sora-sk0IGMY5.js";import{_ as M}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as I,o as w,c as k,f as S,a as e,b as i,d as a,e as t}from"./app-NIm1MqEj.js";const L={},A=e("h1",{id:"multimodal-large-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#multimodal-large-models","aria-hidden":"true"},"#"),i(" Multimodal Large Models")],-1),G=e("ul",null,[e("li",null,"Multimodal Tasks"),e("li",null,"Training of Multimodal Large Models"),e("li",null,"Stable Diffusion"),e("li",null,"LLaVA")],-1),P=t('<h2 id="_1-multimodal" tabindex="-1"><a class="header-anchor" href="#_1-multimodal" aria-hidden="true">#</a> 1. Multimodal</h2><p>Modality: The type of signal (or type/form of data)</p><ul><li>Text</li><li>Image</li><li>Video</li><li>Audio</li><li>Further subdivisions <ul><li>Graph</li><li>Table</li></ul></li></ul><p>Multimodal: Designing with two or more different types of modalities (real-world scenarios often involve multiple signals).</p><p>Multimodal Model: An AI model capable of processing and integrating multiple modality data.</p><p>Multimodal System: A system that can handle multiple types of input and output modalities.</p><p>Multimodal Large Models (MLLMs): Large language models (LLMs) extended to handle multiple data types by incorporating additional modalities.</p><p>Multimodal large models are used to solve: Conversion between modalities</p><ul><li>Text → Image</li><li>Text → Video</li><li>Text → Table</li><li>Text → Graph</li><li>Image/Video → Text</li></ul><h2 id="_2-multimodal-tasks" tabindex="-1"><a class="header-anchor" href="#_2-multimodal-tasks" aria-hidden="true">#</a> 2. Multimodal Tasks</h2><p>The two most prominent modalities are language and vision. Related tasks can be divided into two categories: Generation and Vision-Language Understanding.</p>',11),C=e("li",null,"Proposed by OpenAI",-1),D=e("li",null,"API available",-1),z={href:"https://platform.openai.com/docs/guides/images?context=node",target:"_blank",rel:"noopener noreferrer"},V=e("li",null,"Produces the best image effects",-1),N=e("li",null,"Paid service",-1),E={href:"https://docs.midjourney.com/",target:"_blank",rel:"noopener noreferrer"},R={href:"https://midlibrary.io/",target:"_blank",rel:"noopener noreferrer"},O=e("li",null,"Open-source, can run on personal PCs",-1),F={href:"https://github.com/AUTOMATIC1111/stable-diffusion-webui",target:"_blank",rel:"noopener noreferrer"},q={href:"https://civitai.com/",target:"_blank",rel:"noopener noreferrer"},B={href:"https://github.com/VinsonLaro/stable-diffusion-webui-chinese",target:"_blank",rel:"noopener noreferrer"},H={href:"https://github.com/Mikubill/sd-webui-controlnet",target:"_blank",rel:"noopener noreferrer"},U=e("li",null,"OpenPose: Generate images based on human pose skeletons",-1),W=e("li",null,"Canny Edge Detection: Generate images based on sketch contours",-1),Z=e("li",null,"Modify Image: Input original image and text to generate a modified image",-1),j=t("<li>Understanding <ul><li>Visual Question Answering <ul><li>Input image and text, and answer questions based on the image-text pair</li></ul></li><li>Image Captioning <ul><li>Input image and generate a textual description of the image</li></ul></li><li>Image Classification <ul><li>For example, OCR to extract text from images or classify images</li></ul></li><li>Text-based Image Retrieval <ul><li>Method 1: Generate image descriptions, and when users input text queries, find the image description that matches the query text, then retrieve the corresponding image <ul><li><strong>BLIP Model</strong>: Converts images into textual descriptions</li></ul></li><li>Method 2: Train a joint vector space for image and text, generate a vector for the input text, and find the image with the most similar vector <ul><li><strong>CLIP Model</strong>: Maps images and text into a shared vector space</li></ul></li></ul></li></ul></li>",1),Q={href:"https://github.com/AIGC-Audio/AudioGPT",target:"_blank",rel:"noopener noreferrer"},X={href:"https://github.com/rany2/edge-tts",target:"_blank",rel:"noopener noreferrer"},J=e("li",null,"Uses Microsoft Edge's online text-to-speech service without needing Microsoft Edge, Windows, or API keys",-1),K={href:"https://github.com/myshell-ai/MeloTTS",target:"_blank",rel:"noopener noreferrer"},Y={href:"https://huggingface.co/spaces/mrfakename/MeloTTS",target:"_blank",rel:"noopener noreferrer"},$=e("li",null,"High-quality multilingual TTS library by MyShell.ai. Supports English, Spanish, French, Chinese, Japanese, and Korean.",-1),ee=e("li",null,[i("Features "),e("ul",null,[e("li",null,"Fast, real-time speech synthesis even on CPUs"),e("li",null,"Multilingual support"),e("li",null,"Chinese-English mixed language support"),e("li",null,"Easy installation")])],-1),ie=e("li",null,"Speech-to-Text (STT), Automatic Speech Recognition (ASR)",-1),le={href:"https://github.com/RVC-Boss/GPT-SoVITS",target:"_blank",rel:"noopener noreferrer"},ae={href:"https://www.suno.ai/",target:"_blank",rel:"noopener noreferrer"},te=t('<h2 id="_3-stable-diffusion-sd" tabindex="-1"><a class="header-anchor" href="#_3-stable-diffusion-sd" aria-hidden="true">#</a> 3. Stable Diffusion (SD)</h2><figure><img src="'+s+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+n+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+o+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Stable Diffusion is a system composed of multiple components and models, rather than a single model.</p><ul><li>Text Encoder <ul><li>Vectorizes text to capture the semantic information in the text</li></ul></li><li>Image Generator <ul><li>The image generator works entirely in the latent space of image information, which makes diffusion models faster compared to earlier pixel-space diffusion models.</li><li>Components <ul><li>Image Information Creator</li><li>Image Decoder</li></ul></li></ul></li></ul><figure><img src="'+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+u+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+m+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Three main components of the diffusion model (each with its own neural network):</p><ul><li>Clip Text (blue) <ul><li>For text encoding</li><li>Input: Text</li><li>Output: 77 feature vectors, each with 768 dimensions</li></ul></li><li>Grid Network + UNet + Scheduler (pink) <ul><li>Gradually diffuses information into the latent space</li><li>Diffusion process: Step-by-step transformation of information, gradually adding more related information until a high-quality image is generated.</li><li>The diffusion process contains multiple steps, each processing the latent matrix and generating a new latent matrix to better match the &quot;input text&quot; and &quot;visual information&quot; from the model&#39;s image bank.</li><li>Input: Text vector and random initial image information matrix (latent)</li><li>Output: Processed information array (dimension: (4, 64, 64))</li></ul></li><li>Autoencoder Decoder (orange) <ul><li>Draws the final image</li><li>Input: Processed information array</li><li>Output: Resulting image (dimension: (3, 512, 512))</li></ul></li></ul><p>Diffusion model working principle:</p><ul><li>Forward Diffusion <ul><li>Adds noise to training images, gradually turning them into noise images without distinct features.</li></ul></li><li>Reverse Diffusion <ul><li>Starts with noisy, meaningless images and gradually restores images of cats or dogs by reversing the diffusion process.</li></ul></li></ul><figure><img src="'+h+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_4-training-of-multimodal-large-models" tabindex="-1"><a class="header-anchor" href="#_4-training-of-multimodal-large-models" aria-hidden="true">#</a> 4. Training of Multimodal Large Models</h2><ul><li>Traditional Training Methods <ul><li>End-to-End Training <ul><li>Image Captioning Task: Image → Description/Caption</li><li>Image → CNN → Vector (shared) → RNN/LSTM → Text (Encoder-Decoder structure)</li><li>Training Data: (Image1, Des1), (Image2, Des2), ..., (ImageN, DesN)</li></ul></li><li>Problems <ul><li>Training from scratch is costly</li><li>Each task requires large datasets, and manually annotating data is difficult without large models</li></ul></li></ul></li><li>Training Multimodal Large Models <ul><li>Foundation Models <ul><li>Text Domain <ul><li>GPT-4, LLaMA, ChatGLM, Qwen</li></ul></li><li>Image Domain <ul><li>CLIP</li></ul></li><li>Video Domain <ul><li>Sora</li></ul></li><li>Graph Domain <ul><li>GNN</li></ul></li></ul></li><li>Multimodal Systems <ul><li>Text (language models) as the intermediary (since all other modalities reduce to expressing meaning)</li><li>Image/Video/Graph → Each modality has its adapter → Align Language model, Image model, Video model, and Graph model (making modalities aligned)</li><li>Advantages <ul><li>Lower training costs <ul><li>Only the adapters for each modality are trained, while the foundation model parameters remain frozen <ul><li>Stage 1: Pre-training for Feature Alignment <ul><li>Only the adapter parts are updated</li><li>This step is necessary because the adapters are newly introduced and initially have no effect</li><li>Large amounts of data are required for this stage</li></ul></li><li>Stage 2: Fine-tuning End-to-End <ul><li>Both the adapter and the language model parts are updated</li><li>This stage requires a smaller amount of data</li></ul></li></ul></li></ul></li><li>Easier adaptation for each task</li></ul></li></ul></li></ul></li></ul><figure><img src="'+f+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_5-flamingo" tabindex="-1"><a class="header-anchor" href="#_5-flamingo" aria-hidden="true">#</a> 5. Flamingo</h2><p>Github: https://github.com/lucidrains/flamingo-pytorch</p><figure><img src="'+_+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Input: image + text + image + text (Images and text alternate)</li><li>Vision Encoder: Base model for processing images - Foundation Models</li><li>Perceiver Resampler: Adapter</li><li>LM block: Language Model</li></ul><h2 id="_6-llava" tabindex="-1"><a class="header-anchor" href="#_6-llava" aria-hidden="true">#</a> 6. LLaVA</h2><p>Github: https://github.com/haotian-liu/LLaVA</p><p>Paper Name: Visual Instruction Tuning Paper: https://arxiv.org/pdf/2304.08485</p><figure><img src="'+b+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>',29),se=e("ul",null,[e("li",null,[i("Vision Encoder: Base model for processing images - Clip "),e("ul",null,[e("li",null,[i("provides the visual feature "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"Z"),e("mi",null,"v")]),e("mo",null,"="),e("mi",null,"g"),e("mo",{stretchy:"false"},"("),e("msub",null,[e("mi",null,"X"),e("mi",null,"v")]),e("mo",{stretchy:"false"},")")]),e("annotation",{encoding:"application/x-tex"},"Z_v = g(X_v)")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"Z"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"-0.0715em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"v")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.15em"}},[e("span")])])])])]),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),e("span",{class:"mrel"},"="),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),e("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"g"),e("span",{class:"mopen"},"("),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"-0.0785em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"v")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.15em"}},[e("span")])])])])]),e("span",{class:"mclose"},")")])])])])])]),e("li",null,[i("Projection W: Adapter, converts to a vector with the same dimension as text "),e("ul",null,[e("li",null,[i("apply a trainable projection matrix W to convert "),e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"Z"),e("mi",null,"v")])]),e("annotation",{encoding:"application/x-tex"},"Z_v")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"Z"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"-0.0715em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"v")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.15em"}},[e("span")])])])])])])])]),i(" into language embedding tokens H")]),e("li",null,[e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("semantics",null,[e("mrow",null,[e("msub",null,[e("mi",null,"H"),e("mi",null,"v")]),e("mo",null,"="),e("mi",null,"W"),e("mo",{separator:"true"},"⋅"),e("msub",null,[e("mi",null,"Z"),e("mi",null,"v")])]),e("annotation",{encoding:"application/x-tex"},"H_v = W · Z_v")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.08125em"}},"H"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"-0.0813em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"v")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.15em"}},[e("span")])])])])]),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),e("span",{class:"mrel"},"="),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),e("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),e("span",{class:"mpunct"},"⋅"),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"mord"},[e("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"Z"),e("span",{class:"msupsub"},[e("span",{class:"vlist-t vlist-t2"},[e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.1514em"}},[e("span",{style:{top:"-2.55em","margin-left":"-0.0715em","margin-right":"0.05em"}},[e("span",{class:"pstrut",style:{height:"2.7em"}}),e("span",{class:"sizing reset-size6 size3 mtight"},[e("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"v")])])]),e("span",{class:"vlist-s"},"​")]),e("span",{class:"vlist-r"},[e("span",{class:"vlist",style:{height:"0.15em"}},[e("span")])])])])])])])])])])]),e("li",null,"Language Model: Vicuna")],-1),ne=t('<p>Only the adapters for each modality need to be trained, while the parameters of the base models for each modality remain frozen.</p><ul><li>Stage 1: Pre-training for Feature Alignment <ul><li>Only the adapter parts are updated</li></ul></li><li>Stage 2: Fine-tuning End-to-End <ul><li>Both the adapter parts and the language model parts are updated</li></ul></li></ul><p>Data Generation: GPT-assisted Visual Instruction Data Generation</p><ul><li>The prompts provided to GPT include text descriptions (Captions) and bounding boxes, but do not include the images themselves. The GPT used is also a pure language model. <ul><li>Text Description Caption</li><li>Bounding Box</li></ul></li><li>GPT&#39;s responses include <ul><li>Q&amp;A pairs (QA Conversation)</li><li>Detailed Description</li><li>Complex Reasoning</li></ul></li></ul><figure><img src="'+y+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_7-minigpt-4" tabindex="-1"><a class="header-anchor" href="#_7-minigpt-4" aria-hidden="true">#</a> 7. MiniGPT-4</h2><p>Github: https://github.com/Vision-CAIR/MiniGPT-4</p><p>Paper Name: MINIGPT-4: ENHANCING VISION-LANGUAGE UNDERSTANDING WITH ADVANCED LARGE LANGUAGE MODELS</p><p>Paper: https://arxiv.org/pdf/2304.10592</p><figure><img src="'+x+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_8-sora-video-generation-large-model" tabindex="-1"><a class="header-anchor" href="#_8-sora-video-generation-large-model" aria-hidden="true">#</a> 8. Sora Video Generation Large Model</h2><p>Paper Name: Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models</p><p>Paper: https://arxiv.org/pdf/2402.17177</p><p>Github: https://github.com/lichao-sun/SoraReview</p><p>Note: This is not an official technical report from OpenAI.</p><figure><img src="'+v+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Sora is essentially a diffusion transformer model with flexible sampling dimensions. It consists of three parts:</p><ul><li><ol><li>Time-Space Compressor: Maps the original video into latent space</li></ol><ul><li>A time-space compressor: maps the original video into latent space.</li></ul></li><li><ol start="2"><li>Vision Transformer (ViT): Processes the tokenized latent representation and outputs the denoised latent representation</li></ol><ul><li>A ViT then processes the tokenized latent representation and outputs the denoised latent representation.</li></ul></li><li><ol start="3"><li>CLIP-like Model: Guides the video generation process, creating videos with specific styles or themes</li></ol><ul><li>A CLIP-like conditioning mechanism receives LLM-augmented user instructions and potentially visual prompts to guide the diffusion model to generate styled or themed videos.</li></ul></li></ul><figure><img src="'+T+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_9-prospects-of-multimodal-large-models" tabindex="-1"><a class="header-anchor" href="#_9-prospects-of-multimodal-large-models" aria-hidden="true">#</a> 9. Prospects of Multimodal Large Models</h2><p>Current Status</p><ul><li>In the early stages</li><li>Rapid technological iteration</li><li>In the long run, it is the endpoint of large models</li></ul><p>Analysis of Development in the Field of Large Models</p><ul><li>The foundation of multimodal large models is text large models <ul><li>The upper limit of text large models determines the upper limit of other large models</li><li>Text large models will promote the development of other modalities</li><li>Other modalities will subsequently develop the text large models</li></ul></li></ul><p>Opportunities in 2024</p><ul><li>Agent</li><li>Small Model/Model Quantization/Fine-tuning Small Models (Models embedded in smart devices, 0.5B, 1B) <ul><li>Smart hardware, such as smartwatches</li><li>How to run models on CPUs</li></ul></li><li>Multimodal</li><li>Inference acceleration, reducing inference costs</li></ul><h2 id="_10-reference" tabindex="-1"><a class="header-anchor" href="#_10-reference" aria-hidden="true">#</a> 10. Reference</h2>',27),oe={href:"https://jalammar.github.io/illustrated-stable-diffusion/?spm=a2c6h.12873639.article-detail.7.412f5124rpbR0C",target:"_blank",rel:"noopener noreferrer"};function re(ue,me){const l=I("ExternalLinkIcon");return w(),k("div",null,[A,G,S(" more "),P,e("ul",null,[e("li",null,[i("Language Modality + Visual Modality (Text + Image) "),e("ul",null,[e("li",null,[i("Generation "),e("ul",null,[e("li",null,[i("Text-to-Image: Generate an image from input text "),e("ul",null,[e("li",null,[i("DALL-E series "),e("ul",null,[C,D,e("li",null,[e("a",z,[i("API Documentation"),a(l)])])])]),e("li",null,[i("Midjourney "),e("ul",null,[V,N,e("li",null,[e("a",E,[i("Official Documentation"),a(l)])]),e("li",null,[e("a",R,[i("Prompt Reference Library"),a(l)])])])]),e("li",null,[i("Stable Diffusion "),e("ul",null,[O,e("li",null,[i("Web UI: "),e("a",F,[i("Stable Diffusion Web UI"),a(l)])]),e("li",null,[i("Model download site: Civitai, a great AI model library designed for Stable Diffusion models "),e("a",q,[i("Civitai"),a(l)])]),e("li",null,[i("Plugin for interface localization: "),e("a",B,[i("Chinese Interface"),a(l)])]),e("li",null,[i("Plugin - ControlNet: "),e("a",H,[i("ControlNet Plugin"),a(l)])]),U,W])])])]),Z])]),j])]),e("li",null,[i("Language Modality + Auditory Modality (Text + Audio) "),e("ul",null,[e("li",null,[i("Various audio-related tasks: "),e("a",Q,[i("AudioGPT GitHub"),a(l)])]),e("li",null,[i("Text-to-Speech (TTS) "),e("ul",null,[e("li",null,[i("edge-tts "),e("ul",null,[e("li",null,[e("a",X,[i("edge-tts GitHub"),a(l)])]),J])]),e("li",null,[i("MeloTTS "),e("ul",null,[e("li",null,[e("a",K,[i("MeloTTS GitHub"),a(l)])]),e("li",null,[e("a",Y,[i("MeloTTS Hugging Face"),a(l)])]),$,ee])])])]),ie,e("li",null,[i("Voice Cloning "),e("ul",null,[e("li",null,[i("GPT-SoVITS "),e("ul",null,[e("li",null,[e("a",le,[i("GPT-SoVITS GitHub"),a(l)])])])])])]),e("li",null,[i("Music Generation "),e("ul",null,[e("li",null,[i("Suno "),e("ul",null,[e("li",null,[e("a",ae,[i("Suno AI"),a(l)]),i(": Generate full lyrics and melodies based on text prompts")])])])])])])])]),te,se,ne,e("ul",null,[e("li",null,[e("a",oe,[i("The Illustrated Stable Diffusion"),a(l)])])])])}const ce=M(L,[["render",re],["__file","016_multimodal.html.vue"]]);export{ce as default};
