import{_ as i,a,b as t,c as r,d as n,e as o,f as l,g as s,h as c,i as d,j as h,k as u,l as g,m,n as p,o as f,p as b,q as y,r as v,s as _,t as w,u as x,v as S,w as k,x as z,y as N,z as C,A as q,B as T,C as I,D as A,E as P,F as D,G as M,H as Q,I as V,J as B,K as L}from"./011_pq_distance-ZKY0AwQw.js";import{_ as W}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as E,c as H,f as K,e}from"./app-0IasQefm.js";const F={},R=e('<h1 id="vector-databases-and-similarity-search" tabindex="-1"><a class="header-anchor" href="#vector-databases-and-similarity-search" aria-hidden="true">#</a> Vector Databases and Similarity Search</h1><ul><li><ol><li>Vector Embeddings</li></ol></li><li><ol start="2"><li>Similarity (Distance) Calculation Between Vectors</li></ol></li><li><ol start="3"><li>Traditional Recommendation System Modules</li></ol></li><li><ol start="4"><li>Vector Databases</li></ol></li><li><ol start="5"><li>Indexing Algorithms: KNN, ANN, NSW, HNSW, PQ</li></ol></li><li><ol start="6"><li>Sparse Search, Dense Search, Hybrid Search</li></ol></li></ul>',2),O=e('<h2 id="_1-vector-embeddings" tabindex="-1"><a class="header-anchor" href="#_1-vector-embeddings" aria-hidden="true">#</a> 1. Vector Embeddings</h2><ul><li>Vector embeddings capture the underlying meaning of the data and can be viewed as a machine-readable format of the data.</li><li>Embedding Models <ul><li>Open-source library: sentence-transformers <ul><li>Available via the Hugging Face model hub or directly from the source code repository.</li><li>Provides embedding models with dimensions in the range of 384, 512, and 768.</li></ul></li><li>Paid API Services <ul><li>OpenAI Embedding API <ul><li>Higher dimensionality with better quality, reaching thousands of dimensions.</li></ul></li><li>Cohere Embedding API <ul><li>Known for high-quality multilingual embedding models that outperform open-source variants.</li></ul></li></ul></li></ul></li><li>Embedding Evaluation Leaderboardï¼š MTEB LeaderBoard: https://huggingface.co/spaces/mteb/leaderboard</li></ul><h2 id="_2-similarity-distance-calculation-between-vectors" tabindex="-1"><a class="header-anchor" href="#_2-similarity-distance-calculation-between-vectors" aria-hidden="true">#</a> 2. Similarity (Distance) Calculation Between Vectors</h2><p>The greater the similarity, the smaller the distance; the two metrics are inversely related.</p><p>4 Distance Metrics</p><p>Dot Product and Cosine Distance are commonly used in the field of NLP, to evaluate how similar two sentence embeddings are.</p><ul><li>Euclidean Distance (L2) <ul><li>Calculates the shortest path.</li><li>Smaller values indicate better matches.</li></ul></li></ul><figure><img src="'+i+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Manhattan Distance (L1) <ul><li>Moves one direction axis at a time.</li><li>Smaller values indicate better matches.</li></ul></li></ul><figure><img src="'+a+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Dot Product <ul><li>The projection of one vector onto another.</li><li>Produces a non-normalized value with arbitrary size.</li><li>Larger values indicate better matches, while negative values usually indicate greater distance.</li></ul></li></ul><figure><img src="'+t+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Cosine Distance <ul><li>Calculates the angle between vectors.</li><li>Produces a normalized value (between -1 and 1).</li><li>Smaller values indicate better matches.</li></ul></li></ul><figure><img src="'+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_3-traditional-recommendation-system-modules" tabindex="-1"><a class="header-anchor" href="#_3-traditional-recommendation-system-modules" aria-hidden="true">#</a> 3. Traditional Recommendation System Modules</h2><h3 id="_3-1-classification" tabindex="-1"><a class="header-anchor" href="#_3-1-classification" aria-hidden="true">#</a> 3.1. Classification</h3><ul><li>Content-Based Recommendation <ul><li>Based on previously liked content.</li><li>Pros: Recommendations are explicitly aligned with user interests.</li><li>Cons: Does not recommend types of content not previously encountered.</li></ul></li><li>Collaborative Filtering Recommendation <ul><li>Recommends content liked by friends.</li><li>Pros: Can recommend types of content previously unseen.</li><li>Cons: Might recommend content the user is not interested in.</li></ul></li></ul><h3 id="_3-2-input" tabindex="-1"><a class="header-anchor" href="#_3-2-input" aria-hidden="true">#</a> 3.2. Input</h3><ul><li><ol><li>Behavior (e.g., articles viewed, articles clicked)</li></ol></li><li><ol start="2"><li>User&#39;s basic information</li></ol></li><li><ol start="3"><li>Article list</li></ol></li></ul><h3 id="_3-3-goal" tabindex="-1"><a class="header-anchor" href="#_3-3-goal" aria-hidden="true">#</a> 3.3. Goal</h3><ul><li>Recommend new content.</li></ul><h3 id="_3-4-approach" tabindex="-1"><a class="header-anchor" href="#_3-4-approach" aria-hidden="true">#</a> 3.4. Approach</h3><ul><li>Build user profiles based on behavior and basic information. <ul><li>User vectors (similar to tags, but these are all vectors).</li><li>Tags (liked topics, disliked topics, preferences, etc.) <ul><li>A collection of tags from articles the user has viewed (both provided and learned).</li></ul></li></ul></li><li>Recall candidates from the article database based on tags. <ul><li>Multiple recall iterations can be designed.</li></ul></li><li>Coarse ranking + fine ranking. <ul><li>Multiple ranking iterations can be designed.</li><li>Considerations: <ul><li>Changes in user interests.</li><li>Source and timeliness of articles.</li><li>More detailed exploration of user interests.</li></ul></li></ul></li><li>Top 10 articles.</li></ul><h3 id="_3-5-traditional-recommendation-system-process" tabindex="-1"><a class="header-anchor" href="#_3-5-traditional-recommendation-system-process" aria-hidden="true">#</a> 3.5. Traditional Recommendation System Process</h3><p>Article -&gt; User Profile -&gt; Recall -&gt; Candidates -&gt; Rank -&gt; Top 10</p><ul><li><ol><li>Article -&gt; User Profile</li></ol></li><li><ol start="2"><li>User Profile -&gt; Recall</li></ol></li><li><ol start="3"><li>Recall -&gt; Candidates</li></ol></li><li><ol start="4"><li>Candidates -&gt; Rank</li></ol></li><li><ol start="5"><li>Rank -&gt; Top 10</li></ol></li></ul><h3 id="_3-6-recommendation-systems-combined-with-llms" tabindex="-1"><a class="header-anchor" href="#_3-6-recommendation-systems-combined-with-llms" aria-hidden="true">#</a> 3.6. Recommendation Systems Combined with LLMs</h3><p>Consider which processes can incorporate LLMs:</p><ul><li>Step 1 <ul><li>Use LLM to extract tags.</li></ul></li><li>Step 4 <ul><li>Use LLM for ranking.</li></ul></li></ul><h2 id="_4-vector-databases" tabindex="-1"><a class="header-anchor" href="#_4-vector-databases" aria-hidden="true">#</a> 4. Vector Databases</h2><h3 id="_4-1-vector-databases-solving-efficiency-issues" tabindex="-1"><a class="header-anchor" href="#_4-1-vector-databases-solving-efficiency-issues" aria-hidden="true">#</a> 4.1. Vector Databases: Solving Efficiency Issues</h3><p>Vector databases have existed prior to the explosion of generative AI and have long been part of semantic search applications, which search based on the meaning similarity of words or phrases rather than exact keyword matching.</p><p>The main goal of vector databases is to provide a fast and efficient way to store and perform semantic query data.</p><ul><li>Previously: Similarity calculations with each vector.</li><li>Now: Approximate search.</li></ul><h3 id="_4-2-vector-database-products" tabindex="-1"><a class="header-anchor" href="#_4-2-vector-database-products" aria-hidden="true">#</a> 4.2. Vector Database Products</h3><ul><li>Open Source &amp; Closed Source <ul><li>Among all listed options, only one is completely closed-source: Pinecone. Zilliz is also a fully closed commercial solution but is entirely built on Milvus and can be considered the parent company of Milvus.</li></ul></li></ul><figure><img src="'+n+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Development History <ul><li>Vespa was one of the first major vendors to incorporate vector similarity search alongside the mainstream BM25 keyword search algorithm.</li><li>Established vendors like Elasticsearch, Redis, and PostgreSQL only began offering vector search in 2022 and beyond, much later than originally anticipated.</li></ul></li></ul><figure><img src="'+o+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Hosting Options <ul><li>Self-hosted (On-Premises) <ul><li>Follows a client-server architecture.</li></ul></li><li>Managed (Cloud-Native) <ul><li>Follows a client-server architecture.</li></ul></li><li>Recent Option: Embedded Mode <ul><li>The database itself is tightly coupled with application code, running in a serverless manner.</li><li>Currently, only Chroma and LanceDB are available as embedded databases.</li></ul></li></ul></li></ul><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_4-3-vector-database-comparison" tabindex="-1"><a class="header-anchor" href="#_4-3-vector-database-comparison" aria-hidden="true">#</a> 4.3. Vector Database Comparison</h3><ul><li>Trade-offs <ul><li>Hybrid search or keyword search? A hybrid of keyword and vector search can yield the best results; each vector database vendor recognizes this and provides their own customized hybrid search solution.</li><li>On-premises or cloud-native deployment? Many vendors tout &quot;cloud-native&quot; as a selling point, as if infrastructure is the biggest pain point globally, but on-premises deployment might be more cost-effective in the long run.</li><li>Open source or fully managed? Most vendors build on accessible or open-source code to demonstrate their foundational approaches and then offer deployment and infrastructure through fully managed SaaS. While self-hosting many solutions is still possible, it requires additional manpower and internal skill requirements.</li></ul></li></ul><figure><img src="'+s+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_4-4-indexing-algorithms" tabindex="-1"><a class="header-anchor" href="#_4-4-indexing-algorithms" aria-hidden="true">#</a> 4.4. Indexing Algorithms</h3><p>Data is stored in vector databases through indexing, which refers to creating data structures called indexes for efficient vector lookup by rapidly narrowing the search space.</p><p>As with most cases, choosing a vector index involves a trade-off between precision (accuracy/recall) and speed/throughput.</p><ul><li>Keyword Search / Sparse Search <ul><li>BM25 = IDF (Inverse Document Frequency)</li><li>IVF (Inverted File) â€“ Widely used in search engines</li></ul></li><li>Vector Search / Dense Search <ul><li>Flat = KNN (K-Nearest Neighbors) = Brute-force search = Computes similarity with every vector</li><li>Approximate KNN = Divides the space into multiple clusters, first finds the distance to each centroid, then searches within the space of the closest centroid</li><li>ANN (Approximate Nearest Neighbor)</li><li>NSW (Navigable Small World)</li><li>HNSW (Hierarchical Navigable Small World)</li><li>PQ (Product Quantization)</li></ul></li></ul><figure><img src="'+d+'" alt="Indexing algorithms used by various vector databases" tabindex="0" loading="lazy"><figcaption>Indexing algorithms used by various vector databases</figcaption></figure><figure><img src="'+h+'" alt="Classification of vector indexes based on underlying data structures" tabindex="0" loading="lazy"><figcaption>Classification of vector indexes based on underlying data structures</figcaption></figure><h3 id="_4-5-index-evaluation-criteria" tabindex="-1"><a class="header-anchor" href="#_4-5-index-evaluation-criteria" aria-hidden="true">#</a> 4.5. Index Evaluation Criteria</h3><p>The evaluation of an index&#39;s quality always depends on the specific data model, generally including the following points:</p><ul><li>Query Time. The speed of queries is crucial, especially in large models.</li><li>Query Quality. ANN queries may not always return the most accurate results, but the quality should not deviate too much. Query quality has many metrics, with recall being one of the most commonly used.</li><li>Memory Consumption. The memory consumed by query indexing; looking up in memory is significantly faster than looking up on disk.</li><li>Training Time. Some query methods require training to achieve better performance.</li><li>Write Time. The impact on the index when writing vectors, including all maintenance.</li></ul><h2 id="_5-search" tabindex="-1"><a class="header-anchor" href="#_5-search" aria-hidden="true">#</a> 5. Search</h2><h3 id="_5-1-sparse-search-keyword-search-vs-dense-search-vector-search-semantic-search" tabindex="-1"><a class="header-anchor" href="#_5-1-sparse-search-keyword-search-vs-dense-search-vector-search-semantic-search" aria-hidden="true">#</a> 5.1. Sparse Search (Keyword Search) vs Dense Search (Vector Search/Semantic Search)</h3><ul><li>Sparse Search (Keyword Search) <ul><li>Keyword search, text matching.</li></ul></li><li>Dense Search (Vector Search/Semantic Search) <ul><li>Vector similarity search.</li><li>Searching for the closest objects in vector space, known as semantic search or vector search.</li><li>For example, &quot;Baby dogs&quot; &lt;=&gt; &quot;Here is content on puppies!&quot;</li><li>Limitations: <ul><li>Influenced by the embedding model used; vector similarity is only effective on the training dataset. If the search content is significantly different from the dataset used to train the embedding model, the search results will be poor due to the mismatch in distribution.</li><li>Do vectors truly represent semantics? They can be seen as hashes rather than true understanding, which can lead to misleading results.</li><li>With a large volume of vector data, search accuracy can significantly decline (refer to https://mp.weixin.qq.com/s/DH4-QCK1U8BYGlAblQLTLw).</li><li>Search is not suitable for data types lacking semantic information, such as sequential numbers or tabular data.</li></ul></li></ul></li><li>Hybrid Search <ul><li>Combines the ranking results of both sparse and dense searches.</li></ul></li></ul><h3 id="_5-2-sparse-search" tabindex="-1"><a class="header-anchor" href="#_5-2-sparse-search" aria-hidden="true">#</a> 5.2. Sparse Search</h3><figure><img src="'+u+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Bag of Words</p><p>The simplest way to perform keyword matching is through Bag of Wordsâ€”counting how many times a word occurs in the query and the data vector, then returning objects with the highest matching word frequencies.</p><p>In practice, this method may capture only a small percentage of the available words, resulting in many zeros in the data.</p><h3 id="_5-3-hybrid-search" tabindex="-1"><a class="header-anchor" href="#_5-3-hybrid-search" aria-hidden="true">#</a> 5.3. Hybrid Search</h3><p>Combines the results of both search types by setting appropriate weight ratios.</p><figure><img src="'+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_5-4-multilingual-search" tabindex="-1"><a class="header-anchor" href="#_5-4-multilingual-search" aria-hidden="true">#</a> 5.4. Multilingual Search</h3><p>In multilingual search scenarios, texts with the same meaning but in different languages will generate very similar (if not identical) embeddings.</p><figure><img src="'+m+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_6-keyword-matching-algorithm-bm25-best-matching-25" tabindex="-1"><a class="header-anchor" href="#_6-keyword-matching-algorithm-bm25-best-matching-25" aria-hidden="true">#</a> 6. Keyword Matching Algorithm: BM25 (Best Matching 25)</h2><p>Belongs to sparse search (keyword search).</p><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>BM25 performs exceptionally well when searching across multiple keywords.</p><p>Concept: It counts the occurrences of words in the incoming phrases, where more frequently appearing words are weighted as less important during matching, while rarer words score significantly higher.</p><h2 id="_7-inverted-index-inverted-file-index-ivf-fast-text-search" tabindex="-1"><a class="header-anchor" href="#_7-inverted-index-inverted-file-index-ivf-fast-text-search" aria-hidden="true">#</a> 7. Inverted Index / Inverted File Index (IVF) - Fast Text Search</h2><p>Belongs to sparse search (keyword search).</p><p>Used by search engines like Google and Baidu, it is particularly suitable for text retrieval.</p><p>Forward Index: DocId -&gt; Value<br> Inverted Index: Value -&gt; DocId</p><figure><img src="'+f+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol><li>Extract words from documents that are not part of the stop words list (function words that have little actual meaning and are used in almost every document).</li><li>Create a mapping between each word and the multiple document IDs it appears in.</li><li>When querying keywords, recall (Recall) all documents containing those keywords.</li><li>When querying a sentence, which has multiple keywords, recall the corresponding documents for each keyword, then take the intersection, rank them, and return the top k documents as results.</li></ol><h2 id="_8-knn-k-nearest-neighbors-algorithm-knn-search-brute-force-search" tabindex="-1"><a class="header-anchor" href="#_8-knn-k-nearest-neighbors-algorithm-knn-search-brute-force-search" aria-hidden="true">#</a> 8. KNN (K-Nearest Neighbors Algorithm, KNN Search, Brute Force Search)</h2><p>nickname</p><ul><li>a flat index</li><li>a brute force</li></ul><p>Brute force search: Calculates similarity with every vector.</p><p>In classical machine learning, this is known as the K-Nearest Neighbors (KNN) algorithm.</p><p>It compares the query vector with each vector in the database.</p><ol><li>Given a Query, find the distance between all vectors and the Query vector.</li><li>Sort all distances.</li><li>Return the top K matching objects that are closest.</li></ol><figure><img src="'+b+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>The issue with brute force search is the enormous computational cost; the total query time increases with the number of stored objects.</p><p>The more vectors you have, the longer the query takes, and in practical applications, you might be handling tens of millions or even hundreds of millions of objects.</p><figure><img src="'+y+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Advantage: Precise</li><li>Disadvantage: High computational cost, long processing time, time complexity O(n).</li></ul><h2 id="_9-approximate-knn-dividing-space-into-several-modules" tabindex="-1"><a class="header-anchor" href="#_9-approximate-knn-dividing-space-into-several-modules" aria-hidden="true">#</a> 9. Approximate KNN - Dividing Space into Several Modules</h2><figure><img src="'+v+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+_+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol><li>Use a clustering algorithm (like K-means) to initialize k cluster centers (also called centroids).</li><li>Calculate the distance from each sample to the centroids and assign samples to the nearest centroid, forming clusters.</li><li>Divide the space using these centroids and the samples belonging to each cluster, resulting in a Voronoi diagram. <ul><li>The main property of a Voronoi diagram is that the distance from a centroid to any point in its region is less than the distance from that point to any other centroid.</li></ul></li><li>Given a Query, calculate the distance from the Query to each centroid and find the corresponding space for the Query. <ul><li>When given a new object, calculate the distances to all centroids of the Voronoi partitions. Then select the closest centroid and consider the vectors within that partition as candidates.</li></ul></li><li>The Query then computes distances with all nodes in the space of the centroid. <ul><li>By calculating distances to candidates and selecting the top k closest, it returns the final answer.</li></ul></li></ol><p>If there are k centroids, then the average number of nodes per space is n/k, resulting in a time complexity of O(k + n/k).</p><h3 id="_9-1-boundary-problem" tabindex="-1"><a class="header-anchor" href="#_9-1-boundary-problem" aria-hidden="true">#</a> 9.1. Boundary Problem</h3><p>In the image below, we can see a situation where the actual nearest neighbor is located in the red area, but we only select candidates from the green area. This is referred to as the boundary problem.</p><figure><img src="'+w+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>The boundary problem essentially arises because:</p><p>The Voronoi diagram only guarantees that the distance from a node to its region&#39;s centroid is less than to other centroids, but does not guarantee that the distance from a node to other nodes within the region is less than to nodes in other regions.</p><p><strong>Solution:</strong> Expand the search range.</p><p>This situation often occurs when the query object is near the boundary of another region. To reduce errors in such cases, we can increase the search range and select several regions to search candidates based on the nearest m centroids.</p><figure><img src="'+x+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>The time complexity for expanding the search range is O(k + 3 * n/k).</p><p>The more regions searched, the more accurate the results, but the time required to compute them increases. A trade-off must be made between accuracy and processing time.</p><h2 id="_10-ann-approximate-nearest-neighbors-algorithm" tabindex="-1"><a class="header-anchor" href="#_10-ann-approximate-nearest-neighbors-algorithm" aria-hidden="true">#</a> 10. ANN (Approximate Nearest Neighbors Algorithm)</h2><p>It doesn&#39;t always find the best match, but it finds approximate nearest neighbor matches, which is still a fairly good result, even if it&#39;s not perfect.</p><p>By sacrificing some accuracy (not always returning the true nearest neighbors), substantial performance improvements can be achieved using ANN algorithms.</p><ul><li>NSW</li><li>HNSW</li></ul><h2 id="_11-nsw-navigable-small-world" tabindex="-1"><a class="header-anchor" href="#_11-nsw-navigable-small-world" aria-hidden="true">#</a> 11. NSW (Navigable Small World)</h2><h3 id="_11-1-graph-construction-one-by-one" tabindex="-1"><a class="header-anchor" href="#_11-1-graph-construction-one-by-one" aria-hidden="true">#</a> 11.1. Graph Construction (one by one)</h3><figure><img src="'+S+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li><p>m=2: The number of neighbors, meaning how many nodes are connected.</p></li><li><p>Highway Concept:</p><ul><li>Initially, connections may be long because they are made one by one, without better or closer options. As connections progress, they become denser and shorter.</li><li>This approach has the advantage of resembling highways: the initially distant connections allow for traveling long distances without going through many small paths, thus improving search efficiency.</li></ul></li><li><p>Construction Complexity: O(nÂ²)</p><ul><li>The i-th node is compared with the previous i-1 nodes.</li></ul></li></ul><h3 id="_11-2-search" tabindex="-1"><a class="header-anchor" href="#_11-2-search" aria-hidden="true">#</a> 11.2. Search</h3><ol><li>Start from a random entry node and move towards the Query along the nearest neighbors.</li><li>Keep track of the top k points in a priority queue.</li></ol><p><img src="'+k+'" alt="" loading="lazy"><img src="'+z+'" alt="" loading="lazy"><img src="'+N+'" alt="" loading="lazy"></p><ul><li>Query Complexity: O(n log n)</li><li>Problem: Can&#39;t find the next step? Early Stopping <ul><li>Early stopping occurs when the two neighbors of the current node are farther from the query. The algorithm may return the current node as the response, even though closer nodes exist.</li><li>Improvement: Using multiple entry points can enhance search accuracy.</li></ul></li></ul><figure><img src="'+C+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_12-hnsw-hierarchical-navigable-small-world" tabindex="-1"><a class="header-anchor" href="#_12-hnsw-hierarchical-navigable-small-world" aria-hidden="true">#</a> 12. HNSW (Hierarchical Navigable Small World)</h2><ul><li>HNSW Algorithm: An optimization of NSW based on the Skip List concept. <ul><li>HNSW explicitly distinguishes between highways in the highway concept, imagining layers for different modes of transport: airplane, train, car, bicycle, and walking.</li></ul></li><li>It has driven the development of some of the most powerful vector databases, which utilize this structure.</li></ul><figure><img src="'+q+'" alt="Skip List" tabindex="0" loading="lazy"><figcaption>Skip List</figcaption></figure><h3 id="_12-1-building-hierarchical-structure" tabindex="-1"><a class="header-anchor" href="#_12-1-building-hierarchical-structure" aria-hidden="true">#</a> 12.1. Building - Hierarchical Structure</h3><ul><li>The structure is divided into layers, with the number of nodes increasing from top to bottom, with the bottom layer containing all nodes.</li><li>How to allocate nodes into layers? <ul><li>Each node is assigned a random number as its maximum layer (Max Layer). A node with a Max Layer of zero exists only at the bottom layer. If the random number is 2, the node exists at layers 0, 1, and 2, and so forth.</li></ul></li><li>Construction Complexity: O(n log n)</li></ul><figure><img src="'+T+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_12-2-querying" tabindex="-1"><a class="header-anchor" href="#_12-2-querying" aria-hidden="true">#</a> 12.2. Querying</h3><ul><li>Query Process: <ol><li>Start from a random entry node at the top layer, moving towards the nearest neighbors.</li><li>Move to the nearest node in that layer.</li><li>Enter the next layer and continue to the nearest neighbor until reaching the bottom layer.</li></ol></li></ul><p><img src="'+I+'" alt="" loading="lazy"><img src="'+A+'" alt="" loading="lazy"></p><ul><li>Query Complexity: O(log n) <ul><li>Query time grows logarithmically, meaning that it doesn&#39;t significantly impact speed as data volume increases.</li></ul></li></ul><figure><img src="'+P+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_12-3-further-optimization-hnsw-approximate-knn" tabindex="-1"><a class="header-anchor" href="#_12-3-further-optimization-hnsw-approximate-knn" aria-hidden="true">#</a> 12.3. Further Optimization: HNSW + Approximate KNN</h3><figure><img src="'+D+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_13-product-quantization-pq" tabindex="-1"><a class="header-anchor" href="#_13-product-quantization-pq" aria-hidden="true">#</a> 13. Product Quantization (PQ)</h2><p>Enhances search efficiency while compressing data and reducing memory usage.</p><p>Core:</p><ul><li>Fast <ul><li>Time Complexity: O(number of vectors * (subspace/dimension))</li></ul></li><li>Compression <ul><li>Data Compression: For instance, if each vector in the database is 1024-dimensional and split into four 256-dimensional subvectors, the storage requirement is greatly reduced by converting from 1024 floats to 4 ints.</li></ul></li></ul><p>Product Quantization aims to reduce memory usage and improve query speed (due to reduced computational load). PQ is a lossy compression method, which can lead to decreased retrieval accuracy, but this is acceptable in the context of ANN requirements.</p><h3 id="_13-1-construction" tabindex="-1"><a class="header-anchor" href="#_13-1-construction" aria-hidden="true">#</a> 13.1. Construction</h3><p><img src="'+M+'" alt="" loading="lazy"><img src="'+Q+'" alt="" loading="lazy"></p><p>Steps:</p><ol><li><strong>Subvectors</strong>: Split the original high-dimensional vector into n low-dimensional subvectors. <ul><li>For example, if each vector in the database is 1024-dimensional, split it into four 256-dimensional subvectors.</li></ul></li><li><strong>Codebook</strong>: Use k-means to compute Voronoi diagrams for each of the n subvectors, resulting in n distinct Voronoi diagrams (assuming each diagram has k centroids).</li><li><strong>Clustering</strong>: Place the n subvectors into their respective completed Voronoi diagrams to find the nearest centroid.</li><li><strong>Quantized Vectors</strong>: Treat the nearest centroids as new vectors, leading to quantized vectors.</li><li><strong>Reproduction Values</strong>: Use the indices of the nearest centroids in each of the n subvectors as new values, collectively referred to as the PQ code. <ul><li>Each subvector gets the index of its nearest centroid, resulting in four new values per vector in the database.</li></ul></li></ol><figure><img src="'+V+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Based on the n subvectors and the k centroids in each subspace, we obtain an n*k centroid matrix. The index of each subvector&#39;s nearest centroid is the PQ code.</p><h3 id="_13-2-querying" tabindex="-1"><a class="header-anchor" href="#_13-2-querying" aria-hidden="true">#</a> 13.2. Querying</h3><p>To find the centroids using the PQ code, look for KNN in the subspace of the centroid, approximating the distance between the query vector and an existing vector ( y ) using the centroid.</p><p>Query Steps:</p><ol><li>Split the query vector into multiple subvectors.</li><li>Calculate the distance between the query subvectors and the centroid matrix (finding the distance from each query subvector to each centroid), resulting in a distance matrix.</li><li>For each vector in the database, use its PQ code to find the corresponding centroid, retrieve distances from the distance matrix, and calculate the approximate distance between the query vector and any vector in the database by summing the squared distances of each subvector and taking the square root.</li></ol><p><img src="'+B+'" alt="" loading="lazy"><img src="'+L+'" alt="" loading="lazy"></p><h3 id="_13-3-further-optimization" tabindex="-1"><a class="header-anchor" href="#_13-3-further-optimization" aria-hidden="true">#</a> 13.3. Further Optimization</h3><p>Combining PQ with Approximate KNN allows for not needing to compute distances with every vector in the database.</p><h2 id="_14-references" tabindex="-1"><a class="header-anchor" href="#_14-references" aria-hidden="true">#</a> 14. References</h2><p>[Middleware for Large Models: Principles and Selection of Vector Databases] (https://hub.baai.ac.cn/view/29516)</p><p>https://www.modb.pro/db/1817186648364507136</p><p>https://www.xiaozhuai.com/similarity-search-part-4-hierarchical-navigable-small-world-hnsw.html</p>',154);function U(j,G){return E(),H("div",null,[R,K(" more "),O])}const X=W(F,[["render",U],["__file","011_vector_database.html.vue"]]);export{X as default};
