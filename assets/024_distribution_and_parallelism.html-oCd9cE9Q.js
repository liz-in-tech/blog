const e=JSON.parse('{"key":"v-62e58826","path":"/zh/posts/llm/024_distribution_and_parallelism.html","title":"分布式训练之一：模型训练的内存占用","lang":"zh-CN","frontmatter":{"icon":"lightbulb","sidebar":false,"date":"2025-02-26T00:00:00.000Z","prev":"./025_distribution_and_parallelism_1","next":"./023_agent_framework","category":["LLM"],"tag":["分布式","并行"],"description":"分布式训练之一：模型训练的内存占用","head":[["link",{"rel":"alternate","hreflang":"en-us","href":"https://liz-in-tech.github.io/blog/posts/llm/024_distribution_and_parallelism.html"}],["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/blog/zh/posts/llm/024_distribution_and_parallelism.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"分布式训练之一：模型训练的内存占用"}],["meta",{"property":"og:description","content":"分布式训练之一：模型训练的内存占用"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:locale:alternate","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-03-30T08:06:25.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:tag","content":"分布式"}],["meta",{"property":"article:tag","content":"并行"}],["meta",{"property":"article:published_time","content":"2025-02-26T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-03-30T08:06:25.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"分布式训练之一：模型训练的内存占用\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-02-26T00:00:00.000Z\\",\\"dateModified\\":\\"2025-03-30T08:06:25.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. 指标","slug":"_1-指标","link":"#_1-指标","children":[]},{"level":2,"title":"2. 三大关键挑战 three key challenges","slug":"_2-三大关键挑战-three-key-challenges","link":"#_2-三大关键挑战-three-key-challenges","children":[]},{"level":2,"title":"3. 模型训练基础","slug":"_3-模型训练基础","link":"#_3-模型训练基础","children":[{"level":3,"title":"3.1. 模型训练过程","slug":"_3-1-模型训练过程","link":"#_3-1-模型训练过程","children":[]},{"level":3,"title":"3.2. 重要超参数 -- 批大小","slug":"_3-2-重要超参数-批大小","link":"#_3-2-重要超参数-批大小","children":[]}]},{"level":2,"title":"4. 模型训练的内存使用","slug":"_4-模型训练的内存使用","link":"#_4-模型训练的内存使用","children":[]},{"level":2,"title":"5. 内存优化建议","slug":"_5-内存优化建议","link":"#_5-内存优化建议","children":[]},{"level":2,"title":"6. 激活值重新计算 Activation Recomputation / Gradient Checkpointing / rematerialization","slug":"_6-激活值重新计算-activation-recomputation-gradient-checkpointing-rematerialization","link":"#_6-激活值重新计算-activation-recomputation-gradient-checkpointing-rematerialization","children":[]},{"level":2,"title":"7. 梯度累积 Gradient Accumulation","slug":"_7-梯度累积-gradient-accumulation","link":"#_7-梯度累积-gradient-accumulation","children":[]},{"level":2,"title":"8. 混合精度训练 Mixed Precision Training","slug":"_8-混合精度训练-mixed-precision-training","link":"#_8-混合精度训练-mixed-precision-training","children":[{"level":3,"title":"8.1. 浮点数的数值范围与精度","slug":"_8-1-浮点数的数值范围与精度","link":"#_8-1-浮点数的数值范围与精度","children":[]},{"level":3,"title":"8.2. 混合精度训练理念","slug":"_8-2-混合精度训练理念","link":"#_8-2-混合精度训练理念","children":[]},{"level":3,"title":"8.3. 混合精度训练的已知方法汇总","slug":"_8-3-混合精度训练的已知方法汇总","link":"#_8-3-混合精度训练的已知方法汇总","children":[]},{"level":3,"title":"8.4. FP16和BF16训练","slug":"_8-4-fp16和bf16训练","link":"#_8-4-fp16和bf16训练","children":[]},{"level":3,"title":"8.5. FP8训练","slug":"_8-5-fp8训练","link":"#_8-5-fp8训练","children":[]}]},{"level":2,"title":"9. 小工具","slug":"_9-小工具","link":"#_9-小工具","children":[]},{"level":2,"title":"9.1. 计算内存使用量工具：Predict Memory","slug":"_9-1-计算内存使用量工具-predict-memory","link":"#_9-1-计算内存使用量工具-predict-memory","children":[]},{"level":2,"title":"9.2. 可视化GPU计算和通信成本的分布式训练工具：Profiler","slug":"_9-2-可视化gpu计算和通信成本的分布式训练工具-profiler","link":"#_9-2-可视化gpu计算和通信成本的分布式训练工具-profiler","children":[]},{"level":2,"title":"10. 参考资料：ultrascale-playbook","slug":"_10-参考资料-ultrascale-playbook","link":"#_10-参考资料-ultrascale-playbook","children":[{"level":3,"title":"10.1. 概览","slug":"_10-1-概览","link":"#_10-1-概览","children":[]},{"level":3,"title":"10.2. 前置基础知识","slug":"_10-2-前置基础知识","link":"#_10-2-前置基础知识","children":[]},{"level":3,"title":"10.3. scaling experiments","slug":"_10-3-scaling-experiments","link":"#_10-3-scaling-experiments","children":[]}]}],"git":{"createdTime":1741444326000,"updatedTime":1743321985000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro-2.local","commits":1},{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":14.61,"words":4383},"filePathRelative":"zh/posts/llm/024_distribution_and_parallelism.md","localizedDate":"2025年2月26日","excerpt":"<h1> 分布式训练之一：模型训练的内存占用</h1>\\n","autoDesc":true}');export{e as data};
