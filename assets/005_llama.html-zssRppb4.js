const e=JSON.parse('{"key":"v-f15445b6","path":"/posts/llm/005_llama.html","title":"Llama Source Code Exploration","lang":"en-US","frontmatter":{"icon":"lightbulb","sidebar":false,"date":"2024-06-01T00:00:00.000Z","prev":"./006_llm_leaderboard","next":"./004_transformer","category":["LLM"],"tag":["Llama","Source Code"],"description":"Llama Source Code Exploration About Llama Overall Architecture Llama Code Logic Hyperparameters Tensor Dimensionality Transformation Number of Trainable Parameters Source Code","head":[["link",{"rel":"alternate","hreflang":"zh-cn","href":"https://liz-in-tech.github.io/blog/zh/posts/llm/005_llama.html"}],["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/blog/posts/llm/005_llama.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"Llama Source Code Exploration"}],["meta",{"property":"og:description","content":"Llama Source Code Exploration About Llama Overall Architecture Llama Code Logic Hyperparameters Tensor Dimensionality Transformation Number of Trainable Parameters Source Code"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:locale:alternate","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-03-16T10:53:15.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:tag","content":"Llama"}],["meta",{"property":"article:tag","content":"Source Code"}],["meta",{"property":"article:published_time","content":"2024-06-01T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-03-16T10:53:15.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Llama Source Code Exploration\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-06-01T00:00:00.000Z\\",\\"dateModified\\":\\"2025-03-16T10:53:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. About","slug":"_1-about","link":"#_1-about","children":[]},{"level":2,"title":"2. Llama Overall Architecture","slug":"_2-llama-overall-architecture","link":"#_2-llama-overall-architecture","children":[]},{"level":2,"title":"3. Llama Code Logic","slug":"_3-llama-code-logic","link":"#_3-llama-code-logic","children":[{"level":3,"title":"3.1. Llama Inference Code Logic","slug":"_3-1-llama-inference-code-logic","link":"#_3-1-llama-inference-code-logic","children":[]},{"level":3,"title":"3.2. Llama Training Code Logic","slug":"_3-2-llama-training-code-logic","link":"#_3-2-llama-training-code-logic","children":[]}]},{"level":2,"title":"4. Hyperparameters","slug":"_4-hyperparameters","link":"#_4-hyperparameters","children":[]},{"level":2,"title":"5. Tensor Dimensionality Transformation","slug":"_5-tensor-dimensionality-transformation","link":"#_5-tensor-dimensionality-transformation","children":[]},{"level":2,"title":"6. Number of Trainable Parameters","slug":"_6-number-of-trainable-parameters","link":"#_6-number-of-trainable-parameters","children":[]},{"level":2,"title":"7. Source Code","slug":"_7-source-code","link":"#_7-source-code","children":[{"level":3,"title":"7.1. Entrance","slug":"_7-1-entrance","link":"#_7-1-entrance","children":[]},{"level":3,"title":"7.2. GenerationMixin","slug":"_7-2-generationmixin","link":"#_7-2-generationmixin","children":[]},{"level":3,"title":"7.3. LlamaForCausalLM","slug":"_7-3-llamaforcausallm","link":"#_7-3-llamaforcausallm","children":[]},{"level":3,"title":"7.4. LlamaModel","slug":"_7-4-llamamodel","link":"#_7-4-llamamodel","children":[]},{"level":3,"title":"7.5. LlamaDecoderLayer","slug":"_7-5-llamadecoderlayer","link":"#_7-5-llamadecoderlayer","children":[]},{"level":3,"title":"7.6. LlamaRMSNorm","slug":"_7-6-llamarmsnorm","link":"#_7-6-llamarmsnorm","children":[]},{"level":3,"title":"7.7. LlamaSdpaAttention","slug":"_7-7-llamasdpaattention","link":"#_7-7-llamasdpaattention","children":[]},{"level":3,"title":"7.8. LlamaRotaryEmbedding","slug":"_7-8-llamarotaryembedding","link":"#_7-8-llamarotaryembedding","children":[]},{"level":3,"title":"7.9. LlamaMLP","slug":"_7-9-llamamlp","link":"#_7-9-llamamlp","children":[]}]}],"git":{"createdTime":1730208252000,"updatedTime":1742122395000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro.local","commits":2},{"name":"unknown","email":"15721607377@163.com","commits":1}]},"readingTime":{"minutes":6.57,"words":1972},"filePathRelative":"posts/llm/005_llama.md","localizedDate":"June 1, 2024","excerpt":"<h1> Llama Source Code Exploration</h1>\\n<ul>\\n<li>\\n<ol>\\n<li>About</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"2\\">\\n<li>Llama Overall Architecture</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"3\\">\\n<li>Llama Code Logic</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"4\\">\\n<li>Hyperparameters</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"5\\">\\n<li>Tensor Dimensionality Transformation</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"6\\">\\n<li>Number of Trainable Parameters</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"7\\">\\n<li>Source Code</li>\\n</ol>\\n</li>\\n</ul>\\n","autoDesc":true}');export{e as data};
