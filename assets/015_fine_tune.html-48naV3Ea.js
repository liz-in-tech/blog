const e=JSON.parse('{"key":"v-db76df04","path":"/posts/llm/015_fine_tune.html","title":"Fine-tuning","lang":"en-US","frontmatter":{"icon":"lightbulb","sidebar":false,"date":"2024-11-05T00:00:00.000Z","prev":"./016_multimodal","next":"./014_rag_evaluation","category":["LLM"],"tag":["Fine-tuning"],"description":"Fine-tuning Model Fine-tuning Process LoRA Llama-factory Base Open-Source Models MoE: Mixture of Experts Model RLHF: Reinforcement Learning from Human Feedback","head":[["link",{"rel":"alternate","hreflang":"zh-cn","href":"https://liz-in-tech.github.io/blog/zh/posts/llm/015_fine_tune.html"}],["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/blog/posts/llm/015_fine_tune.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"Fine-tuning"}],["meta",{"property":"og:description","content":"Fine-tuning Model Fine-tuning Process LoRA Llama-factory Base Open-Source Models MoE: Mixture of Experts Model RLHF: Reinforcement Learning from Human Feedback"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:locale:alternate","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-03-10T14:44:44.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:tag","content":"Fine-tuning"}],["meta",{"property":"article:published_time","content":"2024-11-05T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-03-10T14:44:44.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Fine-tuning\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-11-05T00:00:00.000Z\\",\\"dateModified\\":\\"2025-03-10T14:44:44.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. Fine-tuning","slug":"_1-fine-tuning","link":"#_1-fine-tuning","children":[{"level":3,"title":"1.1. Instruction Fine-tuning / Supervised Fine-tuning","slug":"_1-1-instruction-fine-tuning-supervised-fine-tuning","link":"#_1-1-instruction-fine-tuning-supervised-fine-tuning","children":[]},{"level":3,"title":"1.2. Multi-task Fine-tuning","slug":"_1-2-multi-task-fine-tuning","link":"#_1-2-multi-task-fine-tuning","children":[]}]},{"level":2,"title":"2. Model Fine-tuning Process","slug":"_2-model-fine-tuning-process","link":"#_2-model-fine-tuning-process","children":[]},{"level":2,"title":"3. Data Construction","slug":"_3-data-construction","link":"#_3-data-construction","children":[]},{"level":2,"title":"4. Fine-tuning Strategies","slug":"_4-fine-tuning-strategies","link":"#_4-fine-tuning-strategies","children":[{"level":3,"title":"4.1. Based on Fine-tuning Scope: Full and Partial Parameter Fine-tuning","slug":"_4-1-based-on-fine-tuning-scope-full-and-partial-parameter-fine-tuning","link":"#_4-1-based-on-fine-tuning-scope-full-and-partial-parameter-fine-tuning","children":[]},{"level":3,"title":"4.2. Based on Task: SFT, RLHF, RLAIF","slug":"_4-2-based-on-task-sft-rlhf-rlaif","link":"#_4-2-based-on-task-sft-rlhf-rlaif","children":[]},{"level":3,"title":"4.3. Low-resource Fine-tuning","slug":"_4-3-low-resource-fine-tuning","link":"#_4-3-low-resource-fine-tuning","children":[]},{"level":3,"title":"4.4. DeepSpeed","slug":"_4-4-deepspeed","link":"#_4-4-deepspeed","children":[]}]},{"level":2,"title":"5. LoRA and QLoRA","slug":"_5-lora-and-qlora","link":"#_5-lora-and-qlora","children":[{"level":3,"title":"5.1. LoRA","slug":"_5-1-lora","link":"#_5-1-lora","children":[]},{"level":3,"title":"5.2. QLoRA","slug":"_5-2-qlora","link":"#_5-2-qlora","children":[]}]},{"level":2,"title":"6. Fine-tuning Practice","slug":"_6-fine-tuning-practice","link":"#_6-fine-tuning-practice","children":[{"level":3,"title":"6.1. 3 Key Components","slug":"_6-1-3-key-components","link":"#_6-1-3-key-components","children":[]},{"level":3,"title":"6.2. Advanced Fine-tuning Settings","slug":"_6-2-advanced-fine-tuning-settings","link":"#_6-2-advanced-fine-tuning-settings","children":[]},{"level":3,"title":"6.3. GitHub: tloen/alpaca-lora","slug":"_6-3-github-tloen-alpaca-lora","link":"#_6-3-github-tloen-alpaca-lora","children":[]},{"level":3,"title":"6.4. LLaMA-Factory","slug":"_6-4-llama-factory","link":"#_6-4-llama-factory","children":[]},{"level":3,"title":"6.5. More","slug":"_6-5-more","link":"#_6-5-more","children":[]}]},{"level":2,"title":"7. Open-Source Models","slug":"_7-open-source-models","link":"#_7-open-source-models","children":[{"level":3,"title":"7.1. Mistral-7B","slug":"_7-1-mistral-7b","link":"#_7-1-mistral-7b","children":[]}]},{"level":2,"title":"8. Mixture of Experts Model (MoE)","slug":"_8-mixture-of-experts-model-moe","link":"#_8-mixture-of-experts-model-moe","children":[{"level":3,"title":"MoE Advantages","slug":"moe-advantages","link":"#moe-advantages","children":[]}]},{"level":2,"title":"9. RLHF (Reinforcement Learning from Human Feedback)","slug":"_9-rlhf-reinforcement-learning-from-human-feedback","link":"#_9-rlhf-reinforcement-learning-from-human-feedback","children":[]},{"level":2,"title":"10. References","slug":"_10-references","link":"#_10-references","children":[]}],"git":{"createdTime":1731475585000,"updatedTime":1741617884000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro.local","commits":1},{"name":"unknown","email":"15721607377@163.com","commits":1}]},"readingTime":{"minutes":7.32,"words":2197},"filePathRelative":"posts/llm/015_fine_tune.md","localizedDate":"November 5, 2024","excerpt":"<h1> Fine-tuning</h1>\\n<ul>\\n<li>Model Fine-tuning Process</li>\\n<li>LoRA</li>\\n<li>Llama-factory</li>\\n<li>Base Open-Source Models</li>\\n<li>MoE: Mixture of Experts Model</li>\\n<li>RLHF: Reinforcement Learning from Human Feedback</li>\\n</ul>\\n","autoDesc":true}');export{e as data};
