const e=JSON.parse('{"key":"v-32300dfc","path":"/zh/posts/llm/025_distribution_and_parallelism_1.html","title":"分布式训练之二：并行编程 Parallel Programming","lang":"zh-CN","frontmatter":{"icon":"lightbulb","sidebar":false,"date":"2025-02-28T00:00:00.000Z","prev":"./026_distribution_and_parallelism_2","next":"./024_distribution_and_parallelism","category":["LLM"],"tag":["分布式","并行"],"description":"分布式训练之二：并行编程 Parallel Programming","head":[["link",{"rel":"alternate","hreflang":"en-us","href":"https://liz-in-tech.github.io/blog/posts/llm/025_distribution_and_parallelism_1.html"}],["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/blog/zh/posts/llm/025_distribution_and_parallelism_1.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"分布式训练之二：并行编程 Parallel Programming"}],["meta",{"property":"og:description","content":"分布式训练之二：并行编程 Parallel Programming"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:locale:alternate","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-03-08T14:32:06.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:tag","content":"分布式"}],["meta",{"property":"article:tag","content":"并行"}],["meta",{"property":"article:published_time","content":"2025-02-28T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-03-08T14:32:06.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"分布式训练之二：并行编程 Parallel Programming\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-02-28T00:00:00.000Z\\",\\"dateModified\\":\\"2025-03-08T14:32:06.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. 概览","slug":"_1-概览","link":"#_1-概览","children":[]},{"level":2,"title":"2. Broadcast","slug":"_2-broadcast","link":"#_2-broadcast","children":[]},{"level":2,"title":"3. Reduce & AllReduce","slug":"_3-reduce-allreduce","link":"#_3-reduce-allreduce","children":[]},{"level":2,"title":"4. Gather & AllGather","slug":"_4-gather-allgather","link":"#_4-gather-allgather","children":[]},{"level":2,"title":"5. Scatter & ReduceScatter","slug":"_5-scatter-reducescatter","link":"#_5-scatter-reducescatter","children":[]},{"level":2,"title":"6. Barrier","slug":"_6-barrier","link":"#_6-barrier","children":[]},{"level":2,"title":"7. PyTorch代码实现","slug":"_7-pytorch代码实现","link":"#_7-pytorch代码实现","children":[{"level":3,"title":"7.1. NCCL是什么","slug":"_7-1-nccl是什么","link":"#_7-1-nccl是什么","children":[]},{"level":3,"title":"7.2. Broadcast","slug":"_7-2-broadcast","link":"#_7-2-broadcast","children":[]},{"level":3,"title":"7.3. Reduce","slug":"_7-3-reduce","link":"#_7-3-reduce","children":[]},{"level":3,"title":"7.4. AllReduce","slug":"_7-4-allreduce","link":"#_7-4-allreduce","children":[]},{"level":3,"title":"7.5. Gather","slug":"_7-5-gather","link":"#_7-5-gather","children":[]},{"level":3,"title":"7.6. AllGather","slug":"_7-6-allgather","link":"#_7-6-allgather","children":[]},{"level":3,"title":"7.7. Scatter","slug":"_7-7-scatter","link":"#_7-7-scatter","children":[]},{"level":3,"title":"7.8. ReduceScatter","slug":"_7-8-reducescatter","link":"#_7-8-reducescatter","children":[]},{"level":3,"title":"7.9. Barrier","slug":"_7-9-barrier","link":"#_7-9-barrier","children":[]}]}],"git":{"createdTime":1741444326000,"updatedTime":1741444326000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":4.84,"words":1452},"filePathRelative":"zh/posts/llm/025_distribution_and_parallelism_1.md","localizedDate":"2025年2月28日","excerpt":"<h1> 分布式训练之二：并行编程 Parallel Programming</h1>\\n","autoDesc":true}');export{e as data};
