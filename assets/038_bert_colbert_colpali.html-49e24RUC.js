import{_ as t,a as l,b as s,c as r,d as o,e as m,f as c,g as d,h as u,i as h,j as p}from"./038_standard_retrieval_vs_colpali-42FzQEl5.js";import{_ as g}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as f,o as b,c as v,f as y,a as e,b as a,d as _,e as i}from"./app--klcE6vy.js";const w={},x=e("h1",{id:"bert-colbert-and-colpali",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#bert-colbert-and-colpali","aria-hidden":"true"},"#"),a(" BERT, ColBERT and ColPali")],-1),q=e("ul",null,[e("li",null,"BERT"),e("li",null,"ColBERT"),e("li",null,"ColPali")],-1),k=i('<h2 id="_1-bert" tabindex="-1"><a class="header-anchor" href="#_1-bert" aria-hidden="true">#</a> 1. BERT</h2><h3 id="_1-1-what-is-bert" tabindex="-1"><a class="header-anchor" href="#_1-1-what-is-bert" aria-hidden="true">#</a> 1.1. What is BERT?</h3><p>BERT, short for Bidirectional Encoder Representations from Transformers, is a language model based on the transformer architecture and excels in dense embedding and retrieval tasks. Unlike traditional sequential natural language processing methods that move from left to right of a sentence or vice versa, BERT grasps word context by analyzing the entire word sequence simultaneously, thereby generating dense embeddings.</p><figure><img src="'+t+'" alt="BERT vs GPT vs ELMo" tabindex="0" loading="lazy"><figcaption>BERT vs GPT vs ELMo</figcaption></figure><h3 id="_1-2-how-bert-generates-vector-embeddings" tabindex="-1"><a class="header-anchor" href="#_1-2-how-bert-generates-vector-embeddings" aria-hidden="true">#</a> 1.2. How BERT generates vector embeddings</h3><p>User query: Milvus is a vector database built for scalable similarity search.</p><figure><img src="'+l+'" alt="How BERT generates vector embeddings" tabindex="0" loading="lazy"><figcaption>How BERT generates vector embeddings</figcaption></figure><p>When we feed a query into BERT, the process unfolds as follows:</p><ul><li><p>Tokenization: Firstly, The text is first tokenized into a sequence of word pieces known as tokens. Then, it adds a special token [CLS] at the beginning of the sequence of generated tokens and a token [SEP] at the end of these tokens to separate sentences and indicate the end.</p></li><li><p>Embedding: Each token is converted into vectors using an embedding matrix, similar to models like Word2Vec. Positional embeddings are added to these token embeddings to preserve information about the order of words, while segment embeddings distinguish different sentences.</p></li><li><p>Encoders: The vectors pass through multiple layers of encoders, each composed of self-attention mechanisms and feed-forward neural networks. These layers iteratively refine the representation of each token based on the context provided by all other tokens in the sequence.</p></li><li><p>Dense Embeddings Output: The final layer outputs a sequence of embeddings. BERT generates dense embeddings that capture the meaning of individual words and their interrelations within sentences. This approach has proven immensely successful in various language understanding tasks, setting new standards in NLP benchmarks.</p></li><li><p>Pooling Operations (Optional): All token vectors are combined using pooling operations to form a unified dense representation.</p></li></ul><figure><img src="'+s+'" alt="From tokens to BERT dense embeddings" tabindex="0" loading="lazy"><figcaption>From tokens to BERT dense embeddings</figcaption></figure><h2 id="_2-colbert" tabindex="-1"><a class="header-anchor" href="#_2-colbert" aria-hidden="true">#</a> 2. ColBERT</h2><h3 id="_2-1-what-is-colbert" tabindex="-1"><a class="header-anchor" href="#_2-1-what-is-colbert" aria-hidden="true">#</a> 2.1. What is ColBERT?</h3><p>With the rise of natural language understanding (NLU) technologies, many neural ranking models have been developed to enhance traditional retrieval methods such as BM25.</p><p>ColBERT is a neural ranking model based on BERT with contextualized late interaction, and it marks an important shift from conventional embedding approaches.</p><ul><li>Unlike BERT, which consolidates token vectors into a singular representation, ColBERT maintains per-token representations, offering a finer granularity in similarity calculation.</li><li>What sets ColBERT apart is its introduction of a novel late interaction mechanism. This approach provides a more detailed and nuanced understanding of the semantic relationship between queries and documents.</li></ul><h4 id="_2-1-1-colbert-performance" tabindex="-1"><a class="header-anchor" href="#_2-1-1-colbert-performance" aria-hidden="true">#</a> 2.1.1. ColBERT Performance</h4><figure><img src="'+r+'" alt="ColBERT Performance" tabindex="0" loading="lazy"><figcaption>ColBERT Performance</figcaption></figure><p>ColBERT is not only competitive with existing BERT-based models—surpassing all non-BERT baselines—but also remarkably efficient, running up to two orders of magnitude faster while reducing the required FLOPs per query by four orders of magnitude.</p><h3 id="_2-2-understanding-colbert-architecture" tabindex="-1"><a class="header-anchor" href="#_2-2-understanding-colbert-architecture" aria-hidden="true">#</a> 2.2. Understanding ColBERT Architecture</h3><p>The diagram below demonstrates the general architecture of ColBERT, which comprises:</p><ul><li>A query encoder</li><li>A document encoder</li><li>The late interaction mechanism</li></ul><figure><img src="'+o+'" alt="The general architecture of ColBERT, given a query q and a document d" tabindex="0" loading="lazy"><figcaption>The general architecture of ColBERT, given a query q and a document d</figcaption></figure><h4 id="_2-2-1-the-query-encoder-to-transform-a-query-q-into-a-set-of-fixed-size-embeddings-eq" tabindex="-1"><a class="header-anchor" href="#_2-2-1-the-query-encoder-to-transform-a-query-q-into-a-set-of-fixed-size-embeddings-eq" aria-hidden="true">#</a> 2.2.1. The Query Encoder: to transform a query <code>Q</code> into a set of fixed-size embeddings <code>Eq</code></h4>',23),T=e("p",{class:"katex-block"},[e("span",{class:"katex-display"},[e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[e("semantics",null,[e("mrow",null,[e("mi",null,"E"),e("mi",null,"q"),e("mo",null,":"),e("mo",null,"="),e("mi",null,"N"),e("mi",null,"o"),e("mi",null,"r"),e("mi",null,"m"),e("mi",null,"a"),e("mi",null,"l"),e("mi",null,"i"),e("mi",null,"z"),e("mi",null,"e"),e("mo",{stretchy:"false"},"("),e("mi",null,"C"),e("mi",null,"N"),e("mi",null,"N"),e("mo",{stretchy:"false"},"("),e("mi",null,"B"),e("mi",null,"E"),e("mi",null,"R"),e("mi",null,"T"),e("mo",{stretchy:"false"},"("),e("mi",{mathvariant:"normal"},'"'),e("mo",{stretchy:"false"},"["),e("mi",null,"Q"),e("mo",{stretchy:"false"},"]"),e("mo",{separator:"true"},","),e("mi",null,"q"),e("mn",null,"0"),e("mo",{separator:"true"},","),e("mi",null,"q"),e("mn",null,"1"),e("mo",{separator:"true"},","),e("mi",{mathvariant:"normal"},"."),e("mi",{mathvariant:"normal"},"."),e("mi",{mathvariant:"normal"},"."),e("mi",null,"q"),e("mi",null,"l"),e("mo",{separator:"true"},","),e("mo",{stretchy:"false"},"["),e("mi",null,"m"),e("mi",null,"a"),e("mi",null,"s"),e("mi",null,"k"),e("mo",{stretchy:"false"},"]"),e("mo",{separator:"true"},","),e("mo",{stretchy:"false"},"["),e("mi",null,"m"),e("mi",null,"a"),e("mi",null,"s"),e("mi",null,"k"),e("mo",{stretchy:"false"},"]"),e("mo",{separator:"true"},","),e("mo",null,"…"),e("mo",{separator:"true"},","),e("mo",{stretchy:"false"},"["),e("mi",null,"m"),e("mi",null,"a"),e("mi",null,"s"),e("mi",null,"k"),e("mo",{stretchy:"false"},"]"),e("mi",{mathvariant:"normal"},'"'),e("mo",{stretchy:"false"},")"),e("mo",{stretchy:"false"},")"),e("mo",{stretchy:"false"},")")]),e("annotation",{encoding:"application/x-tex"},' Eq := Normalize( CNN( BERT("[Q], q0, q1, ...ql, [mask], [mask], …, [mask]") ) ) ')])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.8778em","vertical-align":"-0.1944em"}}),e("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"Eq"),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),e("span",{class:"mrel"},":="),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),e("span",{class:"mord mathnormal",style:{"margin-right":"0.10903em"}},"N"),e("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"or"),e("span",{class:"mord mathnormal"},"ma"),e("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),e("span",{class:"mord mathnormal"},"i"),e("span",{class:"mord mathnormal"},"ze"),e("span",{class:"mopen"},"("),e("span",{class:"mord mathnormal",style:{"margin-right":"0.10903em"}},"CNN"),e("span",{class:"mopen"},"("),e("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"BERT"),e("span",{class:"mopen"},"("),e("span",{class:"mord"},'"'),e("span",{class:"mopen"},"["),e("span",{class:"mord mathnormal"},"Q"),e("span",{class:"mclose"},"]"),e("span",{class:"mpunct"},","),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),e("span",{class:"mord"},"0"),e("span",{class:"mpunct"},","),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),e("span",{class:"mord"},"1"),e("span",{class:"mpunct"},","),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"mord"},"..."),e("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"ql"),e("span",{class:"mpunct"},","),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"mopen"},"["),e("span",{class:"mord mathnormal"},"ma"),e("span",{class:"mord mathnormal"},"s"),e("span",{class:"mord mathnormal",style:{"margin-right":"0.03148em"}},"k"),e("span",{class:"mclose"},"]"),e("span",{class:"mpunct"},","),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"mopen"},"["),e("span",{class:"mord mathnormal"},"ma"),e("span",{class:"mord mathnormal"},"s"),e("span",{class:"mord mathnormal",style:{"margin-right":"0.03148em"}},"k"),e("span",{class:"mclose"},"]"),e("span",{class:"mpunct"},","),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"minner"},"…"),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"mpunct"},","),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"mopen"},"["),e("span",{class:"mord mathnormal"},"ma"),e("span",{class:"mord mathnormal"},"s"),e("span",{class:"mord mathnormal",style:{"margin-right":"0.03148em"}},"k"),e("span",{class:"mclose"},"]"),e("span",{class:"mord"},'"'),e("span",{class:"mclose"},")))")])])])])],-1),E=e("ul",null,[e("li",null,"Step1: Tokenization"),e("li",null,"Step2: BERT"),e("li",null,"Step3: CNN"),e("li",null,"Step4: Normalization")],-1),C=e("h4",{id:"_2-2-2-the-document-encoder-to-transform-document-d-into-a-set-of-fixed-size-embeddings-ed",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-2-2-the-document-encoder-to-transform-document-d-into-a-set-of-fixed-size-embeddings-ed","aria-hidden":"true"},"#"),a(" 2.2.2. The Document Encoder: to transform document "),e("code",null,"D"),a(" into a set of fixed-size embeddings "),e("code",null,"Ed")],-1),B=e("p",{class:"katex-block"},[e("span",{class:"katex-display"},[e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[e("semantics",null,[e("mrow",null,[e("mi",null,"E"),e("mi",null,"d"),e("mo",null,":"),e("mo",null,"="),e("mi",null,"F"),e("mi",null,"i"),e("mi",null,"l"),e("mi",null,"t"),e("mi",null,"e"),e("mi",null,"r"),e("mo",{stretchy:"false"},"("),e("mi",null,"N"),e("mi",null,"o"),e("mi",null,"r"),e("mi",null,"m"),e("mi",null,"a"),e("mi",null,"l"),e("mi",null,"i"),e("mi",null,"z"),e("mi",null,"e"),e("mo",{stretchy:"false"},"("),e("mi",null,"C"),e("mi",null,"N"),e("mi",null,"N"),e("mo",{stretchy:"false"},"("),e("mi",null,"B"),e("mi",null,"E"),e("mi",null,"R"),e("mi",null,"T"),e("mo",{stretchy:"false"},"("),e("mi",{mathvariant:"normal"},'"'),e("mo",{stretchy:"false"},"["),e("mi",null,"D"),e("mo",{stretchy:"false"},"]"),e("mo",{separator:"true"},","),e("mi",null,"d"),e("mn",null,"0"),e("mo",{separator:"true"},","),e("mi",null,"d"),e("mn",null,"1"),e("mo",{separator:"true"},","),e("mi",{mathvariant:"normal"},"."),e("mi",{mathvariant:"normal"},"."),e("mi",{mathvariant:"normal"},"."),e("mo",{separator:"true"},","),e("mi",null,"d"),e("mi",null,"n"),e("mi",{mathvariant:"normal"},'"'),e("mo",{stretchy:"false"},")"),e("mo",{stretchy:"false"},")"),e("mo",{stretchy:"false"},")"),e("mo",{stretchy:"false"},")")]),e("annotation",{encoding:"application/x-tex"},' Ed := Filter( Normalize( CNN( BERT("[D], d0, d1, ..., dn") ) ) ) ')])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6944em"}}),e("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"E"),e("span",{class:"mord mathnormal"},"d"),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),e("span",{class:"mrel"},":="),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),e("span",{class:"base"},[e("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),e("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"F"),e("span",{class:"mord mathnormal"},"i"),e("span",{class:"mord mathnormal"},"lt"),e("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"er"),e("span",{class:"mopen"},"("),e("span",{class:"mord mathnormal",style:{"margin-right":"0.10903em"}},"N"),e("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"or"),e("span",{class:"mord mathnormal"},"ma"),e("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),e("span",{class:"mord mathnormal"},"i"),e("span",{class:"mord mathnormal"},"ze"),e("span",{class:"mopen"},"("),e("span",{class:"mord mathnormal",style:{"margin-right":"0.10903em"}},"CNN"),e("span",{class:"mopen"},"("),e("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"BERT"),e("span",{class:"mopen"},"("),e("span",{class:"mord"},'"'),e("span",{class:"mopen"},"["),e("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"D"),e("span",{class:"mclose"},"]"),e("span",{class:"mpunct"},","),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"mord mathnormal"},"d"),e("span",{class:"mord"},"0"),e("span",{class:"mpunct"},","),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"mord mathnormal"},"d"),e("span",{class:"mord"},"1"),e("span",{class:"mpunct"},","),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"mord"},"..."),e("span",{class:"mpunct"},","),e("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),e("span",{class:"mord mathnormal"},"d"),e("span",{class:"mord mathnormal"},"n"),e("span",{class:"mord"},'"'),e("span",{class:"mclose"},"))))")])])])])],-1),P=i('<ul><li>Step1: Tokenization</li><li>Step2: BERT</li><li>Step3: CNN</li><li>Step4: Normalization</li><li>Step5: Filter: to remove punctuation symbols based on a predefined list(aims to reduce the number of embeddings per document)</li></ul><h4 id="_2-2-3-the-late-interaction-mechanism-to-calculate-the-relevance-score-between-q-and-d-with-eq-and-ed-in-hand" tabindex="-1"><a class="header-anchor" href="#_2-2-3-the-late-interaction-mechanism-to-calculate-the-relevance-score-between-q-and-d-with-eq-and-ed-in-hand" aria-hidden="true">#</a> 2.2.3. The Late Interaction Mechanism: to calculate the relevance score between <code>Q</code> and <code>D</code> with Eq and Ed in hand</h4><h5 id="_2-2-3-1-what-is-late-interaction" tabindex="-1"><a class="header-anchor" href="#_2-2-3-1-what-is-late-interaction" aria-hidden="true">#</a> 2.2.3.1. What is &quot;Late interaction&quot;?</h5><p>In information retrieval, &quot;interaction&quot; refers to assessing the relevance between a query and a document by comparing their vector representations. &quot;Late interaction&quot; signifies that this comparison occurs after the query and document have been independently encoded. This approach contrasts &quot;early interaction&quot; models like BERT, where query and document embeddings interact at earlier stages, often before or during their encoding.</p><p>ColBERT adopts a late interaction mechanism that enables the pre-computation of query and document representations. It then employs a streamlined interaction step towards the end to compare the similarity of the already encoded representations. Compared to the early interaction approach, late interaction results in faster retrieval times and reduced computational requirements, making it well-suited for processing extensive document collections efficiently.</p><h5 id="_2-2-3-2-how-does-the-late-interaction-process-work" tabindex="-1"><a class="header-anchor" href="#_2-2-3-2-how-does-the-late-interaction-process-work" aria-hidden="true">#</a> 2.2.3.2. How does the late interaction process work?</h5><figure><img src="'+m+'" alt="Late Interaction Formula" tabindex="0" loading="lazy"><figcaption>Late Interaction Formula</figcaption></figure><ul><li>Step0: the encoders transform the query and document into lists of token-level embeddings <code>Eq</code> and <code>Ed</code></li><li>Step1: to perform a series of maximum similarity (MaxSim) computations （max-pooling） <ul><li>using either cosine similarity or squared L2 distance metrics</li></ul></li><li>Step2: aggregate the maximum similarity scores across all query tokens by summing them to obtain an overall relevance score for each document.</li><li>Step3: Rank the documents based on their relevance scores in descending order.</li></ul><h5 id="_2-2-3-3-unique-value-of-late-interaction" tabindex="-1"><a class="header-anchor" href="#_2-2-3-3-unique-value-of-late-interaction" aria-hidden="true">#</a> 2.2.3.3. Unique Value of &quot;Late interaction&quot;</h5><ul><li>Offline indexing: Document representations can be precomputed and stored, enabling efficient retrieval at query time.</li><li>Fine-grained matching: By preserving token-level interactions, late interaction captures nuanced similarities between queries and documents, leading to more accurate search and matching results.</li><li>Lightweight yet powerful interaction: The summation of maximum similarity (MaxSim) scores offers two key advantages. First, it serves as a particularly lightweight interaction mechanism. More importantly, it enables efficient top-k pruning during retrieval—delivering both speed and effectiveness.</li></ul><h3 id="_2-3-colbertv2-an-improved-colbert-for-better-retrieval-effectiveness-and-storage-efficiency" tabindex="-1"><a class="header-anchor" href="#_2-3-colbertv2-an-improved-colbert-for-better-retrieval-effectiveness-and-storage-efficiency" aria-hidden="true">#</a> 2.3. ColBERTv2: an Improved ColBERT for Better Retrieval Effectiveness and Storage Efficiency</h3><ul><li>Challenge: storage consumption <ul><li>Unlike Sentence-BERT, which generates a single vector per sentence, ColBERT generates one vector for each word piece token within a sentence. This approach is effective in similarity retrieval but inflates the space footprint of these models by order of magnitude, requiring heightened storage consumption during practical deployment in retrieval systems.</li></ul></li><li>Address: product quantization (PQ) <ul><li>ColBERTv2 was introduced to address these challenges. This iteration enhances ColBERT by integrating product quantization (PQ) with centroid-based encoding strategies. The application of product quantization allows ColBERTv2 to compress token embeddings without significant information loss, thus reducing storage costs while maintaining the model&#39;s retrieval effectiveness. This change optimizes the storage efficiency and retains the model&#39;s capability for fine-grained similarity assessments, making ColBERTv2 a more viable solution for large-scale retrieval systems.</li></ul></li></ul><h2 id="_3-colpali" tabindex="-1"><a class="header-anchor" href="#_3-colpali" aria-hidden="true">#</a> 3. ColPali</h2><h3 id="_3-1-what-is-colpali" tabindex="-1"><a class="header-anchor" href="#_3-1-what-is-colpali" aria-hidden="true">#</a> 3.1. What is ColPali</h3><p>ColPali is a <strong>multimodal model</strong> capable of processing both images and text to efficiently index and retrieve documents. It is an extension of <strong>PaliGemma-3B</strong> that produces <strong>ColBERT-style multi-vector representations</strong> for text and image content.</p><p><strong>Key Features</strong></p><ul><li><strong>Multimodal</strong>: Simultaneously handles both textual and visual information, allowing richer document representations.</li><li><strong>Multi-Vector Embeddings</strong>: Similar to ColBERT, ColPali generates a separate embedding vector for each patch of a page image, rather than representing the entire page with a single vector.</li><li><strong>Late Interaction</strong>: During retrieval, the model computes the similarity between each query token embedding and all patch embeddings of the document page. The final document-query relevance score is obtained by summing the maximum similarity for each query token across all patches.</li></ul><h3 id="_3-2-colpali-architecture" tabindex="-1"><a class="header-anchor" href="#_3-2-colpali-architecture" aria-hidden="true">#</a> 3.2. ColPali Architecture</h3><figure><img src="'+c+'" alt="ColPali architecture" tabindex="0" loading="lazy"><figcaption>ColPali architecture</figcaption></figure><h4 id="_3-2-1-related-model" tabindex="-1"><a class="header-anchor" href="#_3-2-1-related-model" aria-hidden="true">#</a> 3.2.1. Related Model</h4><ul><li>SigLIP: A powerful vision encoder that produces a single-vector embedding for an input, whether image or text.</li><li>Gemma: A language model focused on textual understanding.</li><li>PaliGemma: A multimodal model capable of processing both images and text, combining the strengths of SigLIP and Gemma.</li><li>ColPali: Integrates the multimodal capabilities of PaliGemma with the late interaction strategy of ColBERT. It leverages ViT patch embeddings to represent an entire PDF page as multiple embeddings, generated at the patch level.</li></ul><h4 id="_3-2-2-colpali-workflow" tabindex="-1"><a class="header-anchor" href="#_3-2-2-colpali-workflow" aria-hidden="true">#</a> 3.2.2. ColPali Workflow</h4><ul><li>The Offline Document Encoder <ul><li>Each document page is fed through the vision encoder (SigLIP)</li><li>A projection layer</li><li>The resulting image patch embeddings are processed by the language model (Gemma-2B)</li><li>A projection layer maps the output to a lower-dimensional space (D=128)</li></ul></li><li>The Online Query Encoder <ul><li>The query is encoded using the language model</li><li>A projection layer</li></ul></li><li>The Late Interaction Mechanism <ul><li>computes similarity scores between query tokens and document patches</li></ul></li></ul><h3 id="_3-3-colpali-performance" tabindex="-1"><a class="header-anchor" href="#_3-3-colpali-performance" aria-hidden="true">#</a> 3.3. ColPali Performance</h3><figure><img src="'+d+'" alt="Latency" tabindex="0" loading="lazy"><figcaption>Latency</figcaption></figure><p>To evaluate ColPali’s performance, the researchers introduced a new benchmark called ViDoRe (Visual Document Retrieval). This comprehensive benchmark includes various tasks spanning multiple domains, modalities, and languages.</p><figure><img src="'+u+'" alt="Accuracy" tabindex="0" loading="lazy"><figcaption>Accuracy</figcaption></figure><p>Some key results from the ViDoRe benchmark:</p><figure><img src="'+h+'" alt="key results" tabindex="0" loading="lazy"><figcaption>key results</figcaption></figure><p>As we can see, ColPali significantly outperforms other methods across a wide range of tasks, demonstrating its effectiveness in multimodal document retrieval.</p><h3 id="_3-4-the-problem-colpali-solves" tabindex="-1"><a class="header-anchor" href="#_3-4-the-problem-colpali-solves" aria-hidden="true">#</a> 3.4. The problem ColPali solves</h3><p>Traditional document retrieval systems face several challenges:</p><ul><li>Documents contain diverse visual elements—text, charts, tables, layouts, and even fonts. Converting all non-text elements into text can lose significant information.</li><li>Handling different document types and elements (text, images, tables, formulas, etc.) separately is time-consuming, fragile, and complex.</li></ul><p>ColPali addresses these issues by operating directly on document images, eliminating the need for complex preprocessing steps and enabling faster, more accurate retrieval.</p><figure><img src="'+p+'" alt="ColPali simplifies document retrieval" tabindex="0" loading="lazy"><figcaption>ColPali simplifies document retrieval</figcaption></figure><h3 id="_3-5-advantages-of-colpali" tabindex="-1"><a class="header-anchor" href="#_3-5-advantages-of-colpali" aria-hidden="true">#</a> 3.5. Advantages of ColPali</h3><ul><li>Efficiency: Enables fast indexing and ensures low latency during the querying phase.</li><li>Accuracy: By leveraging visual features, ColPali achieves higher accuracy in document retrieval, particularly for visually rich documents.</li><li>Flexible End-to-End Processing: ColPali can process a variety of documents—such as PDF, PPT, and Word files—containing diverse visual elements, including text, tables, and figures, in a fully end-to-end manner. By directly embedding document page images, it eliminates the need for complex and fragile pipelines such as OCR, layout analysis, or chunking, allowing a single model to simultaneously leverage both textual and visual content. , simplifying the entire retrieval workflow.</li><li>Interpretability: Offers visualizations highlighting which image patches contribute most to a retrieval decision, enhancing transparency and explainability.</li><li>Multilingual: Supports retrieval across documents in different languages.</li><li>Multimodal: Integrates textual and visual information seamlessly for more robust retrieval.</li></ul><h3 id="_3-6-potential-applications" tabindex="-1"><a class="header-anchor" href="#_3-6-potential-applications" aria-hidden="true">#</a> 3.6. Potential applications</h3><p>The capabilities of ColPali open up numerous possibilities for improving existing systems and enabling new applications:</p><ul><li>Enhanced RAG systems: By incorporating ColPali, RAG systems can better understand and retrieve information from visually rich documents.</li><li>Improved search engines: ColPali could significantly enhance the accuracy and speed of document search in large-scale systems.</li><li>Document analysis: The model’s ability to understand document layouts and visual elements makes it valuable for automated document analysis tasks.</li><li>Multimodal question answering: ColPali’s understanding of both text and visual elements makes it well-suited for answering questions about complex documents.</li><li>Legal and medical document retrieval: Fields that deal with large volumes of complex, multimodal documents could benefit greatly from ColPali’s capabilities.</li></ul><h3 id="_3-7-limitations-and-future-work" tabindex="-1"><a class="header-anchor" href="#_3-7-limitations-and-future-work" aria-hidden="true">#</a> 3.7. Limitations and future work</h3><p>While ColPali represents a significant advancement, there are still areas for improvement and further research:</p><ul><li>Memory Footprint: The multi-vector representation requires more storage than traditional single-vector embeddings.</li><li>Scalability: Further optimization may be needed for extremely large document corpora.</li><li>Integration with existing systems: Work is needed to integrate ColPali with popular vector databases and retrieval frameworks.</li></ul><p>Future research directions might include:</p><ul><li>Exploring sub-image decomposition techniques</li><li>Optimizing image patch resampling strategies</li><li>Incorporating hard-negative mining during training</li></ul><h3 id="_3-8-how-to-use" tabindex="-1"><a class="header-anchor" href="#_3-8-how-to-use" aria-hidden="true">#</a> 3.8. How to use</h3><ul><li>Official Version: https://huggingface.co/vidore/colpali-v1.3</li><li>ColQwen2.5: https://huggingface.co/tsystems/colqwen2.5-3b-multilingual-v1.0</li><li>Byaldi library: https://github.com/AnswerDotAI/byaldi</li><li>LitePali: https://litepali.com/</li><li>ColPali combined with Qwen2 VL notebook: https://github.com/merveenoyan/smol-vision/blob/main/ColPali_%2B_Qwen2_VL.ipynb</li></ul><h4 id="_3-8-1-official-version" tabindex="-1"><a class="header-anchor" href="#_3-8-1-official-version" aria-hidden="true">#</a> 3.8.1. Official Version</h4>',48),R={href:"https://huggingface.co/vidore/colpali-v1.3",target:"_blank",rel:"noopener noreferrer"},z=i(`<p>Install colpali-engine:</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>pip install colpali-engine&gt;=0.3.0,&lt;0.4.0
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>Then run the following code:</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>from typing import cast

import torch
from PIL import Image

from colpali_engine.models import ColPali, ColPaliProcessor

model_name = &quot;vidore/colpali-v1.3&quot;

model = ColPali.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map=&quot;cuda:0&quot;,  # or &quot;mps&quot; if on Apple Silicon
).eval()

processor = ColPaliProcessor.from_pretrained(model_name)

# Your inputs
images = [
    Image.new(&quot;RGB&quot;, (32, 32), color=&quot;white&quot;),
    Image.new(&quot;RGB&quot;, (16, 16), color=&quot;black&quot;),
]
queries = [
    &quot;Is attention really all you need?&quot;,
    &quot;Are Benjamin, Antoine, Merve, and Jo best friends?&quot;,
]

# Process the inputs
batch_images = processor.process_images(images).to(model.device)
batch_queries = processor.process_queries(queries).to(model.device)

# Forward pass
with torch.no_grad():
    image_embeddings = model(**batch_images)
    query_embeddings = model(**batch_queries)

scores = processor.score_multi_vector(query_embeddings, image_embeddings)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_4-reference" tabindex="-1"><a class="header-anchor" href="#_4-reference" aria-hidden="true">#</a> 4. Reference</h2><ul><li>ColPali Official <ul><li>ColPali Paper: https://arxiv.org/pdf/2407.01449</li><li>ColPali Github: https://github.com/illuin-tech/colpali</li><li>ColPali Huggingface (models, data, code and benchmarks): https://huggingface.co/vidore/colpali</li></ul></li><li>ColQwen2.5: https://huggingface.co/tsystems/colqwen2.5-3b-multilingual-v1.0</li><li>Bert Paper: https://arxiv.org/abs/1810.04805</li><li>ColBert Paper: https://arxiv.org/pdf/2004.12832</li><li>Bert &amp; ColBERT &amp; ColBERTv2: https://zilliz.com/learn/explore-colbert-token-level-embedding-and-ranking-model-for-similarity-search</li><li>Bert &amp; Exploring BGE-M3 and Splade: https://zilliz.com/learn/bge-m3-and-splade-two-machine-learning-models-for-generating-sparse-embeddings#BERT-The-Foundation-Model-for-BGE-M3-and-Splade</li><li>ColPali — Revolutionizing multimodal document retrieval: https://medium.com/@simeon.emanuilov/colpali-revolutionizing-multimodal-document-retrieval-324eab1cf480</li></ul>`,6);function L(N,S){const n=f("ExternalLinkIcon");return b(),v("div",null,[x,q,y(" more "),k,T,E,C,B,P,e("p",null,[a("For best performance, newer models are available ("),e("a",R,[a("vidore/colpali-v1.3"),_(n)]),a(")")]),z])}const F=g(w,[["render",L],["__file","038_bert_colbert_colpali.html.vue"]]);export{F as default};
