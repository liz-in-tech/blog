const e=JSON.parse('{"key":"v-349fb87b","path":"/posts/LLM/llama.html","title":"Llama Source Code Exploration","lang":"en-US","frontmatter":{"icon":"lightbulb","date":"2024-06-01T00:00:00.000Z","sticky":true,"star":true,"category":["LLM"],"tag":["LLM"],"description":"Llama Source Code Exploration About Llama Overall Architecture Hyperparameters Tensor Dimensionality Transformation Number of Trainable Parameters Source Code","head":[["link",{"rel":"alternate","hreflang":"zh-cn","href":"https://liz-starfield.github.io/blog/zh/posts/LLM/llama.html"}],["meta",{"property":"og:url","content":"https://liz-starfield.github.io/blog/posts/LLM/llama.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"Llama Source Code Exploration"}],["meta",{"property":"og:description","content":"Llama Source Code Exploration About Llama Overall Architecture Hyperparameters Tensor Dimensionality Transformation Number of Trainable Parameters Source Code"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:locale:alternate","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-06-03T01:33:06.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:published_time","content":"2024-06-01T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-06-03T01:33:06.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Llama Source Code Exploration\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-06-01T00:00:00.000Z\\",\\"dateModified\\":\\"2024-06-03T01:33:06.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-starfield\\"}]}"]]},"headers":[{"level":2,"title":"1. About","slug":"_1-about","link":"#_1-about","children":[]},{"level":2,"title":"2. Llama Overall Architecture","slug":"_2-llama-overall-architecture","link":"#_2-llama-overall-architecture","children":[]},{"level":2,"title":"3. Hyperparameters","slug":"_3-hyperparameters","link":"#_3-hyperparameters","children":[]},{"level":2,"title":"4. Tensor Dimensionality Transformation","slug":"_4-tensor-dimensionality-transformation","link":"#_4-tensor-dimensionality-transformation","children":[]},{"level":2,"title":"5. Number of Trainable Parameters","slug":"_5-number-of-trainable-parameters","link":"#_5-number-of-trainable-parameters","children":[]},{"level":2,"title":"6. Source Code","slug":"_6-source-code","link":"#_6-source-code","children":[{"level":3,"title":"6.1. Entrance","slug":"_6-1-entrance","link":"#_6-1-entrance","children":[]},{"level":3,"title":"6.2. GenerationMixin","slug":"_6-2-generationmixin","link":"#_6-2-generationmixin","children":[]},{"level":3,"title":"6.3. LlamaForCausalLM","slug":"_6-3-llamaforcausallm","link":"#_6-3-llamaforcausallm","children":[]},{"level":3,"title":"6.4. LlamaModel","slug":"_6-4-llamamodel","link":"#_6-4-llamamodel","children":[]},{"level":3,"title":"6.5. LlamaDecoderLayer","slug":"_6-5-llamadecoderlayer","link":"#_6-5-llamadecoderlayer","children":[]},{"level":3,"title":"6.6. LlamaRMSNorm","slug":"_6-6-llamarmsnorm","link":"#_6-6-llamarmsnorm","children":[]},{"level":3,"title":"6.7. LlamaSdpaAttention","slug":"_6-7-llamasdpaattention","link":"#_6-7-llamasdpaattention","children":[]},{"level":3,"title":"6.8. LlamaRotaryEmbedding","slug":"_6-8-llamarotaryembedding","link":"#_6-8-llamarotaryembedding","children":[]},{"level":3,"title":"6.9. LlamaMLP","slug":"_6-9-llamamlp","link":"#_6-9-llamamlp","children":[]}]}],"git":{"createdTime":1717378386000,"updatedTime":1717378386000,"contributors":[{"name":"unknown","email":"15721607377@163.com","commits":1}]},"readingTime":{"minutes":5.46,"words":1639},"filePathRelative":"posts/LLM/llama.md","localizedDate":"June 1, 2024","excerpt":"<h1> Llama Source Code Exploration</h1>\\n<ul>\\n<li>\\n<ol>\\n<li>About</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"2\\">\\n<li>Llama Overall Architecture</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"3\\">\\n<li>Hyperparameters</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"4\\">\\n<li>Tensor Dimensionality Transformation</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"5\\">\\n<li>Number of Trainable Parameters</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"6\\">\\n<li>Source Code</li>\\n</ol>\\n</li>\\n</ul>\\n","autoDesc":true}');export{e as data};
