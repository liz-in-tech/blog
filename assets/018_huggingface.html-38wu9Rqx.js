import{_ as r,a as s,b as t,c as o,d,e as u,f as c,g as m}from"./018_huggingface_task-I5iSVkQS.js";import{_ as p}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as g,o as f,c as h,f as v,a as e,b as i,d as n,e as a}from"./app-2p_NxrSB.js";const b={},_=e("h1",{id:"hugging-face-and-transformers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#hugging-face-and-transformers","aria-hidden":"true"},"#"),i(" Hugging Face and Transformers")],-1),x=e("ul",null,[e("li",null,"主要功能"),e("li",null,"Models"),e("li",null,"Datasets"),e("li",null,"Transformers")],-1),q=e("h2",{id:"_1-官网",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_1-官网","aria-hidden":"true"},"#"),i(" 1. 官网")],-1),A={href:"https://huggingface.co/",target:"_blank",rel:"noopener noreferrer"},P=e("p",null,"简介：Hugging Face 是一个专注于自然语言处理（NLP）和人工智能的开源平台，提供了丰富的工具和资源，它拥有当前最活跃、最受关注、影响力最大的LLM社区，最新最强的LLM大多在这里发布和开源。",-1),k=e("h2",{id:"_2-主要功能",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-主要功能","aria-hidden":"true"},"#"),i(" 2. 主要功能")],-1),T=a("<li>Models 模型库 <ul><li>模型下载</li><li>模型分享</li></ul></li><li>Datasets 数据集 <ul><li>数据集下载</li><li>数据集分享</li></ul></li><li>Spaces 分享空间 <ul><li>里面有近期有趣的比较火的应用</li><li>支持 Gradio 和 Streamlit 等工具，快速构建可视化界面</li><li>用于展示模型效果、测试模型性能或作为教学工具</li></ul></li><li>Trending 趋势 <ul><li>在首页展示了最近7天最火的Models，Datasets和Spaces</li></ul></li><li>Docs 文档库 <ul><li>HuggingFace核心库以及模型算法的说明使用文档</li></ul></li><li>核心库 <ul><li>Transformers库 <ul><li>Hugging Face 的核心开源库，提供了API和工具，可以轻松下载和训练最先进的预训练模型，支持多种深度学习框架（PyTorch,TensorFlow和JAX）其中Pytorch支持所有的模型和框架。</li></ul></li><li>Datasets库 <ul><li>提供了最大的现成机器学习数据集中心，带有快速、易用和高效的数据操作工具，提供统一的 API 来访问和管理各种机器学习数据集，便于处理不同数据源</li></ul></li><li>Tokenizers库 <ul><li>用于创建和使用分词器的库，对自然语言处理模型的文本处理至关重要，提供高性能和灵活性</li></ul></li><li>Evaluate库 <ul><li>用于评估和比较模型性能的库，提供简单统一的 API，用于评估机器学习模型在不同任务和指标上的性能，便于基准测试和比较不同模型。</li></ul></li><li>PEFT库 <ul><li>参数高效微调库，高效微调大语言模型，仅更新模型参数的一小部分，有助于在有限计算资源下适应新任务</li></ul></li><li>TRL (Transformer Reinforcement Learning)库 <ul><li>强化学习库，提供工具以强化学习技术训练大语言模型，便于监督微调、奖励建模和近端策略优化（PPO），用于大型语言模型的进一步微调</li></ul></li><li>Accelerate库 <ul><li>分布式训练和推理库，使同一代码能够在各种分布式配置（如单 GPU、多节点集群）上运行，仅需少量修改，支持自动混合精度和分布式训练，增强了大规模机器学习任务的效率</li></ul></li><li>Optimum库 <ul><li>优化加速库，使用易用的硬件优化工具加速模型的推理和训练，提供工具以优化模型在各种硬件上的性能，包括多 GPU 系统和 TPU，便于扩展和加速模型训练和推理。</li></ul></li><li>Gradio库 <ul><li>用于创建机器学习模型的用户界面，允许开发者快速构建和分享交互式机器学习应用，通过 Hugging Face Spaces 免费托管演示。</li></ul></li></ul></li><li>Daily Papers 每日精选论文 <ul><li>提供 AI 领域的最新研究论文，用户可以与作者互动并推荐相关论文</li></ul></li>",7),z={href:"https://huggingface.co/learn",target:"_blank",rel:"noopener noreferrer"},L=e("li",null,"社区论坛",-1),M=a('<h2 id="_3-models" tabindex="-1"><a class="header-anchor" href="#_3-models" aria-hidden="true">#</a> 3. Models</h2><h3 id="_3-1-模型列表页" tabindex="-1"><a class="header-anchor" href="#_3-1-模型列表页" aria-hidden="true">#</a> 3.1. 模型列表页</h3><ul><li>涵盖了各种任务，可根据任务选择模型，也可以直接搜索模型名称 <ul><li>Multimodal 多模态</li><li>Computer Vision 计算机视觉</li><li>Natural Language Processing 自然语言处理</li><li>Audio 语音</li><li>Tabular 表格</li><li>Reinforcement Learning 强化学习</li></ul></li><li>不带前缀是官方提供的模型，带前缀是第三方提供的模型</li></ul><figure><img src="'+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_3-2-模型详情页" tabindex="-1"><a class="header-anchor" href="#_3-2-模型详情页" aria-hidden="true">#</a> 3.2. 模型详情页</h3><ul><li>Model card: 模型介绍</li><li>File and versions: 模型文件</li><li>Use this model: 使用该模型的样例代码，一般都提供pipeline的方式</li><li>Inference Providers: 界面提供</li><li>Model tree: 模型的变体</li><li>Spaces using model: 使用该模型的Space</li></ul><figure><img src="'+s+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_3-3-模型文件" tabindex="-1"><a class="header-anchor" href="#_3-3-模型文件" aria-hidden="true">#</a> 3.3. 模型文件</h3><figure><img src="'+t+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>文档/元数据：提供使用指南、许可和版本控制 <ul><li>README.md：模型的说明文档，通常包含模型的介绍、使用方法、训练细节、性能评测等信息</li><li>LICENSE：开源许可协议，规定了该模型的使用、分发和修改权限</li><li>.gitattributes：用于 Git 版本控制，尤其是 Git LFS（Large File Storage），通常包含对大文件的 LFS 追踪规则</li></ul></li><li>模型配置文件：定义模型架构和推理参数 <ul><li>config.json：模型的主要配置文件，定义了模型架构（如层数、隐藏单元数、注意力头数等）</li><li>generation_config.json：生成文本时的默认参数配置文件，如最大长度、温度、top-k 采样等，用于控制推理阶段的文本生成方式</li></ul></li><li>模型权重文件：存储神经网络参数 <ul><li>safetensors文件：模型的主要权重文件，采用 safetensors 格式（比传统的 .bin 更安全、加载更快），通过 Git LFS 存储 <ul><li>model-00001-of-000002.safetensors</li><li>model-00002-of-000002.safetensors</li></ul></li><li>model.safetensors.index.json：索引文件，描述了模型权重在不同 .safetensors 文件中的存储位置，允许分片加载模型</li></ul></li><li>分词器：用于文本预处理和后处理 <ul><li>tokenizer.json：分词器的核心文件，包含所有 token 及其对应的 ID，用于文本编码和解码</li><li>tokenizer_config.json：定义分词器的相关参数，如是否使用特殊 token，是否是 BPE/WordPiece 分词器等</li></ul></li></ul><h2 id="_4-datasets" tabindex="-1"><a class="header-anchor" href="#_4-datasets" aria-hidden="true">#</a> 4. Datasets</h2><p>数据集列表页</p><figure><img src="'+o+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>数据集详情页</p><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_5-trending" tabindex="-1"><a class="header-anchor" href="#_5-trending" aria-hidden="true">#</a> 5. Trending</h2><p>Trending展示了近期比较火的模型、数据集和分享空间</p><figure><img src="'+u+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_6-transformers" tabindex="-1"><a class="header-anchor" href="#_6-transformers" aria-hidden="true">#</a> 6. Transformers</h2><h3 id="_6-1-transformers-api" tabindex="-1"><a class="header-anchor" href="#_6-1-transformers-api" aria-hidden="true">#</a> 6.1. Transformers API</h3><p>API 描述了所有类和函数：</p><ul><li>MAIN CLASSES : 详细介绍了最重要的类，如configuration, model, tokenizer, and pipeline <ul><li>Pipelines : 提供高层次 API，简化常见任务的执行</li><li>AutoClasses : 提供统一API自动加载模型、tokenizer 和配置的类</li><li>Configuration : 定义模型的参数和结构，确保模型的正确初始化</li><li>Models : 模型</li><li>Tokenizer : 分词器，对数据进行预处理，文本到 token 序列的互相转换</li><li>Trainer : 支持模型训练和微调，提供完整的 PyTorch 训练 API，支持分布式训练和混合精度</li></ul></li><li>MODELS : 详细介绍了与库中实现的每个模型相关的类和函数 <ul><li>text models</li><li>vision models</li><li>audio models</li><li>video models</li><li>multimodal models</li><li>reinforcement learning models</li><li>time series models</li><li>graph models</li></ul></li><li>INTERNAL HELPERS : 详细介绍了内部使用的实用类和函数</li></ul><h3 id="_6-2-pipelines" tabindex="-1"><a class="header-anchor" href="#_6-2-pipelines" aria-hidden="true">#</a> 6.2. Pipelines</h3><h4 id="_6-2-1-pipelines介绍" tabindex="-1"><a class="header-anchor" href="#_6-2-1-pipelines介绍" aria-hidden="true">#</a> 6.2.1. Pipelines介绍</h4><p>Pipeline是Transformers库的一个高层次封装类，将数据预处理、模型调用和结果后处理三部分组装成流水线。</p><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>pipeline() 是使用预训练模型进行推理的最简单、最快捷的方法。可以将 pipeline() 用于不同模式下的许多任务，下表显示了其中一些任务：</p><figure><img src="'+m+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="_6-2-2-pipelines使用" tabindex="-1"><a class="header-anchor" href="#_6-2-2-pipelines使用" aria-hidden="true">#</a> 6.2.2. Pipelines使用</h4><p>1个参数：只指定任务类型</p><ul><li>如果不指定模型，将下载目标任务的默认模型和配套tokenizer</li></ul><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>from transformers import pipeline

pipe = pipeline(&quot;text-classification&quot;)
pipe(&quot;This restaurant is awesome&quot;)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>1个参数：只指定模型名称</p><ul><li>模型有相应的任务类型和配套的 tokenizer</li></ul><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>from transformers import pipeline

pipe = pipeline(model=&quot;FacebookAI/roberta-large-mnli&quot;)
pipe(&quot;This restaurant is awesome&quot;)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>2个参数：指定任务类型和模型名称</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>from transformers import pipeline

messages = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who are you?&quot;},
]
pipe = pipeline(&quot;text-generation&quot;, model=&quot;Qwen/QwQ-32B&quot;)
pipe(messages)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>3个参数：指定任务类型、模型名称、embedding模型名称</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>from transformers import pipeline

oracle = pipeline(
    &quot;question-answering&quot;, model=&quot;distilbert/distilbert-base-cased-distilled-squad&quot;, tokenizer=&quot;google-bert/bert-base-cased&quot;
)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>3个参数：先加载模型再创建Pipeline</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code># Named entity recognition pipeline, passing in a specific model and tokenizer

from transformers import *

model = AutoModelForTokenClassification.from_pretrained(&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;google-bert/bert-base-cased&quot;)
recognizer = pipeline(&quot;ner&quot;, model=model, tokenizer=tokenizer)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>一般来说，第一次执行时pipeline会加载模型，模型会自动下载到本地，可以直接用</li><li>Pipeline加载模型的方式和先加载模型再创建Pipeline其中的加载模型方式一致1，都是用Auto Classes的方式进行加载</li></ul>`,42),S={href:"https://huggingface.co/docs/transformers/main_classes/pipelines#pipelines",target:"_blank",rel:"noopener noreferrer"},C=a(`<h3 id="_6-3-autoclasses" tabindex="-1"><a class="header-anchor" href="#_6-3-autoclasses" aria-hidden="true">#</a> 6.3. AutoClasses</h3><ul><li>AutoClasses可以从from_pretrained()方法的传参，也就是通过预训练模型的名称或路径，自动检索预训练模型的架构来进行自动加载Model、Tokenizer和Config</li><li>主要包括 AutoConfig、AutoModel 和 AutoTokenizer</li></ul><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>from transformers import AutoConfig

config = AutoConfig.from_pretrained(&quot;google-bert/bert-base-uncased&quot;)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>from transformers import AutoModel

model = AutoModel.from_pretrained(&quot;google-bert/bert-base-cased&quot;)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;google-bert/bert-base-uncased&quot;)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>特定任务的 AutoModel类（如 AutoModelForSequenceClassification），后缀ForSequenceClassification指明了任务类型，之所以有特定任务AutoModel类，是因为骨干模型可以处理多个任务：相同的 model，接入不同的 post processing 模块，就可以用于不同的任务</li><li>这些类通过提供统一的 API，使代码更灵活，适合不同模型，它们在简化模型使用方面非常有效。</li></ul>`,6),I={href:"https://huggingface.co/docs/transformers/model_doc/auto",target:"_blank",rel:"noopener noreferrer"};function F(E,y){const l=g("ExternalLinkIcon");return f(),h("div",null,[_,x,v(" more "),q,e("p",null,[i("官网："),e("a",A,[i("https://huggingface.co/"),n(l)])]),P,k,e("ul",null,[T,e("li",null,[i("Learn 课程 "),e("ul",null,[e("li",null,[e("a",z,[i("https://huggingface.co/learn"),n(l)])])])]),L]),M,e("p",null,[i("Pipeline官方文档："),e("a",S,[i("https://huggingface.co/docs/transformers/main_classes/pipelines#pipelines"),n(l)])]),C,e("p",null,[i("Auto Classes官方文档："),e("a",I,[i("https://huggingface.co/docs/transformers/model_doc/auto"),n(l)])])])}const w=p(b,[["render",F],["__file","018_huggingface.html.vue"]]);export{w as default};
