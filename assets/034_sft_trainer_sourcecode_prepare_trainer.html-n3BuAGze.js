import{_ as s}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as a,c as t,f as p,e as n}from"./app-kXW_Y_m4.js";const e={},o=n('<h1 id="sfttrainer-sourcecode-prepare-train" tabindex="-1"><a class="header-anchor" href="#sfttrainer-sourcecode-prepare-train" aria-hidden="true">#</a> SFTTrainer Sourcecode -- Prepare Train</h1><ul><li>Prepare Train Overall Logic</li><li>Prepare Train Code Details <ul><li>_inner_training_loop</li><li>training_step</li><li>compute_loss</li><li>PeftModelForCausalLM.forward</li><li>Linear4bit.forward</li></ul></li></ul>',2),l=n(`<h2 id="_1-prepare-train-overall-logic" tabindex="-1"><a class="header-anchor" href="#_1-prepare-train-overall-logic" aria-hidden="true">#</a> 1. Prepare Train Overall Logic</h2><p>Overall Logic</p><ul><li>Initialize SFTTrainer</li><li>Execute trainer.train() <ul><li>Execute inner_training_loop() <ul><li>get_train_dataloader</li><li>Setting up training control variables <ul><li>Number of training epochs: num_train_epochs</li><li>Number of update steps per epoch: num_update_steps_per_epoch</li><li>Total number of update steps: max_steps <ul><li>If max_steps is not set, then max_steps == num_train_epochs * num_update_steps_per_epoch</li></ul></li><li>Global batch size per update step: total_train_batch_size = self._train_batch_size * args.gradient_accumulation_steps * args.world_size</li></ul></li><li>Begin running training</li><li>If args.eval_on_start is True, perform one validation before training</li><li>Epochs loop <ul><li>Load data: train_dataloader</li><li>Update Steps loop (global_batch_size, i.e., the batch size for each parameter update) <ul><li>Get batch data: batch_samples <ul><li>batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches, args.device)</li></ul></li><li>Batch loop (micro_batch_size) <ul><li>Forward pass: outputs = model(**inputs) # **inputs is the batch data <ul><li>PeftModelForCausalLM.forward <ul><li>inputs_embeds = self.word_embeddings(input_ids)</li><li>return self.base_model(inputs_embeds=inputs_embeds, **kwargs)</li></ul></li><li>Linear4bit.forward <ul><li>result = self.base_layer(x, *args, **kwargs)</li><li>output = lora_B(lora_A(dropout(x))) * scaling</li><li>result = result + output</li><li>return result</li></ul></li></ul></li><li>Calculate Loss = outputs[&quot;loss&quot;] if isinstance(outputs, dict) else outputs[0]</li><li>Backward pass: self.accelerator.backward(loss, **kwargs)</li></ul></li><li>Gradient accumulation and parameter update: self.optimizer.step()</li></ul></li></ul></li></ul></li></ul></li></ul><p>Core Training Logic</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs_trained<span class="token punctuation">,</span> num_train_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># Epoch loop</span>
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> inputs <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>batch_samples<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># Batch loop</span>
        tr_loss_step <span class="token operator">=</span> self<span class="token punctuation">.</span>training_step<span class="token punctuation">(</span>model<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> num_items_in_batch<span class="token punctuation">)</span> <span class="token comment"># Execute a single training step (forward pass, loss calculation, and backward pass), returns the loss value tr_loss_step for current batch</span>
        tr_loss <span class="token operator">=</span> tr_loss <span class="token operator">+</span> tr_loss_step <span class="token comment"># Accumulate loss</span>
        <span class="token keyword">if</span> do_sync_step<span class="token punctuation">:</span> <span class="token comment"># Check if synchronization step is needed (do_sync_step indicates whether this is the final gradient accumulation step or the last step of the epoch)</span>
            <span class="token triple-quoted-string string">&quot;&quot;&quot;
            Supports gradient accumulation, allowing gradients from multiple batches to be accumulated before parameter updates
            Optimizer updates are only triggered when do_sync_step is True, ensuring a balance between computational efficiency and memory usage
            &quot;&quot;&quot;</span>
            self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># Update model parameters using accumulated gradients</span>
            <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>accelerator<span class="token punctuation">.</span>optimizer_step_was_skipped<span class="token punctuation">:</span> <span class="token comment"># Check if optimizer update was skipped (e.g., due to gradient overflow or NaN)</span>
                self<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># Update current learning rate (dynamic learning rate adjustment, only executed when optimizer updates successfully, maintaining consistency with training progress)</span>
            model<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># Clear gradients for next computation</span>
            self<span class="token punctuation">.</span>state<span class="token punctuation">.</span>global_step <span class="token operator">+=</span> <span class="token number">1</span> <span class="token comment"># Increment global training step count</span>
            self<span class="token punctuation">.</span>_maybe_log_save_evaluate<span class="token punctuation">(</span>tr_loss<span class="token punctuation">,</span> grad_norm<span class="token punctuation">,</span> model<span class="token punctuation">,</span> trial<span class="token punctuation">,</span> epoch<span class="token punctuation">,</span> ignore_keys_for_eval<span class="token punctuation">,</span> start_time<span class="token punctuation">,</span> learning_rate<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span> <span class="token comment"># Log metrics, save model checkpoints, perform evaluation</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_2-prepare-train-code-details" tabindex="-1"><a class="header-anchor" href="#_2-prepare-train-code-details" aria-hidden="true">#</a> 2. Prepare Train Code Details</h2><h3 id="_2-1-sfttrainer-init" tabindex="-1"><a class="header-anchor" href="#_2-1-sfttrainer-init" aria-hidden="true">#</a> 2.1. SFTTrainer.<strong>init</strong></h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">SFTTrainer</span><span class="token punctuation">(</span>Trainer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Trainer for Supervised Fine-Tuning (SFT) method.
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>
            model<span class="token operator">=</span>model<span class="token punctuation">,</span>
            args<span class="token operator">=</span>args<span class="token punctuation">,</span>
            data_collator<span class="token operator">=</span>data_collator<span class="token punctuation">,</span>
            train_dataset<span class="token operator">=</span>train_dataset<span class="token punctuation">,</span>
            eval_dataset<span class="token operator">=</span>eval_dataset<span class="token punctuation">,</span>
            processing_class<span class="token operator">=</span>processing_class<span class="token punctuation">,</span>
            compute_loss_func<span class="token operator">=</span>compute_loss_func<span class="token punctuation">,</span>
            compute_metrics<span class="token operator">=</span>compute_metrics<span class="token punctuation">,</span>
            callbacks<span class="token operator">=</span>callbacks<span class="token punctuation">,</span>
            optimizers<span class="token operator">=</span>optimizers<span class="token punctuation">,</span>
            preprocess_logits_for_metrics<span class="token operator">=</span>preprocess_logits_for_metrics<span class="token punctuation">,</span>
            <span class="token operator">**</span>super_init_kwargs<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-2-trainer-train" tabindex="-1"><a class="header-anchor" href="#_2-2-trainer-train" aria-hidden="true">#</a> 2.2. Trainer.train()</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Trainer</span>
    <span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Main training entry point.
        &quot;&quot;&quot;</span>

        <span class="token keyword">return</span> inner_training_loop<span class="token punctuation">(</span>
            args<span class="token operator">=</span>args<span class="token punctuation">,</span>
            resume_from_checkpoint<span class="token operator">=</span>resume_from_checkpoint<span class="token punctuation">,</span>
            trial<span class="token operator">=</span>trial<span class="token punctuation">,</span>
            ignore_keys_for_eval<span class="token operator">=</span>ignore_keys_for_eval<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-3-inner-training-loop" tabindex="-1"><a class="header-anchor" href="#_2-3-inner-training-loop" aria-hidden="true">#</a> 2.3. _inner_training_loop</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">_inner_training_loop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Data loader and number of training steps</span>
    train_dataloader <span class="token operator">=</span> self<span class="token punctuation">.</span>get_train_dataloader<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># Setting up training control variables:</span>
    <span class="token comment"># number of training epochs: num_train_epochs</span>
    <span class="token comment"># number of training steps per epoch: num_update_steps_per_epoch</span>
    <span class="token comment"># total number of training steps to execute: max_steps</span>
    total_train_batch_size <span class="token operator">=</span> self<span class="token punctuation">.</span>_train_batch_size <span class="token operator">*</span> args<span class="token punctuation">.</span>gradient_accumulation_steps <span class="token operator">*</span> args<span class="token punctuation">.</span>world_size
    <span class="token punctuation">(</span>
        num_train_epochs<span class="token punctuation">,</span>
        num_update_steps_per_epoch<span class="token punctuation">,</span>
        num_examples<span class="token punctuation">,</span>
        num_train_samples<span class="token punctuation">,</span>
        epoch_based<span class="token punctuation">,</span>
        len_dataloader<span class="token punctuation">,</span>
        max_steps<span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>set_initial_training_values<span class="token punctuation">(</span>args<span class="token punctuation">,</span> train_dataloader<span class="token punctuation">,</span> total_train_batch_size<span class="token punctuation">)</span>

    <span class="token comment"># Train!</span>
    logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">&quot;***** Running training *****&quot;</span><span class="token punctuation">)</span>
    logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;  Num examples = </span><span class="token interpolation"><span class="token punctuation">{</span>num_examples<span class="token punctuation">:</span><span class="token format-spec">,</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;  Num Epochs = </span><span class="token interpolation"><span class="token punctuation">{</span>num_train_epochs<span class="token punctuation">:</span><span class="token format-spec">,</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;  Instantaneous batch size per device = </span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>args<span class="token punctuation">.</span>per_device_train_batch_size<span class="token punctuation">:</span><span class="token format-spec">,</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>args<span class="token punctuation">.</span>per_device_train_batch_size <span class="token operator">!=</span> self<span class="token punctuation">.</span>_train_batch_size<span class="token punctuation">:</span>
        logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;  Training with DataParallel so batch size has been adjusted to: </span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>_train_batch_size<span class="token punctuation">:</span><span class="token format-spec">,</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;  Total train batch size (w. parallel, distributed &amp; accumulation) = </span><span class="token interpolation"><span class="token punctuation">{</span>total_train_batch_size<span class="token punctuation">:</span><span class="token format-spec">,</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;  Gradient Accumulation steps = </span><span class="token interpolation"><span class="token punctuation">{</span>args<span class="token punctuation">.</span>gradient_accumulation_steps<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;  Total optimization steps = </span><span class="token interpolation"><span class="token punctuation">{</span>max_steps<span class="token punctuation">:</span><span class="token format-spec">,</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
    logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;  Number of trainable parameters = </span><span class="token interpolation"><span class="token punctuation">{</span>get_model_param_count<span class="token punctuation">(</span>model<span class="token punctuation">,</span> trainable_only<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">,</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>

    <span class="token comment"># tr_loss is a tensor to avoid synchronization of TPUs through .item()</span>
    tr_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> device<span class="token operator">=</span>args<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    <span class="token comment"># _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses</span>
    self<span class="token punctuation">.</span>_total_loss_scalar <span class="token operator">=</span> <span class="token number">0.0</span>
    self<span class="token punctuation">.</span>_globalstep_last_logged <span class="token operator">=</span> self<span class="token punctuation">.</span>state<span class="token punctuation">.</span>global_step
    model<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    grad_norm<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">float</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span>
    learning_rate <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">if</span> args<span class="token punctuation">.</span>eval_on_start<span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>_evaluate<span class="token punctuation">(</span>trial<span class="token punctuation">,</span> ignore_keys_for_eval<span class="token punctuation">,</span> skip_scheduler<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

    epochs_trained <span class="token operator">=</span> <span class="token number">0</span>

    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs_trained<span class="token punctuation">,</span> num_train_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        epoch_dataloader <span class="token operator">=</span> train_dataloader
        
        steps_in_epoch <span class="token operator">=</span> <span class="token punctuation">(</span>
            <span class="token builtin">len</span><span class="token punctuation">(</span>epoch_dataloader<span class="token punctuation">)</span>
            <span class="token keyword">if</span> len_dataloader <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
            <span class="token keyword">else</span> args<span class="token punctuation">.</span>max_steps <span class="token operator">*</span> args<span class="token punctuation">.</span>gradient_accumulation_steps
        <span class="token punctuation">)</span>
        
        step <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>
        epoch_iterator <span class="token operator">=</span> <span class="token builtin">iter</span><span class="token punctuation">(</span>epoch_dataloader<span class="token punctuation">)</span>
        <span class="token comment"># We chunkify the epoch iterator into gradient accumulation steps \`n\` batches</span>
        remainder <span class="token operator">=</span> num_examples <span class="token operator">%</span> args<span class="token punctuation">.</span>gradient_accumulation_steps
        <span class="token keyword">if</span> remainder <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            remainder <span class="token operator">=</span> args<span class="token punctuation">.</span>gradient_accumulation_steps
        update_step <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>
        total_updates <span class="token operator">=</span> steps_in_epoch <span class="token operator">//</span> args<span class="token punctuation">.</span>gradient_accumulation_steps <span class="token operator">+</span> <span class="token number">1</span>
        <span class="token keyword">if</span> args<span class="token punctuation">.</span>gradient_accumulation_steps <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
            total_updates <span class="token operator">-=</span> <span class="token number">1</span>
            
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>total_updates<span class="token punctuation">)</span><span class="token punctuation">:</span>
            update_step <span class="token operator">+=</span> <span class="token number">1</span>
            num_batches <span class="token operator">=</span> args<span class="token punctuation">.</span>gradient_accumulation_steps <span class="token keyword">if</span> update_step <span class="token operator">!=</span> <span class="token punctuation">(</span>total_updates <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">else</span> remainder
            batch_samples<span class="token punctuation">,</span> num_items_in_batch <span class="token operator">=</span> self<span class="token punctuation">.</span>get_batch_samples<span class="token punctuation">(</span>epoch_iterator<span class="token punctuation">,</span> num_batches<span class="token punctuation">,</span> args<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            <span class="token keyword">for</span> i<span class="token punctuation">,</span> inputs <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>batch_samples<span class="token punctuation">)</span><span class="token punctuation">:</span>
                step <span class="token operator">+=</span> <span class="token number">1</span>

                do_sync_step <span class="token operator">=</span> <span class="token punctuation">(</span>step <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> args<span class="token punctuation">.</span>gradient_accumulation_steps <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> <span class="token punctuation">(</span>step <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> steps_in_epoch

                tr_loss_step <span class="token operator">=</span> self<span class="token punctuation">.</span>training_step<span class="token punctuation">(</span>model<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> num_items_in_batch<span class="token punctuation">)</span>
                
                <span class="token keyword">if</span> <span class="token punctuation">(</span>
                    args<span class="token punctuation">.</span>logging_nan_inf_filter
                    <span class="token keyword">and</span> <span class="token keyword">not</span> is_torch_xla_available<span class="token punctuation">(</span><span class="token punctuation">)</span>
                    <span class="token keyword">and</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>isnan<span class="token punctuation">(</span>tr_loss_step<span class="token punctuation">)</span> <span class="token keyword">or</span> torch<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>tr_loss_step<span class="token punctuation">)</span><span class="token punctuation">)</span>
                <span class="token punctuation">)</span><span class="token punctuation">:</span>
                    <span class="token comment"># if loss is nan or inf simply add the average of previous logged losses</span>
                    tr_loss <span class="token operator">=</span> tr_loss <span class="token operator">+</span> tr_loss <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>state<span class="token punctuation">.</span>global_step <span class="token operator">-</span> self<span class="token punctuation">.</span>_globalstep_last_logged<span class="token punctuation">)</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    <span class="token keyword">if</span> tr_loss<span class="token punctuation">.</span>device <span class="token operator">!=</span> tr_loss_step<span class="token punctuation">.</span>device<span class="token punctuation">:</span>
                        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
                            <span class="token string-interpolation"><span class="token string">f&quot;Calculated loss must be on the original device: </span><span class="token interpolation"><span class="token punctuation">{</span>tr_loss<span class="token punctuation">.</span>device<span class="token punctuation">}</span></span><span class="token string"> but device in use is </span><span class="token interpolation"><span class="token punctuation">{</span>tr_loss_step<span class="token punctuation">.</span>device<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span>
                        <span class="token punctuation">)</span>
                    tr_loss <span class="token operator">=</span> tr_loss <span class="token operator">+</span> tr_loss_step

                <span class="token keyword">if</span> do_sync_step<span class="token punctuation">:</span>
                    self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

                    <span class="token comment"># get leaning rate before update</span>
                    learning_rate <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_learning_rate<span class="token punctuation">(</span><span class="token punctuation">)</span>

                    model<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
                    self<span class="token punctuation">.</span>state<span class="token punctuation">.</span>global_step <span class="token operator">+=</span> <span class="token number">1</span>
                    self<span class="token punctuation">.</span>state<span class="token punctuation">.</span>epoch <span class="token operator">=</span> epoch <span class="token operator">+</span> <span class="token punctuation">(</span>step <span class="token operator">+</span> <span class="token number">1</span> <span class="token operator">+</span> steps_skipped<span class="token punctuation">)</span> <span class="token operator">/</span> steps_in_epoch
    
    logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">&quot;\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n&quot;</span><span class="token punctuation">)</span>
    
    <span class="token keyword">if</span> args<span class="token punctuation">.</span>load_best_model_at_end <span class="token keyword">and</span> self<span class="token punctuation">.</span>state<span class="token punctuation">.</span>best_model_checkpoint <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>_load_best_model<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># add remaining tr_loss</span>
    self<span class="token punctuation">.</span>_total_loss_scalar <span class="token operator">+=</span> tr_loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
    effective_global_step <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>state<span class="token punctuation">.</span>global_step<span class="token punctuation">,</span> <span class="token number">0.001</span><span class="token punctuation">)</span>  <span class="token comment"># Avoid ZeroDivisionError</span>
    train_loss <span class="token operator">=</span> self<span class="token punctuation">.</span>_total_loss_scalar <span class="token operator">/</span> effective_global_step

    <span class="token keyword">return</span> TrainOutput<span class="token punctuation">(</span>self<span class="token punctuation">.</span>state<span class="token punctuation">.</span>global_step<span class="token punctuation">,</span> train_loss<span class="token punctuation">,</span> metrics<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-4-training-step" tabindex="-1"><a class="header-anchor" href="#_2-4-training-step" aria-hidden="true">#</a> 2.4. training_step</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">training_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Perform a training step on a batch of inputs.
    &quot;&quot;&quot;</span>

    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>optimizer<span class="token punctuation">,</span> <span class="token string">&quot;train&quot;</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token builtin">callable</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>train<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    inputs <span class="token operator">=</span> self<span class="token punctuation">.</span>_prepare_inputs<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
    
    loss <span class="token operator">=</span> self<span class="token punctuation">.</span>compute_loss<span class="token punctuation">(</span>model<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> num_items_in_batch<span class="token operator">=</span>num_items_in_batch<span class="token punctuation">)</span>
    
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>args<span class="token punctuation">.</span>n_gpu <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
        loss <span class="token operator">=</span> loss<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># mean() to average on multi-gpu parallel training</span>

    <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_apex<span class="token punctuation">:</span>
        <span class="token keyword">with</span> amp<span class="token punctuation">.</span>scale_loss<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> self<span class="token punctuation">.</span>optimizer<span class="token punctuation">)</span> <span class="token keyword">as</span> scaled_loss<span class="token punctuation">:</span>
            scaled_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> 
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token comment"># Finally we need to normalize the loss for reporting</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>model_accepts_loss_kwargs <span class="token keyword">and</span> self<span class="token punctuation">.</span>compute_loss_func <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            loss <span class="token operator">=</span> loss <span class="token operator">/</span> self<span class="token punctuation">.</span>args<span class="token punctuation">.</span>gradient_accumulation_steps

        self<span class="token punctuation">.</span>accelerator<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> 

        <span class="token keyword">return</span> loss<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-5-compute-loss" tabindex="-1"><a class="header-anchor" href="#_2-5-compute-loss" aria-hidden="true">#</a> 2.5. compute_loss</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">compute_loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>label_smoother <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">or</span> self<span class="token punctuation">.</span>compute_loss_func <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token string">&quot;labels&quot;</span> <span class="token keyword">in</span> inputs<span class="token punctuation">:</span>
            labels <span class="token operator">=</span> inputs<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token string">&quot;labels&quot;</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            labels <span class="token operator">=</span> <span class="token boolean">None</span>
    
    outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span>

    <span class="token keyword">if</span> labels <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        unwrapped_model <span class="token operator">=</span> self<span class="token punctuation">.</span>accelerator<span class="token punctuation">.</span>unwrap_model<span class="token punctuation">(</span>model<span class="token punctuation">)</span>
        <span class="token keyword">if</span> _is_peft_model<span class="token punctuation">(</span>unwrapped_model<span class="token punctuation">)</span><span class="token punctuation">:</span>
            model_name <span class="token operator">=</span> unwrapped_model<span class="token punctuation">.</span>base_model<span class="token punctuation">.</span>model<span class="token punctuation">.</span>_get_name<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            model_name <span class="token operator">=</span> unwrapped_model<span class="token punctuation">.</span>_get_name<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># User-defined compute_loss function</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>compute_loss_func <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            loss <span class="token operator">=</span> self<span class="token punctuation">.</span>compute_loss_func<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> num_items_in_batch<span class="token operator">=</span>num_items_in_batch<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> model_name <span class="token keyword">in</span> MODEL_FOR_CAUSAL_LM_MAPPING_NAMES<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            loss <span class="token operator">=</span> self<span class="token punctuation">.</span>label_smoother<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> shift_labels<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            loss <span class="token operator">=</span> self<span class="token punctuation">.</span>label_smoother<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> <span class="token builtin">dict</span><span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token string">&quot;loss&quot;</span> <span class="token keyword">not</span> <span class="token keyword">in</span> outputs<span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
                <span class="token string">&quot;The model did not return a loss from the inputs, only the following keys: &quot;</span>
                <span class="token string-interpolation"><span class="token string">f&quot;</span><span class="token interpolation"><span class="token punctuation">{</span><span class="token string">&#39;,&#39;</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">. For reference, the inputs it received are </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token string">&#39;,&#39;</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">.&quot;</span></span>
            <span class="token punctuation">)</span>
        <span class="token comment"># We don&#39;t use .loss here since the model may return tuples instead of ModelOutput.</span>
        loss <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token string">&quot;loss&quot;</span><span class="token punctuation">]</span> <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> <span class="token builtin">dict</span><span class="token punctuation">)</span> <span class="token keyword">else</span> outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    
    <span class="token keyword">return</span> loss
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-6-peftmodelforcausallm-forward" tabindex="-1"><a class="header-anchor" href="#_2-6-peftmodelforcausallm-forward" aria-hidden="true">#</a> 2.6. PeftModelForCausalLM.forward</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">PeftModelForCausalLM</span><span class="token punctuation">(</span>PeftModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Peft model for causal language modeling.
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
            self<span class="token punctuation">,</span>
            input_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
            attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
            inputs_embeds<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
            labels<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
            output_attentions<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
            output_hidden_states<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
            return_dict<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
            task_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
            <span class="token operator">**</span>kwargs<span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        batch_size <span class="token operator">=</span> _get_batch_size<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> inputs_embeds<span class="token punctuation">)</span>
        <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token comment"># concat prompt attention mask</span>
            prefix_attention_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> peft_config<span class="token punctuation">.</span>num_virtual_tokens<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>attention_mask<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            attention_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>prefix_attention_mask<span class="token punctuation">,</span> attention_mask<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> inputs_embeds <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            inputs_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>word_embeddings<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>
        <span class="token comment"># concat prompt labels</span>
        <span class="token keyword">if</span> labels <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            prefix_labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> peft_config<span class="token punctuation">.</span>num_virtual_tokens<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>labels<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            kwargs<span class="token punctuation">[</span><span class="token string">&quot;labels&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>prefix_labels<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        prompts <span class="token operator">=</span> self<span class="token punctuation">.</span>get_prompt<span class="token punctuation">(</span>batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> task_ids<span class="token operator">=</span>task_ids<span class="token punctuation">)</span>
        prompts <span class="token operator">=</span> prompts<span class="token punctuation">.</span>to<span class="token punctuation">(</span>inputs_embeds<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        inputs_embeds <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>prompts<span class="token punctuation">,</span> inputs_embeds<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>base_model<span class="token punctuation">(</span>inputs_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-7-linear4bit-forward" tabindex="-1"><a class="header-anchor" href="#_2-7-linear4bit-forward" aria-hidden="true">#</a> 2.7. Linear4bit.forward</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Linear4bit</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span> LoraLayer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Lora implemented in a dense layer</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>_check_forward_args<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        adapter_names <span class="token operator">=</span> kwargs<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token string">&quot;adapter_names&quot;</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>disable_adapters<span class="token punctuation">:</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>merged<span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>unmerge<span class="token punctuation">(</span><span class="token punctuation">)</span>
            result <span class="token operator">=</span> self<span class="token punctuation">.</span>base_layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> adapter_names <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            result <span class="token operator">=</span> self<span class="token punctuation">.</span>_mixed_batch_forward<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> adapter_names<span class="token operator">=</span>adapter_names<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>merged<span class="token punctuation">:</span>
            result <span class="token operator">=</span> self<span class="token punctuation">.</span>base_layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            result <span class="token operator">=</span> self<span class="token punctuation">.</span>base_layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
            <span class="token comment"># As per Tim Dettmers, for 4bit, we need to defensively clone here.</span>
            <span class="token comment"># The reason is that in some cases, an error can occur that backprop</span>
            <span class="token comment"># does not work on a manipulated view. This issue may be solved with</span>
            <span class="token comment"># newer PyTorch versions but this would need extensive testing to be</span>
            <span class="token comment"># sure.</span>
            result <span class="token operator">=</span> result<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>

            <span class="token keyword">for</span> active_adapter <span class="token keyword">in</span> self<span class="token punctuation">.</span>active_adapters<span class="token punctuation">:</span>
                <span class="token keyword">if</span> active_adapter <span class="token keyword">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>lora_A<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                    <span class="token keyword">continue</span>
                lora_A <span class="token operator">=</span> self<span class="token punctuation">.</span>lora_A<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span>
                lora_B <span class="token operator">=</span> self<span class="token punctuation">.</span>lora_B<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span>
                dropout <span class="token operator">=</span> self<span class="token punctuation">.</span>lora_dropout<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span>
                scaling <span class="token operator">=</span> self<span class="token punctuation">.</span>scaling<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span>

                requires_conversion <span class="token operator">=</span> <span class="token keyword">not</span> torch<span class="token punctuation">.</span>is_autocast_enabled<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token keyword">if</span> requires_conversion<span class="token punctuation">:</span>
                    expected_dtype <span class="token operator">=</span> result<span class="token punctuation">.</span>dtype
                    x <span class="token operator">=</span> self<span class="token punctuation">.</span>_cast_input_dtype<span class="token punctuation">(</span>x<span class="token punctuation">,</span> lora_A<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>

                <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>use_dora<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span><span class="token punctuation">:</span>
                    output <span class="token operator">=</span> lora_B<span class="token punctuation">(</span>lora_A<span class="token punctuation">(</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> scaling
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>dropout<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Identity<span class="token punctuation">)</span> <span class="token keyword">or</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>training<span class="token punctuation">:</span>
                        base_result <span class="token operator">=</span> result
                    <span class="token keyword">else</span><span class="token punctuation">:</span>
                        x <span class="token operator">=</span> dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
                        base_result <span class="token operator">=</span> <span class="token boolean">None</span>

                    output <span class="token operator">=</span> self<span class="token punctuation">.</span>lora_magnitude_vector<span class="token punctuation">[</span>active_adapter<span class="token punctuation">]</span><span class="token punctuation">(</span>
                        x<span class="token punctuation">,</span>
                        lora_A<span class="token operator">=</span>lora_A<span class="token punctuation">,</span>
                        lora_B<span class="token operator">=</span>lora_B<span class="token punctuation">,</span>
                        scaling<span class="token operator">=</span>scaling<span class="token punctuation">,</span>
                        base_layer<span class="token operator">=</span>self<span class="token punctuation">.</span>get_base_layer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                        base_result<span class="token operator">=</span>base_result<span class="token punctuation">,</span>
                    <span class="token punctuation">)</span>
                <span class="token keyword">if</span> requires_conversion<span class="token punctuation">:</span>
                    output <span class="token operator">=</span> output<span class="token punctuation">.</span>to<span class="token punctuation">(</span>expected_dtype<span class="token punctuation">)</span>
                result <span class="token operator">=</span> result <span class="token operator">+</span> output

        <span class="token keyword">return</span> result
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,20);function c(i,u){return a(),t("div",null,[o,p(" more "),l])}const d=s(e,[["render",c],["__file","034_sft_trainer_sourcecode_prepare_trainer.html.vue"]]);export{d as default};
