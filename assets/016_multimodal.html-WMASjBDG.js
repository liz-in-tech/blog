import{_ as t,a as e,b as n,c as r,d as o,e as u,f as p,g as m,h as g,i as c,j as h,k as d,l as f,m as _,n as b,o as y,p as v,q as x}from"./016_sora-sk0IGMY5.js";import{_ as M}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as A,o as I,c as L,f as T,a as l,b as i,d as S,e as a}from"./app-taMZPyBp.js";const z={},G=l("h1",{id:"多模态大模型",tabindex:"-1"},[l("a",{class:"header-anchor",href:"#多模态大模型","aria-hidden":"true"},"#"),i(" 多模态大模型")],-1),P=l("ul",null,[l("li",null,"多模态任务"),l("li",null,"多模态大模型的训练"),l("li",null,"Stable Diffusion"),l("li",null,"LLaVA")],-1),C=a('<h2 id="_1-多模态" tabindex="-1"><a class="header-anchor" href="#_1-多模态" aria-hidden="true">#</a> 1. 多模态</h2><p>模态（Modality）：信号的类型（数据的类型或形式）</p><ul><li>文本</li><li>图像</li><li>视频</li><li>音频</li><li>更细分 <ul><li>Graph图</li><li>表格</li></ul></li></ul><p>多模态 (Multimodal)：设计两种或更多不同类型的模态 （现实场景中通常会涉及到多种信号）</p><p>多模态模型 (Multimodal Model)：能处理和整合多种模态数据的AI模型。</p><p>多模态系统（Multimodal System）: 能够处理多种模态输入和输出的系统</p><p>多模态大模型（Multimodal Large Language Models, MLLMs）: 将额外的模态融入大预言模型（LLMs）, 也就是将大语言模型扩展到多种数据类型。</p><p>多模态大模型用于解决：模态之间的转换</p><ul><li>文本-&gt;图像</li><li>文本-&gt;视频</li><li>文本-&gt;表格</li><li>文本-&gt;Graph图</li><li>图像/视频-&gt;文本</li></ul><h2 id="_2-多模态任务" tabindex="-1"><a class="header-anchor" href="#_2-多模态任务" aria-hidden="true">#</a> 2. 多模态任务</h2><p>最火的两种模态就是语言和视觉模态，与之相关的任务主要分为两类：生成和视觉语言理解</p><ul><li>语言模态 + 视觉模态 （文本+图像） <ul><li>生成 <ul><li>文生图：输入文本生成图像 <ul><li>DALL-E系列 <ul><li>OpenAI提出来的</li><li>有API接口</li><li>https://platform.openai.com/docs/guides/images?context=node</li></ul></li><li>Midjourney <ul><li>生成的图效果最好</li><li>收费</li><li>官方文档：https://docs.midjourney.com/</li><li>Prompt参考库：https://midlibrary.io/</li></ul></li><li>Stable Diffusion <ul><li>开源，个人PC能跑</li><li>Web UI: https://github.com/AUTOMATIC1111/stable-diffusion-webui</li><li>模型下载网站：Civitai是一个专为Stable Diffusion AI艺术模型设计的网站，是非常好的AI模型库 https://civitai.com/</li><li>扩展插件-界面汉化：https://github.com/VinsonLaro/stable-diffusion-webui-chinese</li><li>扩展插件-ControlNet：https://github.com/Mikubill/sd-webui-controlnet</li><li>OpenPose: 根据人物骨架姿势生成图片</li><li>Canny边缘检测：参考草图轮廓生成图片</li></ul></li></ul></li><li>修改图片：输入原图和文本，生成修改的图</li></ul></li><li>理解 <ul><li>视觉问题问答 <ul><li>输入图片和文本，要求根据图文回答问题</li></ul></li><li>生成图片描述 <ul><li>输入图片，生成图片的文字描述</li></ul></li><li>图像分类 <ul><li>例如，OCR提取图像中的文字，对图像进行分类</li></ul></li><li>基于文本的图像检索 <ul><li>方法1：为图像生成图像描述，当用户输入文本查询时，找出与该文本最相似的图像描述文本，进而找到相关图像 <ul><li><strong>BLIP模型</strong>：图像转文本描述</li></ul></li><li>方法2：训练图像和文本的联合向量空间，用户输入文本，系统生成该文本的向量，直接找到与该文本最相关的图像向量，从而找到图像。 <ul><li><strong>CLIP模型</strong>：把图像和文本映射到共享向量空间</li></ul></li></ul></li></ul></li></ul></li><li>语言模态 + 听觉模态 （文本+音频） <ul><li>各种与音频有关的任务：https://github.com/AIGC-Audio/AudioGPT</li><li>文本转音频 TTS（text-to-speech） <ul><li>edge-tts <ul><li>https://github.com/rany2/edge-tts</li><li>通过python使用Microsoft Edge的在线文本转语音服务，而无需Microsoft Edge或Windows或API密钥</li></ul></li><li>MeloTTS <ul><li>https://github.com/myshell-ai/MeloTTS</li><li>https://huggingface.co/spaces/mrfakename/MeloTTS</li><li>MyShell.ai 的高质量多语言文本转语音库。支持英语、西班牙语、法语、中文、日语和韩语。</li><li>特点 <ul><li>速度快，在CPU上也能实现实时语音合成</li><li>多语言</li><li>中英混合</li><li>安装方便</li></ul></li></ul></li></ul></li><li>音频转文本 STT（speech to text）、ASR(Automatic Speech Recognition, 自动语音识别)</li><li>声音克隆 <ul><li>GPT-SoVITS <ul><li>https://github.com/RVC-Boss/GPT-SoVITS</li></ul></li></ul></li><li>生成音乐 <ul><li>suno <ul><li>https://www.suno.ai/: 根据提示词生成带完整歌词和旋律的歌曲</li></ul></li></ul></li></ul></li></ul><h2 id="_3-stable-diffusion-sd" tabindex="-1"><a class="header-anchor" href="#_3-stable-diffusion-sd" aria-hidden="true">#</a> 3. Stable Diffusion （SD)</h2><figure><img src="'+t+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+e+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+n+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Stable Diffusion是一个由多个组件和模型组成的系统，而非单一的模型。</p><ul><li>文本编码器 Text Encoder <ul><li>将文本向量化，以捕捉文本中的语义信息</li></ul></li><li>图像生成器 Image Generator <ul><li>图像生成器完全在图像信息空间（潜在空间）中工作，这一特性使得扩散模型比之前在像素空间工作的扩散模型更快</li><li>组件 <ul><li>图像信息创建器 Image Information Creator</li><li>图像解码器 Image Decoder</li></ul></li></ul></li></ul><figure><img src="'+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+o+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+u+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+m+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>扩散模型的3个主要组件（每个都有各自的神经网络）</p><ul><li>Clip Text （蓝色） <ul><li>用于文本编码</li><li>输入：文本</li><li>输出：77个表征向量，每个有768维度</li></ul></li><li>网格网络 + 调度 UNet + Scheduler （粉色） <ul><li>逐渐扩散信息到潜在latents空间</li><li>扩散 diffusion: 对信息一步一步step by step处理，每一步增加更多的相关信息，使得高质量图像得以最终生成</li><li>整个diffusion过程包含多个steps，其中每个step都是基于输入的latents矩阵进行操作，并生成另一个latents矩阵以更好地贴合「输入的文本」和从模型图像集中获取的「视觉信息」。</li><li>输入：文本向量 和 随机的初始图像信息矩阵（也成为latents）</li><li>输出：处理后的信息数组（维度：(4,64,64)）</li></ul></li><li>自编码解码器 Autoencoder decoder （橙色） <ul><li>绘出最后的图像</li><li>输入：处理过的信息数组</li><li>输出：结果图像（维度：(3,512,512)）</li></ul></li></ul><p>Diffusion的工作原理 扩散模型 Duffusion Model</p><ul><li>前向扩散 Forward Diffusion <ul><li>将噪声添加到训练图像中，逐渐将其转换为没有特点的噪声图像。</li></ul></li><li>反向/逆向扩散 Reverse Diffusion <ul><li>从嘈杂、无意义的图像开始，反向扩散恢复了猫或狗的图像</li></ul></li></ul><figure><img src="'+h+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_4-多模态大模型的训练" tabindex="-1"><a class="header-anchor" href="#_4-多模态大模型的训练" aria-hidden="true">#</a> 4. 多模态大模型的训练</h2><ul><li>传统方法训练 <ul><li>End-to-End Training <ul><li>Image Caption任务: Image -&gt; Description/Caption</li><li>Image -&gt; CNN -&gt; Vector(共享) -&gt; RNN/LSTM -&gt; 文本 （Encoder-Decoder结构）</li><li>训练数据：(Image1,Des1)(Image2,Des2)...(ImageN,DesN)</li></ul></li><li>问题 <ul><li>从0开始训练，训练成本高</li><li>每个任务都需要大量数据，且没有大模型的情况下人工标注困难</li></ul></li></ul></li><li>多模态大模型的训练 <ul><li>基础模型 Foundation Models <ul><li>文本领域 <ul><li>GPT4, LLaMA, ChatGLM, Qwen</li></ul></li><li>图像领域 Image <ul><li>Clip</li></ul></li><li>视频领域 Video <ul><li>Sora</li></ul></li><li>图 Graph <ul><li>GNN</li></ul></li></ul></li><li>多模态系统 <ul><li>文本（语言模型）作为中驱（因为所有的其他模态都归结为表达的含义）</li><li>图像/视频/图 -&gt; 有各自的适配器Adapter -&gt; Align Language model, Image model, Video model and Graph model (使各模态同频)</li><li>优势 <ul><li>训练成本低 <ul><li>因为只需要训练每个模态的适配器，各模态的基础模型参数进行冻结不变 <ul><li>Stage 1: Pre-training for Feature Alignment <ul><li>只有适配器部分被更新</li><li>之所以需要这一步骤，是因为一开始适配器部分完全是新引入的，不起任何作用</li><li>这部分需要的数据量比较大</li></ul></li><li>Stage 2: Fine-tuning End-to-End <ul><li>适配器部分和语言模型部分被更新</li><li>这部分需要的数据量比较小</li></ul></li></ul></li></ul></li><li>每个任务容易适配</li></ul></li></ul></li></ul></li></ul><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_5-flamingo" tabindex="-1"><a class="header-anchor" href="#_5-flamingo" aria-hidden="true">#</a> 5. Flamingo</h2><p>Github: https://github.com/lucidrains/flamingo-pytorch</p><figure><img src="'+f+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Input: image + text + image + text (图像和文本交替排列)</li><li>Vision Encoder: 处理图像的基础模型 Foundation Models</li><li>Perceiver Resampler: 适配器 Adapter</li><li>LM block: 语言模型</li></ul><h2 id="_6-llava" tabindex="-1"><a class="header-anchor" href="#_6-llava" aria-hidden="true">#</a> 6. LLaVA</h2><p>Github: https://github.com/haotian-liu/LLaVA</p><p>Paper Name: Visual Instruction Tuning Paper: https://arxiv.org/pdf/2304.08485</p><figure><img src="'+_+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>',41),N=l("ul",null,[l("li",null,[i("Vision Encoder: 处理图像的基础模型 Clip "),l("ul",null,[l("li",null,[i("provides the visual feature "),l("span",{class:"katex"},[l("span",{class:"katex-mathml"},[l("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[l("semantics",null,[l("mrow",null,[l("msub",null,[l("mi",null,"Z"),l("mi",null,"v")]),l("mo",null,"="),l("mi",null,"g"),l("mo",{stretchy:"false"},"("),l("msub",null,[l("mi",null,"X"),l("mi",null,"v")]),l("mo",{stretchy:"false"},")")]),l("annotation",{encoding:"application/x-tex"},"Z_v = g(X_v)")])])]),l("span",{class:"katex-html","aria-hidden":"true"},[l("span",{class:"base"},[l("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),l("span",{class:"mord"},[l("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"Z"),l("span",{class:"msupsub"},[l("span",{class:"vlist-t vlist-t2"},[l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.1514em"}},[l("span",{style:{top:"-2.55em","margin-left":"-0.0715em","margin-right":"0.05em"}},[l("span",{class:"pstrut",style:{height:"2.7em"}}),l("span",{class:"sizing reset-size6 size3 mtight"},[l("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"v")])])]),l("span",{class:"vlist-s"},"​")]),l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.15em"}},[l("span")])])])])]),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),l("span",{class:"mrel"},"="),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),l("span",{class:"base"},[l("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),l("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"g"),l("span",{class:"mopen"},"("),l("span",{class:"mord"},[l("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),l("span",{class:"msupsub"},[l("span",{class:"vlist-t vlist-t2"},[l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.1514em"}},[l("span",{style:{top:"-2.55em","margin-left":"-0.0785em","margin-right":"0.05em"}},[l("span",{class:"pstrut",style:{height:"2.7em"}}),l("span",{class:"sizing reset-size6 size3 mtight"},[l("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"v")])])]),l("span",{class:"vlist-s"},"​")]),l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.15em"}},[l("span")])])])])]),l("span",{class:"mclose"},")")])])])])])]),l("li",null,[i("Projection W: 适配器 Adapter, 转换为和文本相同维度的向量 "),l("ul",null,[l("li",null,[i("apply a trainable projection matrix W to convert "),l("span",{class:"katex"},[l("span",{class:"katex-mathml"},[l("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[l("semantics",null,[l("mrow",null,[l("msub",null,[l("mi",null,"Z"),l("mi",null,"v")])]),l("annotation",{encoding:"application/x-tex"},"Z_v")])])]),l("span",{class:"katex-html","aria-hidden":"true"},[l("span",{class:"base"},[l("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),l("span",{class:"mord"},[l("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"Z"),l("span",{class:"msupsub"},[l("span",{class:"vlist-t vlist-t2"},[l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.1514em"}},[l("span",{style:{top:"-2.55em","margin-left":"-0.0715em","margin-right":"0.05em"}},[l("span",{class:"pstrut",style:{height:"2.7em"}}),l("span",{class:"sizing reset-size6 size3 mtight"},[l("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"v")])])]),l("span",{class:"vlist-s"},"​")]),l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.15em"}},[l("span")])])])])])])])]),i(" into language embedding tokens H")]),l("li",null,[l("span",{class:"katex"},[l("span",{class:"katex-mathml"},[l("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[l("semantics",null,[l("mrow",null,[l("msub",null,[l("mi",null,"H"),l("mi",null,"v")]),l("mo",null,"="),l("mi",null,"W"),l("mo",{separator:"true"},"⋅"),l("msub",null,[l("mi",null,"Z"),l("mi",null,"v")])]),l("annotation",{encoding:"application/x-tex"},"H_v = W · Z_v")])])]),l("span",{class:"katex-html","aria-hidden":"true"},[l("span",{class:"base"},[l("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),l("span",{class:"mord"},[l("span",{class:"mord mathnormal",style:{"margin-right":"0.08125em"}},"H"),l("span",{class:"msupsub"},[l("span",{class:"vlist-t vlist-t2"},[l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.1514em"}},[l("span",{style:{top:"-2.55em","margin-left":"-0.0813em","margin-right":"0.05em"}},[l("span",{class:"pstrut",style:{height:"2.7em"}}),l("span",{class:"sizing reset-size6 size3 mtight"},[l("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"v")])])]),l("span",{class:"vlist-s"},"​")]),l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.15em"}},[l("span")])])])])]),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),l("span",{class:"mrel"},"="),l("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),l("span",{class:"base"},[l("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),l("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),l("span",{class:"mpunct"},"⋅"),l("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),l("span",{class:"mord"},[l("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"Z"),l("span",{class:"msupsub"},[l("span",{class:"vlist-t vlist-t2"},[l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.1514em"}},[l("span",{style:{top:"-2.55em","margin-left":"-0.0715em","margin-right":"0.05em"}},[l("span",{class:"pstrut",style:{height:"2.7em"}}),l("span",{class:"sizing reset-size6 size3 mtight"},[l("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"v")])])]),l("span",{class:"vlist-s"},"​")]),l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.15em"}},[l("span")])])])])])])])])])])]),l("li",null,"Language Model: Vicuna")],-1),w=a('<p>只需要训练每个模态的适配器，各模态的基础模型参数进行冻结不变</p><ul><li>Stage 1: Pre-training for Feature Alignment <ul><li>只有适配器部分被更新</li></ul></li><li>Stage 2: Fine-tuning End-to-End <ul><li>适配器部分和语言模型部分被更新</li></ul></li></ul><p>数据生成：GPT-assisted Visual Instruction Data Generation</p><ul><li>提供给GPT的prompt包括文字描述Captions和边界框bounding boxes, 注意不包括图像本身，使用的GPT也是纯语言模型 <ul><li>文字描述 Caption</li><li>边界框 bounding box</li></ul></li><li>GPT给出响应包括 <ul><li>问答对 QA Conversation</li><li>详细描述 Detailed Description</li><li>复杂推理 Complex Reasoning</li></ul></li></ul><figure><img src="'+b+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_7-minigpt-4" tabindex="-1"><a class="header-anchor" href="#_7-minigpt-4" aria-hidden="true">#</a> 7. MiniGPT-4</h2><p>Github: https://github.com/Vision-CAIR/MiniGPT-4</p><p>Paper Name: MINIGPT-4: ENHANCING VISION-LANGUAGE UNDERSTANDING WITH ADVANCED LARGE LANGUAGE MODELS</p><p>Paper: https://arxiv.org/pdf/2304.10592</p><figure><img src="'+y+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_8-sora-视频生成大模型" tabindex="-1"><a class="header-anchor" href="#_8-sora-视频生成大模型" aria-hidden="true">#</a> 8. Sora 视频生成大模型</h2><p>Paper Name: Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models</p><p>Paper: https://arxiv.org/pdf/2402.17177</p><p>Github: https://github.com/lichao-sun/SoraReview</p><p>Note: This is not an official technical report from OpenAI.</p><figure><img src="'+v+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Sora本质上是一个具备灵活采样维度的diffusion transformer模型。它包括三个部分：</p><ul><li><ol><li>时空压缩器：将原始视频映射到潜在空间</li></ol><ul><li>A time-space compressor：maps the original video into latent space.</li></ul></li><li><ol start="2"><li>视觉转换器 ViT: 处理标记化的潜在表示并输出去噪的潜在表示</li></ol><ul><li>A ViT then processes the tokenized latent representation and outputs the denoised latent representation.</li></ul></li><li><ol start="3"><li>类CLIP模型: 引导视频生成过程，创造出具有特定风格或主题的视频</li></ol><ul><li>A CLIP-like conditioning mechanism receives LLM-augmented user instructions and potentially visual prompts to guide the diffusion model to generate styled or themed videos.</li></ul></li></ul><figure><img src="'+x+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_9-多模态大模型展望" tabindex="-1"><a class="header-anchor" href="#_9-多模态大模型展望" aria-hidden="true">#</a> 9. 多模态大模型展望</h2><p>现状</p><ul><li>处于早期阶段</li><li>技术迭代特别快</li><li>长远来看，是大模型的终点</li></ul><p>大模型领域的发展分析</p><ul><li>多模态大模型的基础是文本大模型 <ul><li>文本大模型的上限决定了其他大模型的上限</li><li>文本大模型会促进其它模态的发展</li><li>其他模态会之后文本大模型的发展</li></ul></li></ul><p>24年机会</p><ul><li>Agent</li><li>Small Model/模型量化/小模型fine-tune (在智能设备上嵌入的模型，0.5B,1B) <ul><li>智能硬件，如智能手表</li><li>怎么把模型用在cpu上</li></ul></li><li>多模态</li><li>推理加速，减少推理成本</li></ul><h2 id="_10-reference" tabindex="-1"><a class="header-anchor" href="#_10-reference" aria-hidden="true">#</a> 10. Reference</h2>',27),D={href:"https://jalammar.github.io/illustrated-stable-diffusion/?spm=a2c6h.12873639.article-detail.7.412f5124rpbR0C",target:"_blank",rel:"noopener noreferrer"},V=l("ul",null,[l("li",null,[i("中文翻译 "),l("ul",null,[l("li",null,"https://blog.51cto.com/u_16099326/11808915")])])],-1);function E(k,R){const s=A("ExternalLinkIcon");return I(),L("div",null,[G,P,T(" more "),C,N,w,l("ul",null,[l("li",null,[l("a",D,[i("The Illustrated Stable Diffusion"),S(s)]),V])])])}const O=M(z,[["render",E],["__file","016_multimodal.html.vue"]]);export{O as default};
