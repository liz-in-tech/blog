import{_ as a,a as r,b as n,c as o,d as l,e as s,f as h,g as c,h as d,i as u,j as p,k as g,l as m,m as f}from"./010_hierarchical_index_retrieval-bg_di8nz.js";import{_ as b}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as y,o as v,c as x,f as w,a as e,b as i,d as _,e as R}from"./app-PDpchPSG.js";const k={},C=e("h1",{id:"rag-workflow",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#rag-workflow","aria-hidden":"true"},"#"),i(" RAG Workflow")],-1),z=e("ul",null,[e("li",null,"Raw Data Processing Flow"),e("li",null,"RAG Process in Q&A Scenarios"),e("li",null,"RAG Optimization Points"),e("li",null,"RAG Optimization: Query / Retriever / Ranking / Indexing Optimization")],-1),L=R('<h2 id="_1-raw-data-processing-flow" tabindex="-1"><a class="header-anchor" href="#_1-raw-data-processing-flow" aria-hidden="true">#</a> 1. Raw Data Processing Flow</h2><figure><img src="'+a+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li><ol><li>Raw Data</li></ol></li><li><ol start="2"><li>Data Loader</li></ol></li><li><ol start="3"><li>Data Transformer / Data Cleaning / Data Parsing</li></ol></li><li><ol start="4"><li>Data Split</li></ol><ul><li>Split into multiple Chunks</li><li>Balance in chunk size</li></ul></li><li><ol start="5"><li>Data Vectorization</li></ol><ul><li>Represent each Chunk as a Vector</li><li>Text Embedding Model</li></ul></li><li><ol start="6"><li>Data Vector Store</li></ol><ul><li>vector database</li><li>indexing</li></ul></li><li><ol start="7"><li>Data Retriever</li></ol></li></ul><h2 id="_2-rag-process-in-q-a-scenarios" tabindex="-1"><a class="header-anchor" href="#_2-rag-process-in-q-a-scenarios" aria-hidden="true">#</a> 2. RAG Process in Q&amp;A Scenarios</h2><figure><img src="'+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li><ol><li>User initiates a question Query</li></ol></li><li><ol start="2"><li>The user’s question Query is vectorized using an Embedding Model</li></ol></li><li><ol start="3"><li>Obtain the vectorized user question as a Query Vector</li></ol></li><li><ol start="4"><li>Retrieve from the Vector Database using the Query Vector</li></ol><ul><li>Use of vector databases as external knowledge bases</li></ul></li><li><ol start="5"><li>Retrieve the Top K vectors based on semantic similarity</li></ol></li><li><ol start="6"><li>Use the semantic content of the Top K vectors along with the user Query as a Prompt to the LLM</li></ol></li><li><ol start="7"><li>LLM generates an answer based on the user&#39;s query and the retrieved data</li></ol></li><li><ol start="8"><li>User receives the answer</li></ol></li></ul><figure><img src="'+n+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>The RAG construction and Q&amp;A steps are generally summarised in three points</p><ul><li>Indexing</li><li>Retrieval</li><li>Generation</li></ul><h2 id="_3-rag-optimization-points" tabindex="-1"><a class="header-anchor" href="#_3-rag-optimization-points" aria-hidden="true">#</a> 3. RAG Optimization Points</h2><h3 id="_3-1-optimization-points-for-native-data-processing-flow" tabindex="-1"><a class="header-anchor" href="#_3-1-optimization-points-for-native-data-processing-flow" aria-hidden="true">#</a> 3.1. Optimization Points for Native Data Processing Flow</h3><ul><li>For step1: Native Data <ul><li>Knowledge Graph <ul><li>Increasingly, Knowledge Graphs are introduced into RAG (Retrieval-Augmented Generation) tasks, such as: <ul><li>KnowledGPT for the inference stage</li><li>SUGRE for the fine-tuning stage</li></ul></li></ul></li></ul></li><li>For step3: Document Parsing <ul><li>Challenges: Different document types (e.g., pdf, html, ppt) require different parsing methods; documents are often unstructured and contain tables, formulas, images, etc., which are difficult to parse; multimodal data such as text, images, audio, and video.</li><li>Tables <ul><li>Challenges: <ul><li>Text segmentation can separate tables, leading to data corruption</li><li>Using tables as retrieval data complicates the semantic similarity search process</li></ul></li><li>Solutions: <ul><li>Execute Text-2-SQL queries through code</li><li>Convert tables into descriptive text</li></ul></li></ul></li><li>Consider the accuracy of parsing</li></ul></li><li>For step4: Text Segmentation <ul><li>Challenges: Different segmentation methods impact subsequent tasks</li><li>Consider choices and parameters for chunking, as well as the cost of trying different algorithms</li><li>Chunk Size: If the segmentation granularity is too large, each chunk contains more information, providing more context in a single return; if too small, retrieval can be more accurate for the most relevant chunks</li><li>Research on chunk size can be found in the NodeParser class of LlamaIndex, which provides some advanced options, such as defining custom text segmenters, metadata, node/chunk relationships, etc.</li></ul></li><li>For step6: Vector Indexing <ul><li>Choice of different vector databases</li><li>Choice of different indexing methods</li><li>Optimization of the index <ul><li>Add metadata (e.g., time, type, title, subtitle, part of document)</li></ul></li></ul></li></ul><h3 id="_3-2-optimization-points-in-rag-for-q-a-scenarios" tabindex="-1"><a class="header-anchor" href="#_3-2-optimization-points-in-rag-for-q-a-scenarios" aria-hidden="true">#</a> 3.2. Optimization Points in RAG for Q&amp;A Scenarios</h3><ul><li>For step2: Query Vectorization <ul><li>During query vectorization, decide whether query processing (query cleaning) is needed → Refined Query</li><li>User queries may be incomplete, inaccurate, complex, or not a coherent sentence</li></ul></li><li>For step5: Retrieve Top K <ul><li>Evaluate whether the retrieved items are sufficient to answer the question or if many are redundant</li><li>Separate Recall and Ranking processes</li><li>Recall (e.g., retrieve 50 relevant items from the vector database) =&gt; Ranking (e.g., rank the 50 items and select the top 3)</li><li>Determine whether sorting is necessary and how to sort</li></ul></li><li>For step7: Obtaining the LLM Response <ul><li>Post-processing</li><li>In LlamaIndex, a variety of Postprocessors are available, which can filter results based on similarity score, keywords, metadata, or rerank them with other models like an LLM, a sentence-transformer cross-encoder, or Cohere’s reranking endpoint, or based on metadata like date recency — essentially, all types of processing one could imagine.</li></ul></li></ul><h2 id="_4-prompts-in-rag-scenarios" tabindex="-1"><a class="header-anchor" href="#_4-prompts-in-rag-scenarios" aria-hidden="true">#</a> 4. Prompts in RAG Scenarios</h2><ul><li>Instruction</li><li>Context =&gt; Retrieval</li><li>Input =&gt; Query</li><li>History</li></ul><h2 id="_5-text-segmentation-methods" tabindex="-1"><a class="header-anchor" href="#_5-text-segmentation-methods" aria-hidden="true">#</a> 5. Text Segmentation Methods</h2><ul><li>Purpose <ul><li>Enable retrieval of content more relevant to the Query</li></ul></li><li>Methods <ul><li>Split by sentences</li><li>Fixed window character count</li><li>Moving window character count</li><li>Recursive method: RecursiveCharacterTextSplitter (fixed window + semantic segmentation; used by LangChain; commonly applied)</li><li>Split by semantics (more complex to implement; may result in chunks that are too long or too short)</li></ul></li></ul><h2 id="_6-rag-optimization-——-query-optimization" tabindex="-1"><a class="header-anchor" href="#_6-rag-optimization-——-query-optimization" aria-hidden="true">#</a> 6. RAG Optimization —— Query Optimization</h2><ul><li>Rewrite</li><li>Expand</li></ul><h3 id="_6-1-case1-self-querying-retrieval" tabindex="-1"><a class="header-anchor" href="#_6-1-case1-self-querying-retrieval" aria-hidden="true">#</a> 6.1. Case1: Self-Querying Retrieval</h3><ul><li>Transform the Query through information extraction: Self-Querying Retrieval <ul><li>Convert natural language Query into a structured Query through an LLM</li><li>Store metadata alongside embeddings in the vector database <ul><li>Attributes: metadata (can filter by attributes first)</li><li>Non-attributes: embedding content information (semantic search follows)</li></ul></li></ul></li></ul><figure><img src="'+o+'" alt="Self-Querying Retrieval" tabindex="0" loading="lazy"><figcaption>Self-Querying Retrieval</figcaption></figure><blockquote><p>In the illustration, only the product name undergoes semantic embedding; other information is stored as metadata attributes.</p></blockquote><p>Approach:</p><ul><li><ol><li>Transform the Query through an LLM into a structured Query, separating metadata from the embedding content</li></ol></li><li><ol start="2"><li>Filter in the Vector Database using metadata</li></ol></li><li><ol start="3"><li>Perform a vector search in the Vector Database using the embedding portion</li></ol></li></ul><p>LangChain&#39;s Self-Querying Retrieval</p><p>https://python.langchain.com/docs/how_to/self_query/</p><figure><img src="'+l+'" alt="Self-Querying Retrieval in LangChain" tabindex="0" loading="lazy"><figcaption>Self-Querying Retrieval in LangChain</figcaption></figure><h3 id="_6-2-case2-multiqueryretriever" tabindex="-1"><a class="header-anchor" href="#_6-2-case2-multiqueryretriever" aria-hidden="true">#</a> 6.2. Case2: MultiQueryRetriever</h3><p>Query Expansion with multiple related questions</p><p>Scenario:</p><ul><li>Comparison of multiple products: comparing Xiaomi, Huawei, and Samsung foldable phones from various dimensions, summarized in a table</li><li>Incomplete question: rewrite it into several sub-questions from different perspectives, consult the LLM, then summarize</li></ul><p>Approach:</p><ul><li><ol><li>Break down the Query into multiple SubQueries through the LLM</li></ol></li><li><ol start="2"><li>Retrieve outputs for each SubQuery in parallel using the LLM</li></ol></li><li><ol start="3"><li>Aggregate the outputs into a final result using another LLM</li></ol></li></ul><p>Implementation:</p><ul><li>LangChain&#39;s MultiQueryRetriever <ul><li>https://python.langchain.com/docs/how_to/MultiQueryRetriever/</li></ul></li><li>LlamaIndex&#39;s SubQuestionQueryEngine</li></ul><figure><img src="'+s+'" alt="MultiQueryRetriever" tabindex="0" loading="lazy"><figcaption>MultiQueryRetriever</figcaption></figure><h3 id="_6-3-case3-step-back-prompting" tabindex="-1"><a class="header-anchor" href="#_6-3-case3-step-back-prompting" aria-hidden="true">#</a> 6.3. Case3: Step-Back Prompting</h3><p>Approach:</p><ul><li><ol><li>Use the LLM to abstract the Query into a “step-back question”</li></ol></li><li><ol start="2"><li>Generate a response to the Stepback Question using the LLM</li></ol></li><li><ol start="3"><li>Combine the original Query with the Stepback Answer to obtain the final result via the LLM</li></ol></li></ul><p>Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models</p><p>https://arxiv.org/abs/2310.06117</p><figure><img src="'+h+'" alt="Step-Back Prompting" tabindex="0" loading="lazy"><figcaption>Step-Back Prompting</figcaption></figure><h3 id="_6-4-case4-combining-query-and-history" tabindex="-1"><a class="header-anchor" href="#_6-4-case4-combining-query-and-history" aria-hidden="true">#</a> 6.4. Case4: Combining Query and History</h3><p>Two combination methods:</p><ul><li>Retrieve from the Vector Database with the Query first, then combine with History (ContextChatEngine)</li><li>Combine Query with History first, then retrieve from the Vector Database (CondensePlusContextMode)</li></ul><figure><img src="'+c+'" alt="Combining Query and History" tabindex="0" loading="lazy"><figcaption>Combining Query and History</figcaption></figure><h3 id="_6-5-case5-hypothetical-answer-hypothetical-document" tabindex="-1"><a class="header-anchor" href="#_6-5-case5-hypothetical-answer-hypothetical-document" aria-hidden="true">#</a> 6.5. Case5: Hypothetical Answer / Hypothetical Document</h3><p>Embeddings (HyDE) Use a generated answer to expand the Query</p><p>Approach:</p><ul><li><ol><li>Generate an Answer to the Query using the LLM</li></ol></li><li><ol start="2"><li>Combine the Query and Answer to retrieve from the Vector Database</li></ol></li></ul><figure><img src="'+d+'" alt="Hypothetical Answer" tabindex="0" loading="lazy"><figcaption>Hypothetical Answer</figcaption></figure><p>Paper: https://boston.lti.cs.cmu.edu/luyug/HyDE/HyDE.pdf</p><p>The logic is the reverse of the hypothetical question approach in Indexing optimization.</p><h3 id="_6-6-case6-query-rewriting" tabindex="-1"><a class="header-anchor" href="#_6-6-case6-query-rewriting" aria-hidden="true">#</a> 6.6. Case6: Query Rewriting</h3><p>Use large language models to reconstruct the initial Query to improve retrieval effectiveness. LangChain and LlamaIndex both have implementations; however, LlamaIndex&#39;s solution is generally more robust in this regard.</p><h3 id="_6-7-case7-query-routing" tabindex="-1"><a class="header-anchor" href="#_6-7-case7-query-routing" aria-hidden="true">#</a> 6.7. Case7: Query Routing</h3><p>Query routing is a decision step based on LLMs.</p><p>Query routers are also used to select the appropriate index or a broader data storage location for handling user queries.</p><ul><li>Implementation: <ul><li>llamaindex <ul><li>https://docs.llamaindex.ai/en/stable/module_guides/querying/router/</li></ul></li><li>langchain <ul><li>https://python.langchain.com/docs/how_to/routing/</li></ul></li></ul></li></ul><h2 id="_7-rag-optimization-——-retriever-optimization" tabindex="-1"><a class="header-anchor" href="#_7-rag-optimization-——-retriever-optimization" aria-hidden="true">#</a> 7. RAG Optimization —— Retriever Optimization</h2><h3 id="_7-1-case1-sentence-window-retrieval" tabindex="-1"><a class="header-anchor" href="#_7-1-case1-sentence-window-retrieval" aria-hidden="true">#</a> 7.1. Case1: Sentence Window Retrieval</h3><p>Concept: The idea is to retrieve smaller chunks to improve search quality while including surrounding context for LLM reasoning.<br> Goal: Obtain as much context as possible, while fine-grained chunks increase accuracy.<br> Idea: Neighboring chunks of the retrieved chunk are likely also relevant to the Query, so return these chunks along with the primary result.<br> Drawback: The context size can become too large.</p><p>Approach:</p><ul><li><ol><li>Treat each sentence in the document as a Chunk to improve search accuracy</li></ol></li><li><ol start="2"><li>Expand the retrieved key sentence with k sentences before and after and return the result</li></ol></li></ul><figure><img src="'+u+'" alt="Sentence Window Retrieval" tabindex="0" loading="lazy"><figcaption>Sentence Window Retrieval</figcaption></figure><h3 id="_7-2-case2-parent-child-chunks-retrieval-from-small-to-big" tabindex="-1"><a class="header-anchor" href="#_7-2-case2-parent-child-chunks-retrieval-from-small-to-big" aria-hidden="true">#</a> 7.2. Case2: Parent-Child Chunks Retrieval / from Small to Big</h3><p>Concept: The idea is to retrieve smaller chunks to improve search quality while including surrounding context for LLM reasoning. Idea: Similar to Sentence Window Retrieval — it aims to search for finer pieces of information and then expands the context window before passing it to the LLM for reasoning.</p><p>Approach:</p><ul><li><ol><li>Divide the document into Parent Chunks with coarse granularity; each Parent Chunk is further subdivided into Child Chunks, stored in the Vector Database</li></ol></li><li><ol start="2"><li>Retrieve relevant Child Chunks from the Vector Database with the Query</li></ol></li><li><ol start="3"><li>Return the Parent Chunks (de-duplicated results) containing the retrieved Child Chunks</li></ol></li></ul><figure><img src="'+p+'" alt="Parent-Child Retrieval" tabindex="0" loading="lazy"><figcaption>Parent-Child Retrieval</figcaption></figure><p>For a more in-depth understanding, check out LlamaIndex’s tutorial on recursive retrieval and node referencing.</p><h3 id="_7-3-case3-fusion-retrieval" tabindex="-1"><a class="header-anchor" href="#_7-3-case3-fusion-retrieval" aria-hidden="true">#</a> 7.3. Case3: Fusion Retrieval</h3><p>Combines keyword search and semantic search, integrating dense and sparse search.</p><figure><img src="'+g+'" alt="Fusion Retrieval" tabindex="0" loading="lazy"><figcaption>Fusion Retrieval</figcaption></figure><h2 id="_8-rag-optimization-——-ranking-optimization" tabindex="-1"><a class="header-anchor" href="#_8-rag-optimization-——-ranking-optimization" aria-hidden="true">#</a> 8. RAG Optimization —— Ranking Optimization</h2><ul><li>Coarse Ranking and Fine Ranking <ul><li>Coarse Ranking: Indexing within the Vector Database</li><li>Fine Ranking: Sorting results after retrieval from the Vector Database</li></ul></li></ul><h3 id="_8-1-reciprocal-rank-fusion-rrf-algorithm" tabindex="-1"><a class="header-anchor" href="#_8-1-reciprocal-rank-fusion-rrf-algorithm" aria-hidden="true">#</a> 8.1. Reciprocal Rank Fusion (RRF) Algorithm</h3><figure><img src="'+m+'" alt="Reciprocal Rank Fusion" tabindex="0" loading="lazy"><figcaption>Reciprocal Rank Fusion</figcaption></figure><p>The key here is to correctly combine retrieval results with different similarity scores. This problem is often addressed by the Reciprocal Rank Fusion algorithm, which re-ranks the retrieval results to produce a final output.</p><p>In LangChain, this feature is implemented through the Ensemble Retriever class, which combines a series of retrievers you define, such as a faiss vector index and a BM25-based retriever, and uses the Reciprocal Rank Fusion algorithm (RRF) for result re-ranking. In LlamaIndex, the implementation is very similar.</p><h2 id="_9-rag-optimization-——-indexing-optimization" tabindex="-1"><a class="header-anchor" href="#_9-rag-optimization-——-indexing-optimization" aria-hidden="true">#</a> 9. RAG Optimization —— Indexing Optimization</h2><h3 id="_9-1-hierarchical-indexing" tabindex="-1"><a class="header-anchor" href="#_9-1-hierarchical-indexing" aria-hidden="true">#</a> 9.1. Hierarchical Indexing</h3><p>Create two indexes — one composed of summaries and another of document chunks — then perform a two-step search: filter relevant documents via summaries first, then search within this filtered set.</p><figure><img src="'+f+'" alt="Hierarchical Indexing" tabindex="0" loading="lazy"><figcaption>Hierarchical Indexing</figcaption></figure><h3 id="_9-2-hypothetical-question" tabindex="-1"><a class="header-anchor" href="#_9-2-hypothetical-question" aria-hidden="true">#</a> 9.2. Hypothetical Question</h3><p>Let the LLM generate a hypothetical question for each chunk, then embed these questions as vectors, replacing chunk vectors with question vectors.</p><p>Due to the higher semantic similarity between the Query and hypothetical questions, search quality is improved.</p><p>This approach is the reverse logic of the Hypothetical Answer method in Query optimization.</p><h2 id="_10-rag-optimization-——-other-optimizations" tabindex="-1"><a class="header-anchor" href="#_10-rag-optimization-——-other-optimizations" aria-hidden="true">#</a> 10. RAG Optimization —— Other Optimizations</h2><h3 id="reference-citations" tabindex="-1"><a class="header-anchor" href="#reference-citations" aria-hidden="true">#</a> Reference Citations</h3><p>Determine whether it is possible to accurately trace back to the source of the answer.</p><h2 id="_11-graphrag" tabindex="-1"><a class="header-anchor" href="#_11-graphrag" aria-hidden="true">#</a> 11. GraphRAG</h2><p>Leveraging Knowledge Graphs to Capture Complex Relationships Between Entities, Especially Suitable for Handling Data Rich in Relations</p><p>Scenario: Suppose we are developing an intelligent question-answering system to answer questions about historical figures and their interrelationships.</p><p>Question: The user asks, &quot;Who were Alexander the Great&#39;s generals, and what battles did they participate in together?&quot;</p><p>Traditional RAG Challenges:</p><ul><li>Traditional RAG might retrieve documents about Alexander the Great and his generals, but these documents might contain only partial information or be scattered across different documents.</li><li>It may struggle to extract and integrate all relevant information from the retrieved texts, especially when this information spans multiple documents and contexts.</li></ul><p>GraphRAG Solution:</p><ul><li><ol><li><strong>Constructing the Knowledge Graph</strong>: GraphRAG first extracts entities (such as Alexander the Great and his generals) and relationships (such as command relationships and participation in battles) from historical literature and materials to build a knowledge graph.</li></ol></li><li><ol start="2"><li><strong>Graph Querying</strong>: The user&#39;s query is converted into a graph query, where the system searches the knowledge graph for nodes related to Alexander the Great&#39;s generals and the battle nodes connected to them.</li></ol></li><li><ol start="3"><li><strong>Extracting Subgraphs</strong>: The system retrieves not only individual nodes but also the relationships between those nodes, forming a subgraph that includes all relevant generals and battle information.</li></ol></li><li><ol start="4"><li><strong>Generating the Answer</strong>: GraphRAG uses this subgraph as context to guide the language model in generating a detailed answer that includes not only the names of the generals but also the battles they participated in together.</li></ol></li></ul><p>Pros and Cons</p><ul><li>Pros: Able to capture and represent complex relationships between entities, integrating and retrieving information spread across different documents.</li><li>Cons: High cost of building and maintaining the knowledge graph, as well as the technical challenges of managing complex graph structures. It also faces difficulties when handling abstract concepts.</li></ul><h2 id="_12-hybridrag" tabindex="-1"><a class="header-anchor" href="#_12-hybridrag" aria-hidden="true">#</a> 12. HybridRAG</h2><p>Paper Name: HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction</p><p>Paper: https://arxiv.org/pdf/2408.04948v1</p><ul><li><ol><li><strong>Information Retrieval</strong>: Information related to the query is retrieved from external documents using vector databases for broad, similarity-based retrieval (VectorRAG part), and at the same time, structured and relationship-rich contextual data is retrieved from the knowledge graph (GraphRAG part).</li></ol></li><li><ol start="2"><li><strong>Context Integration</strong>: The information retrieval results from VectorRAG are integrated with the precise relationship data from GraphRAG to form a unified context.</li></ol></li><li><ol start="3"><li><strong>Generating the Answer</strong>: The large language model (LLM) generates the final answer based on the integrated context.</li></ol></li></ul><h2 id="_13-references" tabindex="-1"><a class="header-anchor" href="#_13-references" aria-hidden="true">#</a> 13. References</h2>',108),Q={href:"https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6",target:"_blank",rel:"noopener noreferrer"};function A(q,G){const t=y("ExternalLinkIcon");return v(),x("div",null,[C,z,w(" more "),L,e("ul",null,[e("li",null,[e("a",Q,[i("Advanced RAG Techniques: an Illustrated Overview"),_(t)])])])])}const T=b(k,[["render",A],["__file","010_rag_workflow.html.vue"]]);export{T as default};
