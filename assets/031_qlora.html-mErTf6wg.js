import{_ as a}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as s,c as e,f as t,a as n,b as i,e as l}from"./app-UEw0SYZq.js";const o={},p=n("h1",{id:"qlora-代码实现及过程分析",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#qlora-代码实现及过程分析","aria-hidden":"true"},"#"),i(" QLoRA 代码实现及过程分析")],-1),u=n("ul",null,[n("li",null,"背景介绍: QLoRA/基础模型/数据集"),n("li",null,"QLoRA 代码实现"),n("li",null,"QLoRA 过程分析"),n("li",null,"QLoRA 应用价值"),n("li",null,"QLoRA 疑点思考"),n("li",null,"QLoRA 细节补充")],-1),c=l(`<h2 id="_1-背景介绍-qlora-基础模型-数据集" tabindex="-1"><a class="header-anchor" href="#_1-背景介绍-qlora-基础模型-数据集" aria-hidden="true">#</a> 1. 背景介绍: QLoRA/基础模型/数据集</h2><ul><li>QLoRA <ul><li>QLoRA（Quantized Low-Rank Adaptation）微调方法</li><li>Paper: https://arxiv.org/abs/2305.14314</li><li>QLoRA 结合了 4-bit 量化和 LoRA 技术，具体实现步骤如下： <ul><li>4-bit 量化：使用 bitsandbytes 库实现 4-bit NormalFloat (NF4) 量化，将预训练模型权重压缩至 4 位，显著降低内存占用</li><li>LoRA：通过 peft 库实现 LoRA，添加低秩适配器（例如秩 r=16），仅更新少量参数</li><li>结合技术：加载量化后的模型，附加 LoRA 适配器，使用 16-bit (bfloat16) 进行前向/反向传播计算</li></ul></li></ul></li><li>基础模型：DistilGPT-2 <ul><li>https://huggingface.co/distilbert/distilgpt2</li></ul></li><li>Alpaca指令数据集 <ul><li>官方版本 <ul><li>https://huggingface.co/datasets/tatsu-lab/alpaca</li><li>使用 OpenAI 的 text-davinci-003 模型生成的输出，该数据集可能包含错误或偏见，建议用户谨慎使用并考虑过滤方法，这在数据集描述中有所提及</li></ul></li><li>yahma/alpaca-cleaned <ul><li>https://huggingface.co/datasets/yahma/alpaca-cleaned</li><li>清理版本，修复了原始数据集中的幻觉、错误答案等问题，提供了高质量的数据</li></ul></li><li>vicgalle/alpaca-gpt4 <ul><li>https://huggingface.co/datasets/vicgalle/alpaca-gpt4</li><li>基于 Alpaca 提示，使用 GPT-4 生成的输出，，但未提及清理，可能包含未修正的错误</li></ul></li></ul></li></ul><h2 id="_2-qlora-代码实现" tabindex="-1"><a class="header-anchor" href="#_2-qlora-代码实现" aria-hidden="true">#</a> 2. QLoRA 代码实现</h2><ul><li>Load Model</li><li>Preparing Dataset</li><li>Fine-Tuning</li><li>Save Trained Model</li></ul><h3 id="_2-1-load-model" tabindex="-1"><a class="header-anchor" href="#_2-1-load-model" aria-hidden="true">#</a> 2.1. Load Model</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 量化配置</span>
bnb_config <span class="token operator">=</span> BitsAndBytesConfig<span class="token punctuation">(</span>
    load_in_4bit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment"># 启用 4bit 量化，将模型的线性层（Linear / Conv1D）替换成量化层 Linear4bit</span>
    bnb_4bit_use_double_quant<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment"># 启用嵌套量化，进一步压缩量化参数，减少存储开销 (Linear4bit内部计算逻辑)</span>
    bnb_4bit_quant_type<span class="token operator">=</span><span class="token string">&quot;nf4&quot;</span><span class="token punctuation">,</span> <span class="token comment"># 4bit 量化格式有2种（nf4和fp4），其中nf4基于正态分布优化，通常效果更优</span>
    bnb_4bit_compute_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16 <span class="token comment"># 设置计算时的数据类型，实际权重以 4bit 存储但会映射到 bfloat16 进行计算，也就是 Linear4bit 内部的中间计算使用 bfloat16</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 选择 distilbert/distilgpt2 作为基础模型</span>
model_id <span class="token operator">=</span> <span class="token string">&quot;distilbert/distilgpt2&quot;</span>

<span class="token comment"># 将整个模型加载到 GPU 0</span>
device_map <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&quot;&quot;</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">}</span>

<span class="token comment"># 加载原始模型</span>
original_model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">)</span>

<span class="token comment"># 加载量化模型（将量化配置应用在模型上）</span>
quantized_model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">,</span>
                    quantization_config<span class="token operator">=</span>bnb_config<span class="token punctuation">,</span>
                    device_map<span class="token operator">=</span>device_map<span class="token punctuation">,</span>
                    use_cache <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 加载与模型对应的分词器，并设置填充标记为结束标记</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">)</span>
tokenizer<span class="token punctuation">.</span>pad_token <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>eos_token
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-2-preparing-dataset" tabindex="-1"><a class="header-anchor" href="#_2-2-preparing-dataset" aria-hidden="true">#</a> 2.2. Preparing Dataset</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 选择 yahma/alpaca-cleaned 作为数据集</span>
dataset_name <span class="token operator">=</span> <span class="token string">&quot;yahma/alpaca-cleaned&quot;</span>

<span class="token comment"># 加载数据集</span>
full_dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span>dataset_name<span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">)</span>

<span class="token comment"># 选取小规模子集（1000 条）</span>
small_subset <span class="token operator">=</span> full_dataset<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>seed<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span><span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 定义 Alpaca 数据集的 Prompt 模版</span>
alpaca_prompt <span class="token operator">=</span> <span class="token triple-quoted-string string">&quot;&quot;&quot;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}&quot;&quot;&quot;</span>

<span class="token comment"># 定义 formatting_prompts_func 函数</span>
<span class="token keyword">def</span> <span class="token function">formatting_prompts_func</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">:</span>

    instructions <span class="token operator">=</span> examples<span class="token punctuation">[</span><span class="token string">&quot;instruction&quot;</span><span class="token punctuation">]</span>
    inputs       <span class="token operator">=</span> examples<span class="token punctuation">[</span><span class="token string">&quot;input&quot;</span><span class="token punctuation">]</span>
    outputs      <span class="token operator">=</span> examples<span class="token punctuation">[</span><span class="token string">&quot;output&quot;</span><span class="token punctuation">]</span>

    texts <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> instruction<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> output <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>instructions<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        text <span class="token operator">=</span> alpaca_prompt<span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>instruction<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> output<span class="token punctuation">)</span>
        texts<span class="token punctuation">.</span>append<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">{</span> <span class="token string">&quot;text&quot;</span> <span class="token punctuation">:</span> texts <span class="token punctuation">}</span>


<span class="token comment"># 应用 formatting_prompts_func 函数</span>
small_subset <span class="token operator">=</span> small_subset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>formatting_prompts_func<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 对 &quot;text&quot; 应用 tokenizer；如果超长，截断到模型最大长度；所有样本 pad 到相同长度，方便 batch 训练</span>
small_subset <span class="token operator">=</span> small_subset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> samples<span class="token punctuation">:</span> tokenizer<span class="token punctuation">(</span>samples<span class="token punctuation">[</span><span class="token string">&quot;text&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&quot;max_length&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-3-fine-tuning" tabindex="-1"><a class="header-anchor" href="#_2-3-fine-tuning" aria-hidden="true">#</a> 2.3. Fine-Tuning</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># LoRA 参数配置</span>
peft_config <span class="token operator">=</span> LoraConfig<span class="token punctuation">(</span>
    r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token comment"># 秩，越大表达能力越强，但参数也更多</span>
    lora_alpha<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token comment"># 缩放因子</span>
    lora_dropout<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token comment"># dropout 概率</span>
    target_modules<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;c_attn&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;c_proj&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;c_fc&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment"># 需要插入 LoRA 的模块</span>
    bias<span class="token operator">=</span><span class="token string">&quot;none&quot;</span><span class="token punctuation">,</span> <span class="token comment"># 是否训练 bias 项：否</span>
    task_type<span class="token operator">=</span><span class="token string">&quot;CAUSAL_LM&quot;</span><span class="token punctuation">,</span> <span class="token comment"># 任务类型：因果语言建模</span>
<span class="token punctuation">)</span>

<span class="token comment"># 训练参数配置</span>
training_args <span class="token operator">=</span> SFTConfig<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">&quot;outputs&quot;</span><span class="token punctuation">,</span> <span class="token comment"># 输出路径</span>
    logging_steps<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment"># 多少steps记录一次日志</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token comment"># 训练轮数</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token comment"># 每个设备的训练批次大小</span>
    per_device_eval_batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token comment"># 每个设备的验证批次大小</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token comment"># 梯度累积</span>
    gradient_checkpointing<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token comment"># 启用梯度检查点</span>
    learning_rate<span class="token operator">=</span><span class="token number">2e-4</span><span class="token punctuation">,</span> <span class="token comment"># 学习率</span>
    optim<span class="token operator">=</span><span class="token string">&quot;adamw_8bit&quot;</span><span class="token punctuation">,</span> <span class="token comment"># 优化器</span>
    weight_decay<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token comment"># 权重衰减</span>
    max_grad_norm<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token comment"># 梯度裁剪</span>
    warmup_ratio<span class="token operator">=</span><span class="token number">0.03</span><span class="token punctuation">,</span> <span class="token comment"># 预热比例</span>
    fp16<span class="token operator">=</span><span class="token keyword">not</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_bf16_supported<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># 使用半精度训练</span>
    bf16<span class="token operator">=</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_bf16_supported<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    dataset_text_field<span class="token operator">=</span><span class="token string">&quot;text&quot;</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># 实例化 SFTTrainer</span>
trainer <span class="token operator">=</span> SFTTrainer<span class="token punctuation">(</span>
    model<span class="token operator">=</span>quantized_model<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>small_subset<span class="token punctuation">,</span>
    peft_config<span class="token operator">=</span>peft_config<span class="token punctuation">,</span>
    args<span class="token operator">=</span>training_args<span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># 输出可训练参数量</span>
trainer<span class="token punctuation">.</span>model<span class="token punctuation">.</span>print_trainable_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 开始训练</span>
trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-4-save-trained-model" tabindex="-1"><a class="header-anchor" href="#_2-4-save-trained-model" aria-hidden="true">#</a> 2.4. Save Trained Model</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># Save trained model</span>
peft_model <span class="token operator">=</span> <span class="token string">&quot;distilgpt2-qlora&quot;</span>

trainer<span class="token punctuation">.</span>model<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span>peft_model<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>base_model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_id<span class="token punctuation">,</span>
    low_cpu_mem_usage<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    return_dict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span>
    device_map<span class="token operator">=</span>device_map<span class="token punctuation">,</span>
<span class="token punctuation">)</span>

peft_model <span class="token operator">=</span> PeftModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>base_model<span class="token punctuation">,</span> peft_model<span class="token punctuation">)</span>
merged_model <span class="token operator">=</span> peft_model<span class="token punctuation">.</span>merge_and_unload<span class="token punctuation">(</span><span class="token punctuation">)</span>
merged_model<span class="token punctuation">.</span>save_pretrained<span class="token punctuation">(</span><span class="token string">&quot;merged_model&quot;</span><span class="token punctuation">,</span> safe_serialization<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> max_shard_size<span class="token operator">=</span><span class="token string">&quot;2GB&quot;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_3-qlora-过程分析" tabindex="-1"><a class="header-anchor" href="#_3-qlora-过程分析" aria-hidden="true">#</a> 3. QLoRA 过程分析</h2><ul><li>原始模型和量化模型的对比</li><li>Dataset 处理流程</li><li>可训练参数量计算</li></ul><h3 id="_3-1-原始模型和量化模型的对比" tabindex="-1"><a class="header-anchor" href="#_3-1-原始模型和量化模型的对比" aria-hidden="true">#</a> 3.1. 原始模型和量化模型的对比</h3><ul><li>参数数量不会变（还是那么多矩阵元素） <ul><li>81912576</li></ul></li><li>参数精度和大小变了（用 4-bit 表示） <ul><li>参数大小变化 <ul><li>Original size: 318.47 MB <ul><li>估算：81912576 * 4 bytes / (1024^2) = 308.66 MB</li></ul></li><li>Quantized size: 101.49 MB <ul><li>估算 <ul><li>0.5 bytes 的参数个数：42467328 = 6 layers * (768 * 2304 + 768 * 768 + 2 * 768 * 3072)</li><li>2 bytes 的参数个数：39445248 = 81912576 - 42467328</li><li>（0.5 * 42467328 + 2 * 39445248 ） / (1024^2) = 95.49 MB</li></ul></li></ul></li></ul></li><li>模型结构变化 <ul><li>attn <ul><li>(c_attn): Conv1D(nf=2304, nx=768) -&gt; (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)</li><li>(c_proj): Conv1D(nf=768, nx=768) -&gt; (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)</li></ul></li><li>mlp <ul><li>(c_fc): Conv1D(nf=3072, nx=768) -&gt; (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)</li><li>(c_proj): Conv1D(nf=768, nx=3072) -&gt; (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)</li></ul></li></ul></li><li>参数精度变化 <ul><li>量化前： <ul><li>所有参数全都是dtype=torch.float32（32位） <ul><li>transformer.h.0.attn.c_attn.weight: torch.Size([768, 2304]), dtype=torch.float32</li><li>transformer.h.0.attn.c_proj.weight: torch.Size([768, 768]), dtype=torch.float32</li><li>transformer.h.0.mlp.c_fc.weight: torch.Size([768, 3072]), dtype=torch.float32</li><li>transformer.h.0.mlp.c_proj.weight: torch.Size([3072, 768]), dtype=torch.float32</li></ul></li></ul></li><li>量化后： <ul><li>dtype=torch.uint8（每层的4个地方变为4bit）实际存储中使用压缩技术，将2个4bit组合为int8 <ul><li>transformer.h.0.attn.c_attn.weight: torch.Size([884736, 1]), dtype=torch.uint8</li><li>transformer.h.0.attn.c_proj.weight: torch.Size([294912, 1]), dtype=torch.uint8</li><li>transformer.h.0.mlp.c_fc.weight: torch.Size([1179648, 1]), dtype=torch.uint8</li><li>transformer.h.0.mlp.c_proj.weight: torch.Size([1179648, 1]), dtype=torch.uint8</li></ul></li><li>dtype=torch.float16（其余参数都变为float16）</li></ul></li></ul></li></ul></li></ul><p>变化解释</p><ul><li>变化 <ul><li>量化前：transformer.h.0.attn.c_attn.weight: torch.Size([768, 2304]), dtype=torch.float32</li><li>量化后：transformer.h.0.attn.c_attn.weight: torch.Size([884736, 1]), dtype=torch.uint8</li></ul></li><li>解释 <ul><li>原始 float32 的矩阵 [768, 2304] → 总共参数数量是 768 * 2304 = 1,769,472</li><li>用 4-bit 表示就是 1,769,472 * 0.5 byte = 884,736 bytes = 884736 uint8 （存储为 packed 的 uint8，每 byte 存两个 4-bit 权重）</li><li>[884736, 1]正是把原来的权重展开成1维后量化存储的结果</li></ul></li></ul><h3 id="_3-2-dataset-处理流程" tabindex="-1"><a class="header-anchor" href="#_3-2-dataset-处理流程" aria-hidden="true">#</a> 3.2. Dataset 处理流程</h3><ul><li>1.加载数据集 <ul><li>数据集有3列 <ul><li>instruction</li><li>input</li><li>output</li></ul></li><li>数据集有 51760 条数据</li></ul></li></ul><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>Dataset({
    features: [&#39;output&#39;, &#39;input&#39;, &#39;instruction&#39;],
    num_rows: 51760
})
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>2.选取小规模子集 <ul><li>随机选取 1000 条数据</li></ul></li></ul><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>Dataset({
    features: [&#39;output&#39;, &#39;input&#39;, &#39;instruction&#39;],
    num_rows: 1000
})
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>3.拼接 &#39;output&#39;, &#39;input&#39;, &#39;instruction&#39; 这3个字段为一个字符串，作为 &#39;text&#39; 字段</li></ul><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>Dataset({
    features: [&#39;output&#39;, &#39;input&#39;, &#39;instruction&#39;, &#39;text&#39;],
    num_rows: 1000
})
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>4.将 &quot;text&quot; 字段采用 tokenizer 进行 token 化，生成 &quot;input_ids&quot; 和 &quot;attention_mask&quot; 字段</li></ul><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>Dataset({
    features: [&#39;output&#39;, &#39;input&#39;, &#39;instruction&#39;, &#39;text&#39;, &#39;input_ids&#39;, &#39;attention_mask&#39;],
    num_rows: 1000
})
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_3-3-可训练参数量计算" tabindex="-1"><a class="header-anchor" href="#_3-3-可训练参数量计算" aria-hidden="true">#</a> 3.3. 可训练参数量计算</h3><ul><li>基础模型参数量 <ul><li>81912576</li></ul></li><li>模型结构中的目标模块 <ul><li>attn <ul><li>(c_attn): Conv1D(nf=2304, nx=768) -&gt; (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)</li><li>(c_proj): Conv1D(nf=768, nx=768) -&gt; (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)</li></ul></li><li>mlp <ul><li>(c_fc): Conv1D(nf=3072, nx=768) -&gt; (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)</li><li>(c_proj): Conv1D(nf=768, nx=3072) -&gt; (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)</li></ul></li></ul></li><li>参数之间的关系 <ul><li>总参数量 82,502,400 - QLoRA可训练参数量 589,824 = 基础模型参数量 81912576</li><li>QLoRA可训练参数量 589,824 = 6层 * (24576 + 12288 + 30720 + 30720) = 6 * 98304 <ul><li>共6层，每层4个目标模块 <ul><li>c_attn: 768 * 8 + 2304 * 8 = 24576</li><li>c_proj: 768 * 8 + 768 * 8 = 12288</li><li>c_fc: 3072 * 8 + 768 * 8 = 30720</li><li>c_proj: 768 * 8 + 3072 * 8 = 30720</li></ul></li></ul></li></ul></li></ul><h2 id="_4-qlora-应用价值" tabindex="-1"><a class="header-anchor" href="#_4-qlora-应用价值" aria-hidden="true">#</a> 4. QLoRA 应用价值</h2><h3 id="_4-1-qlora-和全参数微调方法对比" tabindex="-1"><a class="header-anchor" href="#_4-1-qlora-和全参数微调方法对比" aria-hidden="true">#</a> 4.1. QLoRA 和全参数微调⽅法对比</h3><ul><li>微调参数量： <ul><li>QLoRA：仅微调一小部分参数（LoRA Adapter），在本例中，可训练参数仅占总参数的 0.7149%</li><li>全参数微调：更新所有模型参数，需要更多内存和计算资源</li></ul></li><li>内存使用： <ul><li>QLoRA：使用 4-bit 量化，内存占用大幅减少</li><li>全参数微调：需要全精度（例如 16 位或 32 位），通常需要较大的 GPU 内存</li></ul></li><li>训练速度： <ul><li>QLoRA：由于参数少且精度低，训练更快</li><li>全参数微调：更新所有权重，训练时间较长</li></ul></li><li>性能： <ul><li>QLoRA：由于量化和有限的参数更新，性能可能略低于全参数微调，但仍保留大部分能力</li><li>全参数微调：因所有权重都被优化，性能可能更高</li></ul></li><li>适用场景： <ul><li>QLoRA：适合资源受限环境或快速实验</li><li>全参数微调：适合资源充足且需最大精度的场景</li></ul></li></ul><h3 id="_4-2-qlora-在该任务中的优势和潜在局限性" tabindex="-1"><a class="header-anchor" href="#_4-2-qlora-在该任务中的优势和潜在局限性" aria-hidden="true">#</a> 4.2. QLoRA 在该任务中的优势和潜在局限性</h3><p>优势：</p><ul><li>内存效率：4 位量化和 LoRA 减少了内存使用，使微调能在较小的 GPU 上运行（例如 Colab 免费版）</li><li>速度：由于参数少且精度低，训练速度更快，适合快速迭代</li><li>保留预训练知识：冻结大部分权重保留了基础模型的泛化能力，同时适配任务</li><li>适用于小数据集：对像 1,000 条 Alpaca 子集这样的小数据集效果良好，不易过拟合</li></ul><p>局限性：</p><ul><li>性能权衡：量化和 LoRA 可能导致性能略低于全参数微调，尤其在复杂任务中</li><li>任务特定性：LoRA 适配器是任务特定的，切换任务需重新训练或维护多个适配器</li><li>量化噪声：4-bit 精度引入噪声，可能影响输出质量</li><li>超参数敏感性：调整 <code>r</code>、<code>lora_alpha</code> 和量化设置需要实验，增加了复杂性</li></ul><h2 id="_5-qlora-疑点思考" tabindex="-1"><a class="header-anchor" href="#_5-qlora-疑点思考" aria-hidden="true">#</a> 5. QLoRA 疑点思考</h2><h3 id="_5-1-lora和qlora微调的方式是不是只有训练前的模型是否进行了量化这一区别" tabindex="-1"><a class="header-anchor" href="#_5-1-lora和qlora微调的方式是不是只有训练前的模型是否进行了量化这一区别" aria-hidden="true">#</a> 5.1. lora和qlora微调的方式是不是只有训练前的模型是否进行了量化这一区别？</h3><p>是的，主要的区别就是训练前的模型是否已经进行了量化</p><ul><li>LoRA 微调的是一个 未量化的预训练模型，权重通常是 float32。</li><li>QLoRA 微调的是一个 已量化的预训练模型，例如 4bit 或 8bit。权重是 量化后的 uint8 或 int8 格式。</li></ul><p>内存与推理效率：</p><ul><li>LoRA 在微调时占用的内存较大，因为它使用的是 全精度 的预训练模型（通常是 float32）。</li><li>QLoRA 的优势在于 内存占用大大降低，因为模型已经量化到 4bit。即使是大模型，也能在有限的显存中运行。</li></ul><p>相同点</p><ul><li>LoRA 和 QLoRA 都是通过训练一个 低秩适配矩阵 来微调原始模型，无需修改原始的预训练权重</li><li>两者都只训练适配层，因此参数量相对较少，适合资源有限的场景</li></ul><p>LoRA（Low-Rank Adaptation）微调步骤</p><ul><li>加载一个 预训练模型（通常是 float32 权重）。</li><li>冻结原始权重，只训练 LoRA 的适配矩阵。</li><li>微调时，通过训练 LoRA 层来使模型适应特定任务。</li><li>推理时，使用 原始权重 加上训练好的 LoRA 层的适配矩阵</li></ul><p>QLoRA（Quantized LoRA）微调步骤</p><ul><li>量化预训练模型：加载一个 已经量化的模型，例如 4bit 或 8bit 模型。</li><li>冻结量化后的权重，只训练 LoRA 适配矩阵。</li><li>在 量化后的模型 上进行微调，优化 LoRA 层的适配矩阵。</li><li>推理时，仍然使用 量化后的权重，加上训练好的 LoRA 适配矩阵。</li></ul><h3 id="_5-2-为什么量化通常只用于线性层" tabindex="-1"><a class="header-anchor" href="#_5-2-为什么量化通常只用于线性层" aria-hidden="true">#</a> 5.2. 为什么量化通常只用于线性层？</h3><p>原因可以从以下几个角度来解释：数学、工程实现、对模型影响</p><ul><li>1.线性层是参数最多、计算量最大的部分（参数集中） <ul><li>层归一化（LayerNorm）、激活函数（GELU）、Dropout 等几乎不含权重或只有极少量参数</li><li>所以优先量化线性层，收益最大，且对模型结构改动最小</li></ul></li><li>2.线性层结构简单、便于量化和解码（数学结构简单） <ul><li>线性层是少数可以同时量化权重 + 激活 + 梯度的层（如果需要），所以它成为了主战场</li><li>量化通常分为两种 <ul><li>权重量化（Weight Quantization）：将 weight 低精度存储（最常见）</li><li>激活量化（Activation Quantization）：对中间值进行量化（更复杂）</li></ul></li><li>线性变换是一个非常清晰、固定的数学结构，适合被： <ul><li>编码为 4-bit / 8-bit 数组</li><li>解码时用 scale + zero_point 进行恢复</li><li>用 kernel-fusion 加速（比如 bitsandbytes 提供的 CUDA 实现）</li></ul></li></ul></li><li>3.非线性操作不好量化（其他层不适合，量化线性层影响较小） <ul><li>这些层要么不含参数，要么行为变化大，量化这些部分带来的收益不明显，反而增加复杂度和误差 <ul><li>非线性函数（GELU, ReLU）没有固定权重，不好提前编码</li><li>归一化（LayerNorm, BatchNorm）涉及动态计算均值、方差、除法，不好静态量化</li><li>控制结构（Dropout, Mask）行为在训练/推理不同，量化意义不大</li></ul></li></ul></li><li>4.工程上已经高度优化了线性层量化（工程已优化） <ul><li>现有框架都专注优化了 Linear, Conv 这些算子，对非线性操作支持很差甚至没有 <ul><li>bitsandbytes</li><li>Intel Neural Compressor</li><li>NVIDIA TensorRT</li><li>ONNX Runtime + QNNPACK</li></ul></li></ul></li></ul><h3 id="_5-3-为什么不对-lm-head-层进行量化-这个也是线性层" tabindex="-1"><a class="header-anchor" href="#_5-3-为什么不对-lm-head-层进行量化-这个也是线性层" aria-hidden="true">#</a> 5.3. 为什么不对 lm_head 层进行量化，这个也是线性层?</h3><p>按道理讲，这个层也可以量化，但它默认并没有被量化</p><p>原因主要是以下几个：</p><ul><li>1.lm_head 用于输出 → 精度影响更敏感 （精度敏感） <ul><li>lm_head 是模型的最后一层，用于将隐藏状态投影到词表（vocab）空间，输出 logits</li><li>这一层的输出直接影响： <ul><li>softmax 的分布</li><li>预测的 token 排序</li><li>最终生成文本的质量</li></ul></li><li>所以，它对精度非常敏感。轻微的权重误差可能就会导致 token 排序错误，生成完全不同的结果</li></ul></li><li>2.大多数量化框架默认跳过 lm_head （框架默认行为，空间节省有限） <ul><li>比如 bitsandbytes、AutoGPTQ 等，在量化 Transformer 模型时默认不对 lm_head 做 4-bit 量化，因为： <ul><li>这一层通常只有一份（不像 attention 和 MLP 有很多层）</li><li>精度影响更大</li><li>节省空间有限（相对整模型来说），量化意义不大</li></ul></li></ul></li><li>3.可能用于权重共享（tie_weights）（权重共享，与 embedding 权重绑定，量化可能不兼容） <ul><li>有些模型中，lm_head.weight 是和嵌入层 wte.weight 共享权重的（tie-weights）</li><li>此时量化 lm_head 就会影响嵌入 → 编码 → 解码的整个闭环</li><li>如果你量化了 lm_head，但没同步量化 wte，或者反之，可能导致不一致甚至报错</li></ul></li></ul><h3 id="_5-4-为什么非线性层在量化后的dtype也有变化-变为float16" tabindex="-1"><a class="header-anchor" href="#_5-4-为什么非线性层在量化后的dtype也有变化-变为float16" aria-hidden="true">#</a> 5.4. 为什么非线性层在量化后的dtype也有变化，变为float16？</h3><p>虽然没有显式设置这些层的 dtype，但它们会在 load_in_4bit=True 时自动被转换成更轻量的精度</p><p>这其实是 transformers 的 AutoPrecision 推理机制（自动调度）的一部分，和 bitsandbytes 一起配合使用</p><h3 id="_5-5-量化模型精度发生了什么变化" tabindex="-1"><a class="header-anchor" href="#_5-5-量化模型精度发生了什么变化" aria-hidden="true">#</a> 5.5. 量化模型精度发生了什么变化?</h3><ul><li>原始模型精度为 float32 (磁盘)</li><li>量化模型的权重以 int4 存储（packed uint8） （显存）</li><li>input float32/bfloat16 (外部传入)</li><li>权重 int4 临时解码成 bfloat16 计算</li><li>double quant → 再压缩 scale，减少负担</li><li>output bfloat16 → 后续层可以继续低精度执行</li></ul><p>Note1：只有线性层的存储为4bit，其他层的存储、计算、解码输出这些都是bfloat16/float16</p><p>Note2：推理时不会改变模型的参数，只有反向传播时才会改变，推理时bit4的临时变为bfloat16进行计算，然后输出结果output也是bfloat16类型的</p><p>Note3：微调量化模型（比如 LoRA + 4bit），也是只更新部分 float 参数（如 LoRA adapter），4bit 权重本身不会直接修改</p><h3 id="_5-6-bnb-4bit-compute-dtype-torch-bfloat16是做什么的-计算精度和存储精度不同" tabindex="-1"><a class="header-anchor" href="#_5-6-bnb-4bit-compute-dtype-torch-bfloat16是做什么的-计算精度和存储精度不同" aria-hidden="true">#</a> 5.6. bnb_4bit_compute_dtype=torch.bfloat16是做什么的，计算精度和存储精度不同？</h3><p>默认情况，即使你量化了参数，计算时还是 float32</p><p>所以可以设置计算精度为torch.bfloat16 或 torch.float16来加速计算过程，减少显存占用</p><p>计算精度这个 dtype 是在 Linear4bit 内部运行时设置的，属于内部计算逻辑</p><h3 id="_5-7-临时性量化与永久存储" tabindex="-1"><a class="header-anchor" href="#_5-7-临时性量化与永久存储" aria-hidden="true">#</a> 5.7. 临时性量化与永久存储？</h3><ul><li>临时性量化 <ul><li>加载原始的 float32 权重模型，根据量化配置进行量化存在内存中</li><li>推理时从内存中获取4bit临时转为bfloat16进行计算</li><li>关键 <ul><li>量化模型的 4bit 权重只在内存中存在，它们是临时的，不会被永久保存</li><li>并没有改变磁盘上的原始模型文件，而是只影响了内存中的计算方式</li></ul></li></ul></li><li>永久保存 <ul><li>保存模型的量化版本，加载模型时直接加载量化后的模型，不需要进行量化配置</li></ul></li></ul><h3 id="_5-8-qlora存储训练好的模型是按什么精度存储的" tabindex="-1"><a class="header-anchor" href="#_5-8-qlora存储训练好的模型是按什么精度存储的" aria-hidden="true">#</a> 5.8. qlora存储训练好的模型是按什么精度存储的？</h3><p>QLoRA 在训练过程中的量化和存储过程有两种常见的策略</p><ul><li>情况 1：训练时量化，存储时保留原始模型的 float32 权重 <ul><li>原始模型的权重仍然会恢复为 float32 格式，而不是以量化后的格式保存</li><li>量化操作通常是在推理时临时应用的，量化权重在训练时只是为了节省内存和加速计算，但不一定要永久存储</li><li>将原始模型保存在 float32 格式 可以确保更好的模型兼容性和后续的易用性</li><li>LoRA 适配层：LoRA 层的适配权重 仍然是 float32 格式，会和原始模型一起存储</li><li>QLoRA 保存的模型是： <ul><li>存储的是原始模型的权重float32格式。</li><li>存储的是 LoRA 适配层的权重，这些适配层依然是 float32 精度的。</li></ul></li></ul></li><li>情况 2：训练和存储时都量化 <ul><li>这种方法主要用于 节省存储空间 和 加速推理</li><li>QLoRA 保存的模型是： <ul><li>存储的是 量化后的模型，即 4bit（通常是 uint8）权重。</li><li>存储的是 LoRA 适配层的权重，这些适配层依然是 float32 精度的。</li></ul></li></ul></li></ul><p>不需要将 LoRA 适配层权重和模型权重的数据类型保持一致吗？答：数据类型不必完全一致，原因如下</p><ul><li>LoRA 适配层与原始模型权重的独立性</li><li>LoRA 层权重和量化后的模型权重的数据类型不一致并不冲突 <ul><li>推理时，量化的权重（例如 4bit）会通过反量化过程恢复为 float16 或 bfloat16 格式进行计算，而 LoRA 层的权重（float32）依然参与计算，二者的精度不一致不会导致问题</li></ul></li></ul><h2 id="_6-qlora-细节补充" tabindex="-1"><a class="header-anchor" href="#_6-qlora-细节补充" aria-hidden="true">#</a> 6. QLoRA 细节补充</h2><h3 id="_6-1-device-map" tabindex="-1"><a class="header-anchor" href="#_6-1-device-map" aria-hidden="true">#</a> 6.1. device_map</h3><p>device_map 是一个字典，用于告诉模型加载器（比如 transformers 中的 AutoModel.from_pretrained）将模型的各个部分加载到哪些设备上。键（key）通常表示模型的某个部分（例如层或模块的名称），值（value）表示设备编号或设备名称（例如 GPU 的索引号 0、1，或者 &quot;cpu&quot;）。</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>device_map={&quot;&quot;: 0}  # 将整个模型加载到 GPU 0（设备编号为 0 的设备上）（空字符串表示整个模型；在 PyTorch 中，0 通常对应于第一个 GPU（即 cuda:0））
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>device_map = {
    &quot;transformer.layer.0&quot;: 0,  # 第 0 层加载到 GPU 0
    &quot;transformer.layer.1&quot;: 1   # 第 1 层加载到 GPU 1
}
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>device_map = &quot;auto&quot; # 根据可用设备自动分配模型（需要 accelerate 库支持）
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>延伸：如何检查 GPU 可用设备</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>import torch
print(torch.cuda.is_available())  # 检查是否有 GPU
print(torch.cuda.device_count())  # 检查 GPU 数量
print(torch.cuda.current_device())  # 当前默认 GPU 编号
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-2-seed-42-是常见-魔法数字" tabindex="-1"><a class="header-anchor" href="#_6-2-seed-42-是常见-魔法数字" aria-hidden="true">#</a> 6.2. seed=42 是常见“魔法数字”</h3><p>42 是个惯例值，出自《银河系漫游指南》，表示“生命、宇宙以及一切问题的终极答案”。当然你用 123 或别的值也完全没问题，只要保证每次用一样的 seed就能复现</p><h2 id="_7-reference" tabindex="-1"><a class="header-anchor" href="#_7-reference" aria-hidden="true">#</a> 7. Reference</h2><ul><li>huggingface SFTTrainer: https://huggingface.co/docs/trl/v0.7.4/en/sft_trainer</li><li>google: https://ai.google.dev/gemma/docs/core/huggingface_text_finetune_qlora</li><li>unsloth: https://huggingface.co/blog/Andyrasika/finetune-unsloth-qlora</li></ul>`,92);function r(d,m){return s(),e("div",null,[p,u,t(" more "),c])}const k=a(o,[["render",r],["__file","031_qlora.html.vue"]]);export{k as default};
