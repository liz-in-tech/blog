import{_ as o,a as s,b as l,c,d,e as u,f as m,g as p,h,i as g,j as f,k as n,l as b,m as y,n as v,o as w,p as _,q as x,r as z,s as k,t as P,u as T,v as M}from"./024_scaling_experiments1-y0ZJJCSZ.js";import{_ as F}from"./024_mixed_precision_training_list-dG4PWcKt.js";import{_ as U}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as B,o as S,c as A,f as R,a as e,b as i,d as r,e as a}from"./app-8DF3I3_Y.js";const C={},G=e("h1",{id:"distributed-training-part-1-memory-usage-in-model-training",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#distributed-training-part-1-memory-usage-in-model-training","aria-hidden":"true"},"#"),i(" Distributed Training Part 1: Memory Usage in Model Training")],-1),D=e("ul",null,[e("li",null,"Model Training Process and Important Hyperparameter"),e("li",null,"Memory Usage in Model Training"),e("li",null,[i("Memory Optimization Suggestions "),e("ul",null,[e("li",null,"Activation Recomputation / Gradient Checkpointing"),e("li",null,"Gradient Accumulation"),e("li",null,"Mixed Precision Training")])])],-1),q=a('<h2 id="_1-metrics" tabindex="-1"><a class="header-anchor" href="#_1-metrics" aria-hidden="true">#</a> 1. Metrics</h2><p>Objective: Fully utilize the expensive hardware of GPUs</p><ul><li>Throughput</li><li>GPU Utilization</li><li>Training Time</li></ul><h2 id="_2-three-key-challenges" tabindex="-1"><a class="header-anchor" href="#_2-three-key-challenges" aria-hidden="true">#</a> 2. Three Key Challenges</h2><ul><li>Memory Usage <ul><li>It&#39;s a hard limitation - if a training step doesn&#39;t fit in memory, training cannot proceed</li><li>Out-of-Memory (OOM) issues</li></ul></li><li>Compute Efficiency <ul><li>We want our hardware to spend most time computing, so we need to reduce time spent on data transfers or waiting for other GPUs to perform work.</li></ul></li><li>Communication Overhead <ul><li>We want to minimize communication overhead as it keeps GPUs idle. To achieve this, we will try to make the best use of intra-node (fast) and inter-node (slower) bandwidths as well as overlap communication with compute as much as possible.</li></ul></li></ul><h2 id="_3-basics-of-model-training" tabindex="-1"><a class="header-anchor" href="#_3-basics-of-model-training" aria-hidden="true">#</a> 3. Basics of Model Training</h2><h3 id="_3-1-model-training-process" tabindex="-1"><a class="header-anchor" href="#_3-1-model-training-process" aria-hidden="true">#</a> 3.1. Model Training Process</h3><p>Model training consists of three steps:</p><ul><li>Forward Pass: Inputs are passed to the model to produce outputs</li><li>Backward Pass: Gradients are computed</li><li>Optimization Step: The optimizer uses gradients to update model parameters</li></ul><figure><img src="'+o+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_3-2-important-hyperparameter-batch-size" tabindex="-1"><a class="header-anchor" href="#_3-2-important-hyperparameter-batch-size" aria-hidden="true">#</a> 3.2. Important Hyperparameter -- Batch Size</h3><h4 id="_3-2-1-batch-size-bs" tabindex="-1"><a class="header-anchor" href="#_3-2-1-batch-size-bs" aria-hidden="true">#</a> 3.2.1. Batch Size (bs)</h4><p>Affects both model convergence and throughput</p><p>A small batch size can be useful early in training to quickly move along the training landscape reaching an optimal learning point. However, further along the model training, small batch sizes will keep gradients noisy and the model may not be able to converge to the most optimal final performances. At the other extreme, a large batch size while giving very accurate gradient estimations will tend to make less use of each training token rendering convergence slower and potentially wasting compute.</p><p>Batch size also affects the time it takes to train on a given text dataset: a small batch size will require more optimizer steps to train on the same amount of samples. Optimizer steps are costly (in compute time) and the total time to train will thus increase compared to using a larger batch size. This being said, note that the batch size can often be adjusted quite largely around the optimal batch size without major impact to the performance of the model, i.e. the sensitivity of final model performances to the exact batch size value is usually rather low around the optimal batch size.</p><p>Batch size extension links:</p><ul><li>OpenAI Paper: https://arxiv.org/pdf/1812.06162</li><li>MiniMax-01 Paper: https://filecdn.minimax.chat/_Arxiv_MiniMax_01_Report.pdf</li></ul><h4 id="_3-2-2-batch-size-tokens-bst" tabindex="-1"><a class="header-anchor" href="#_3-2-2-batch-size-tokens-bst" aria-hidden="true">#</a> 3.2.2. Batch Size Tokens (bst)</h4><p>In the field of LLM pre-training, batch size is often reported in terms of tokens rather than the number of samples (bst = Batch Size Tokens). This approach makes the amount of training data independent of the specific input sequence length used during training.</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>bst = bs * seq
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>where seq is the model input sequence length</p><ul><li>Ideal Batch Size: 4-60 million tokens</li><li>Llama 1: For 1.4 trillion tokens, batch size is about 4 million tokens</li><li>DeepSeek: For 14 trillion tokens, batch size is about 60 million tokens</li></ul><p>A sweet spot for recent LLM training is typically on the order of 4-60 million tokens per batch. The batch size as well as the training corpus have been steadily increasing over the years: Llama 1 was trained with a batch size of ~4M tokens for 1.4 trillion tokens while DeepSeek was trained with a batch size of ~60M tokens for 14 trillion tokens.</p><h2 id="_4-memory-usage-in-model-training" tabindex="-1"><a class="header-anchor" href="#_4-memory-usage-in-model-training" aria-hidden="true">#</a> 4. Memory Usage in Model Training</h2><p>Four components:</p><ul><li>Model Parameters (weights &amp; Biases) <ul><li>Purpose: Determine the model&#39;s performance; training the model involves updating these parameters</li><li>Memory Usage: Determined by the number of parameters, each being a floating-point number, depending on the precision used</li><li>Variation: Fixed during training</li></ul></li><li>Optimizer States <ul><li>Purpose: Assist in parameter updates to minimize the loss function</li><li>Memory Usage: Different optimizers occupy different amounts of memory. Many optimizers (like Adam, RMSprop) store additional states for each parameter (like momentum, squared gradients). For example, the Adam optimizer stores two additional variables for each parameter: momentum and the average of squared gradients. This means each parameter occupies additional memory, usually twice its parameter memory usage.</li><li>Variation: Fixed during training</li></ul></li><li>Activations <ul><li>Purpose: Outputs of each layer during the forward pass, used to compute gradients during the backward pass</li><li>Memory Usage: Related to batch size, sequence length, and model architecture. Typically large, especially in deep networks or large batch training.</li><li>Variation: Stored after each forward pass and used during the backward pass, varies with batch size and input data, dynamically changes during training</li></ul></li><li>Gradients <ul><li>Purpose: Used in the backward pass to compute the direction and magnitude of parameter updates</li><li>Memory Usage: During the backward pass, a gradient matrix of the same dimension is stored for each parameter (each parameter corresponds to a gradient), occupying memory equal to the size of model parameters</li><li>Variation: Dynamically changes during training, computed and stored during the backward pass, released after parameter updates</li></ul></li></ul><p>These four components are stored as tensors with different shapes and precisions.</p><p>Hyperparameters determining shapes:</p><ul><li>Batch size</li><li>Sequence length</li><li>Model hidden dimensions</li><li>Attention heads</li><li>Vocabulary size</li><li>Model sharding</li></ul><p>Common precisions:</p><ul><li>FP32 (full precision) -&gt; 4 bytes</li><li>BF16 -&gt; 2 bytes</li><li>FP8 -&gt; 1 byte</li></ul><p>During training, memory usage is continuously changing rather than static.</p><ul><li>Initialization <ul><li>Initialize model parameters (neural network models usually randomly initialize weights, with some methods like Xavier initialization, He initialization helping to avoid gradient vanishing or explosion issues)</li></ul></li><li>Iterative Loop <ul><li>Forward Pass <ul><li>Compute model outputs, store activations (hidden_state)</li></ul></li><li>Compute Loss <ul><li>Compute loss value (usually a scalar, small memory usage)</li></ul></li><li>Backward Pass <ul><li>Compute and store gradients</li></ul></li><li>Parameter Update <ul><li>Optimizer updates model parameters based on gradients and maintains optimizer states</li></ul></li></ul></li></ul><figure><img src="`+s+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_5-memory-optimization-suggestions" tabindex="-1"><a class="header-anchor" href="#_5-memory-optimization-suggestions" aria-hidden="true">#</a> 5. Memory Optimization Suggestions</h2><ul><li>Activation Recomputation: Reduce memory usage of activations by storing only part of them and recomputing when needed to save memory</li><li>Gradient Accumulation: Accumulate gradients from multiple small batches before updating parameters to reduce memory usage</li><li>Mixed Precision Training: Reduce memory usage of parameters and gradients by using half-precision floating-point numbers (float16) instead of single-precision (float32)</li><li>Distributed Training: Distribute memory pressure by spreading the model or data across multiple GPUs to reduce memory usage on a single GPU</li><li>Reduce Batch Size: Reducing batch size decreases memory usage but may affect training speed</li><li>Reduce Model Size: Use smaller models or model compression techniques</li></ul><p>Extension Link: https://zdevito.github.io/2022/08/04/cuda-caching-allocator.html</p><h2 id="_6-activation-recomputation-gradient-checkpointing-rematerialization" tabindex="-1"><a class="header-anchor" href="#_6-activation-recomputation-gradient-checkpointing-rematerialization" aria-hidden="true">#</a> 6. Activation Recomputation / Gradient Checkpointing / Rematerialization</h2><p>Trading time for space, computation for memory: discard some activations computed during the forward pass to save memory and spend extra computation during the backward pass to dynamically recompute activations.</p><p>Activation Storage Content:</p><ul><li>Without Recomputation: Store every hidden state between two learnable operations (e.g., feedforward network, layer normalization) for use in computing gradients during the backward pass.</li><li>With Recomputation: Store activations only at a few key points in the model architecture, discard the rest, and dynamically recompute them during the backward pass starting from the nearest saved activation, essentially re-executing a subpart of the forward pass.</li></ul><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Activation Recomputation Strategies:</p><ul><li>Full Recomputation <ul><li>Perform a complete forward pass during the backward pass</li><li>Memory Usage: Activations occupy almost no memory</li><li>Computational Cost: Increases computational cost and time by up to 30-40%</li></ul></li><li>Selective Recomputation (Preferred) <ul><li>Discard and recompute the attention part (which causes the largest growth in activations but has the cheapest computational cost)</li><li>Memory Usage: Reduces activation memory usage by 70% (significantly reduces memory access overhead)</li><li>Computational Cost: Increases computational cost by 2.7% (slightly increases the number of FLOPS, where FLOPS stands for Floating point operations per second)</li><li>This trade-off is particularly advantageous on hardware with small, fast memory, such as GPUs, because accessing memory is usually slower than performing computations. Despite involving additional operations, the overall effect is often faster computation with significantly reduced memory usage.</li></ul></li></ul><figure><img src="'+c+'" alt="No Recomputation - 8B" tabindex="0" loading="lazy"><figcaption>No Recomputation - 8B</figcaption></figure><figure><img src="'+d+'" alt="Full Recomputation - 8B" tabindex="0" loading="lazy"><figcaption>Full Recomputation - 8B</figcaption></figure><figure><img src="'+u+'" alt="Selective Recomputation - 8B" tabindex="0" loading="lazy"><figcaption>Selective Recomputation - 8B</figcaption></figure><figure><img src="'+m+'" alt="No Recomputation - 70B" tabindex="0" loading="lazy"><figcaption>No Recomputation - 70B</figcaption></figure><figure><img src="'+p+'" alt="Full Recomputation - 70B" tabindex="0" loading="lazy"><figcaption>Full Recomputation - 70B</figcaption></figure><figure><img src="'+h+`" alt="Selective Recomputation - 70B" tabindex="0" loading="lazy"><figcaption>Selective Recomputation - 70B</figcaption></figure><ul><li>The smaller the model, the larger the proportion of activations</li><li>The longer the sequence, the larger the proportion of activations</li><li>For small models with long sequences, recomputation has a significant impact on memory</li></ul><p>Implementation of Selective Recomputation: FlashAttention</p><p>However, activations still have a linear dependency on batch size, and all the profiles in the bar charts above use a batch size of 1, so this may become an issue again when we move to larger batch sizes. Don&#39;t despair, because we have a second tool—gradient accumulation to save the day!</p><h2 id="_7-gradient-accumulation" tabindex="-1"><a class="header-anchor" href="#_7-gradient-accumulation" aria-hidden="true">#</a> 7. Gradient Accumulation</h2><p>Trading time for space, computation for memory: further divide the batch into micro-batches, perform forward and backward passes for each micro-batch to compute gradients, then accumulate the gradients from each micro-batch (gradient accumulation is actually averaging rather than summing, so it is not affected by the number of micro-batches), and finally execute the optimizer update step.</p><p>Terminology:</p><ul><li>Batch size (bs)</li><li>Micro batch size (mbs)</li><li>Global batch size (gbs)</li><li>grad_acc: the number of gradient accumulation steps</li></ul><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>bs = gbs = mbs * grad_acc
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><figure><img src="`+g+'" alt="Gradient Accumulation" tabindex="0" loading="lazy"><figcaption>Gradient Accumulation</figcaption></figure><p>Advantages and Disadvantages of Gradient Accumulation:</p><ul><li>Advantages: <ul><li>Allows batch size to be set larger while keeping memory usage stable, reducing memory usage of activations that grow linearly with batch size through micro-batch size</li><li>Compatible with Activation Recomputation, can be used together to reduce memory usage</li><li>Forward and backward computations of multiple micro-batches can be processed in parallel</li></ul></li><li>Disadvantages: <ul><li>Requires computation of multiple forward and backward passes, increasing computational cost</li></ul></li></ul><h2 id="_8-mixed-precision-training" tabindex="-1"><a class="header-anchor" href="#_8-mixed-precision-training" aria-hidden="true">#</a> 8. Mixed Precision Training</h2><h3 id="_8-1-numerical-range-and-precision-of-floating-point-numbers" tabindex="-1"><a class="header-anchor" href="#_8-1-numerical-range-and-precision-of-floating-point-numbers" aria-hidden="true">#</a> 8.1. Numerical Range and Precision of Floating-Point Numbers</h3><p>Default numerical precision of PyTorch tensors: single-precision floating-point format, also known as FP32 or float32</p><ul><li>This means each stored number occupies 32 bits (i.e., 4 bytes)</li></ul><p>The available bits are divided into three parts to represent a number (scientific notation):</p><ul><li>Sign Bit: The first bit determines whether the number is positive or negative</li><li>Exponent: Controls the range of the number</li><li>Mantissa: Determines the significant digits of the number</li></ul><figure><img src="'+f+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>List of floating-point formats provided by PyTorch:</p><ul><li>FP32 / float32 / 32-bit Floating Point</li><li>FP16 / float16 / 16-bit Floating Point</li><li>BF16 / bfloat16 / 16-bit Brain Floating Point</li><li>FP8 / float8 / 8-bit Floating Point</li></ul><figure><img src="'+n+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Note:</p><ul><li>bfloat16 was proposed by Google Brain, where &#39;b&#39; stands for &quot;brain&quot;</li><li>Two types of float8 are named based on exponent and mantissa (e4m3 and e5m2)</li></ul><p>We focus on two aspects of floating-point numbers: precision and numerical range</p><ul><li>Precision: The fineness of the numbers that can be represented (i.e., the gap between two adjacent representable numbers)</li><li>Numerical Range: The maximum and minimum values that can be represented</li></ul><p>Numerical range of different floating-point numbers:</p><figure><img src="'+n+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>From the chart (look at the width, the wider the range, the larger the numerical range):</p><ul><li>float32 and bfloat16 have the same and relatively large numerical range</li><li>float16 and float8_e5m2 have the same numerical range, which is relatively small</li><li>float8_e4m3 has the smallest numerical range</li></ul><p>Precision of different floating-point numbers: <img src="'+b+'" alt="" loading="lazy"></p><p>From the chart (look at the spacing of the vertical lines, the smaller the spacing, the greater the precision):</p><ul><li>bfloat16 has lower precision than float32 and float16</li></ul><h3 id="_8-2-concept-of-mixed-precision-training" tabindex="-1"><a class="header-anchor" href="#_8-2-concept-of-mixed-precision-training" aria-hidden="true">#</a> 8.2. Concept of Mixed Precision Training</h3><p>The concept of mixed precision training is to use lower precision formats to reduce computational and memory requirements while maintaining performance comparable to full precision (float32) training.</p><p>However, completely abandoning float32 is impractical because certain critical parts require higher precision to avoid numerical instability. Therefore, in practice, a mix of high and low precision formats is often used, a method known as &quot;mixed precision training.&quot;</p><h3 id="_8-3-summary-of-known-methods-for-mixed-precision-training" tabindex="-1"><a class="header-anchor" href="#_8-3-summary-of-known-methods-for-mixed-precision-training" aria-hidden="true">#</a> 8.3. Summary of Known Methods for Mixed Precision Training</h3><figure><img src="'+F+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Number of Parameters: Ψ</p><ul><li>BF16+FP32 Mixed Precision Baseline: 2Ψ + 6Ψ + 12Ψ = 20Ψ <ul><li>Model Parameters (half precision): 2 bytes</li><li>Gradients (half precision) + FP32 gradients (accumulated in FP32 precision): 2 + 4 = 6 bytes</li><li>FP32 Model Parameters and Optimizer States: 4 + (4 + 4) = 12 bytes</li></ul></li><li>BF16+FP32 Mixed Precision without FP32 Gradients: 2Ψ + 2Ψ + 12Ψ = 16Ψ <ul><li>Model Parameters (half precision): 2 bytes</li><li>Gradients (half precision): 2 bytes</li><li>FP32 Model Parameters and Optimizer States: 4 + (4 + 4) = 12 bytes</li></ul></li></ul><h3 id="_8-4-fp16-and-bf16-training" tabindex="-1"><a class="header-anchor" href="#_8-4-fp16-and-bf16-training" aria-hidden="true">#</a> 8.4. FP16 and BF16 Training</h3><p>Simply switching all tensors and operations to float16 format usually doesn&#39;t work, often resulting in divergent loss values. However, the initial mixed precision training paper proposed three techniques to maintain the performance of float32 training:</p><ul><li>FP32 Copy of Weights: <ul><li>Using float16 weights can encounter two issues. During training, some weights may become very small and be rounded to 0. Even if the weights themselves are not close to 0, the magnitude difference may cause weights to underflow in addition operations if the update amount is very small. Once weights become 0, they will remain 0 in subsequent training because no gradient signal is passed.</li></ul></li><li>Loss Scaling: <ul><li>Gradients face similar issues because they are often much smaller than 1 and prone to underflow. A simple but effective strategy is to scale (amplify) the loss before backpropagation and then reverse scale (reduce) the gradients after backpropagation. This ensures no underflow occurs during backpropagation, and the scaling operation doesn&#39;t affect training because we reverse scale before further processing gradients (e.g., clipping) and optimization steps.</li></ul></li><li>Accumulation: <ul><li>Performing certain arithmetic operations (e.g., averaging or summing) in 16-bit precision may also encounter underflow or overflow issues. The solution is to use float32 precision to accumulate intermediate results during operations and only convert the result back to 16-bit precision at the end.</li></ul></li></ul><p>The core goal of these three techniques is to leverage the computational acceleration brought by low precision while ensuring training stability by introducing high precision (e.g., float32) operations to avoid numerical instability issues (e.g., gradient or weight underflow). Through these techniques, we can benefit from faster low precision arithmetic operations while maintaining training stability, resulting in higher throughput.</p><h3 id="_8-5-fp8-training" tabindex="-1"><a class="header-anchor" href="#_8-5-fp8-training" aria-hidden="true">#</a> 8.5. FP8 Training</h3><ul><li>FP8 precision and numerical range are very limited, prone to numerical instability and divergent loss, especially in high learning rate scenarios.</li><li>The main advantage of FP8 is its ability to significantly enhance computational efficiency (e.g., on NVIDIA H100 GPUs, FP8 matrix multiplication performance is twice that of bfloat16), making it attractive for training that seeks high throughput and low energy consumption.</li></ul><figure><img src="'+y+'" alt="Divergent Loss" tabindex="0" loading="lazy"><figcaption>Divergent Loss</figcaption></figure><h4 id="_8-5-1-fp8-mixed-precision-training-in-deepseek-v3" tabindex="-1"><a class="header-anchor" href="#_8-5-1-fp8-mixed-precision-training-in-deepseek-v3" aria-hidden="true">#</a> 8.5.1. FP8 Mixed Precision Training in DeepSeek-V3</h4><ul><li>The first successful, very large-scale FP8 mixed precision training was publicly reported in DeepSeek-V3.</li><li>The authors carefully analyzed each operation in the forward pass (Fprop) and activations (Dgrad) and weights (Wgrad) operations in the backward pass.</li><li>To address numerical instability caused by FP8&#39;s low precision, they adopted strategies similar to BF16 mixed precision training: keeping critical parts (e.g., aggregation operations and main weights) at higher precision (possibly float32 or bfloat16) while delegating compute-intensive operations (e.g., matrix multiplication) to FP8, thus ensuring stability while fully leveraging FP8&#39;s high-performance advantages.</li></ul><figure><img src="'+v+'" alt="FP8 Mixed Precision Training Framework in DeepSeek-V3" tabindex="0" loading="lazy"><figcaption>FP8 Mixed Precision Training Framework in DeepSeek-V3</figcaption></figure><p>DeepSeek-V3 Paper: http://arxiv.org/pdf/2412.19437</p><h2 id="_9-tools" tabindex="-1"><a class="header-anchor" href="#_9-tools" aria-hidden="true">#</a> 9. Tools</h2><h2 id="_9-1-memory-usage-calculation-tool-predict-memory" tabindex="-1"><a class="header-anchor" href="#_9-1-memory-usage-calculation-tool-predict-memory" aria-hidden="true">#</a> 9.1. Memory Usage Calculation Tool: Predict Memory</h2><p>Before diving into code and experiments, we want to understand how each method works at a high level and what its advantages and limits are. You&#39;ll learn about which parts of a language model eat away your memory and when during training it happens. You&#39;ll learn how we can solve memory constraints by parallelizing the models and increase the throughput by scaling up GPUs. As a result, you&#39;ll understand how the following widget to compute the memory breakdown of a transformer model works.</p><figure><img src="'+w+'" alt="Memory Usage Widget" tabindex="0" loading="lazy"><figcaption>Memory Usage Widget</figcaption></figure><p>Memory Usage Prediction Tool: https://huggingface.co/spaces/nanotron/predict_memory</p><figure><img src="'+_+'" alt="Memory Timeline" tabindex="0" loading="lazy"><figcaption>Memory Timeline</figcaption></figure><h2 id="_9-2-distributed-training-tool-for-visualizing-gpu-compute-and-communication-costs-profiler" tabindex="-1"><a class="header-anchor" href="#_9-2-distributed-training-tool-for-visualizing-gpu-compute-and-communication-costs-profiler" aria-hidden="true">#</a> 9.2. Distributed Training Tool for Visualizing GPU Compute and Communication Costs: Profiler</h2><p>Purpose: Understand and verify GPU compute and communication costs, identify bottlenecks</p>',108),L={href:"https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html",target:"_blank",rel:"noopener noreferrer"},O=a(`<p>Code:</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ],
    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),
    on_trace_ready=torch.profiler.tensorboard_trace_handler(&#39;./log/profile&#39;),
    with_stack=True
) as prof:
    for step in range(steps):
        train_step() 
        prof.step()
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="`+x+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>This generates a trace that we can visualize in TensorBoard or Chrome&#39;s trace viewer. The trace shows:</p><ul><li>CPU threads asynchronously launching kernels to the GPU</li><li>Multiple CUDA streams processing compute and communication in parallel</li><li>Kernel execution times and memory allocations For example, the trace shows CPU threads asynchronously launching kernels to the GPU, with compute kernels and communication occurring in parallel on different CUDA streams. The trace helps identify bottlenecks, such as:</li><li>Sequential compute and communication that could have been overlapped</li><li>Idle time where the GPU is waiting for data transfers</li><li>Memory movement between CPU and GPU</li><li>Kernel launch overhead on the CPU</li></ul><h2 id="_10-reference-ultrascale-playbook" tabindex="-1"><a class="header-anchor" href="#_10-reference-ultrascale-playbook" aria-hidden="true">#</a> 10. Reference: Ultrascale Playbook</h2>',6),I={href:"https://huggingface.co/spaces/nanotron/ultrascale-playbook",target:"_blank",rel:"noopener noreferrer"},N=a('<h3 id="_10-1-overview" tabindex="-1"><a class="header-anchor" href="#_10-1-overview" aria-hidden="true">#</a> 10.1. Overview</h3><figure><img src="'+z+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+k+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+P+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_10-2-prerequisite-knowledge" tabindex="-1"><a class="header-anchor" href="#_10-2-prerequisite-knowledge" aria-hidden="true">#</a> 10.2. Prerequisite Knowledge</h3><ul><li>Mainstream LLM Architectures</li><li>Basics of Model Training: How Deep Learning Models are Trained <ul><li>Recommended Quality Educational Resources <ul><li>https://www.deeplearning.ai/</li><li>https://pytorch.org/tutorials/beginner/basics/intro.html</li></ul></li></ul></li></ul><h3 id="_10-3-scaling-experiments" tabindex="-1"><a class="header-anchor" href="#_10-3-scaling-experiments" aria-hidden="true">#</a> 10.3. Scaling Experiments</h3><p>We ran over 4000 scaling experiments on up to 512 GPUs and measured throughput (size of markers) and GPU utilization (color of markers). Note that both are normalized per model size in this visualization.</p><p>We ran over 4100 distributed experiments (over 16k including test runs) with up to 512 GPUs to scan many possible distributed training layouts and model sizes.</p><figure><img src="'+T+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+M+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>',11);function V(W,H){const t=B("ExternalLinkIcon");return S(),A("div",null,[G,D,R(" more "),q,e("p",null,[e("a",L,[i("PyTorch's profiler"),r(t)])]),O,e("p",null,[e("a",I,[i("https://huggingface.co/spaces/nanotron/ultrascale-playbook"),r(t)])]),N])}const Q=U(C,[["render",V],["__file","024_distribution_and_parallelism.html.vue"]]);export{Q as default};
