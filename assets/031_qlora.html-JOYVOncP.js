const e=JSON.parse('{"key":"v-9f51a584","path":"/posts/llm/031_qlora.html","title":"QLoRA Code Implementation and Process Analysis","lang":"en-US","frontmatter":{"icon":"lightbulb","sidebar":false,"date":"2025-04-08T00:00:00.000Z","prev":"./032_sft_trainer_sourcecode_prepare_model","next":"./030_wandb","category":["LLM"],"tag":["QLoRA"],"description":"QLoRA Code Implementation and Process Analysis Background Introduction: QLoRA / Base Model / Dataset QLoRA Code Implementation QLoRA Process Analysis QLoRA Application Value QLoRA Questions and Thoughts QLoRA Details Supplement","head":[["link",{"rel":"alternate","hreflang":"zh-cn","href":"https://liz-in-tech.github.io/blog/zh/posts/llm/031_qlora.html"}],["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/blog/posts/llm/031_qlora.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"QLoRA Code Implementation and Process Analysis"}],["meta",{"property":"og:description","content":"QLoRA Code Implementation and Process Analysis Background Introduction: QLoRA / Base Model / Dataset QLoRA Code Implementation QLoRA Process Analysis QLoRA Application Value QLoRA Questions and Thoughts QLoRA Details Supplement"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:locale:alternate","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-04-10T08:01:20.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:tag","content":"QLoRA"}],["meta",{"property":"article:published_time","content":"2025-04-08T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-04-10T08:01:20.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"QLoRA Code Implementation and Process Analysis\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-04-08T00:00:00.000Z\\",\\"dateModified\\":\\"2025-04-10T08:01:20.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. Background Introduction: QLoRA / Base Model / Dataset","slug":"_1-background-introduction-qlora-base-model-dataset","link":"#_1-background-introduction-qlora-base-model-dataset","children":[]},{"level":2,"title":"2. QLoRA Code Implementation","slug":"_2-qlora-code-implementation","link":"#_2-qlora-code-implementation","children":[{"level":3,"title":"2.1. Load Model","slug":"_2-1-load-model","link":"#_2-1-load-model","children":[]},{"level":3,"title":"2.2. Preparing Dataset","slug":"_2-2-preparing-dataset","link":"#_2-2-preparing-dataset","children":[]},{"level":3,"title":"2.3. Fine-Tuning","slug":"_2-3-fine-tuning","link":"#_2-3-fine-tuning","children":[]},{"level":3,"title":"2.4. Save Trained Model","slug":"_2-4-save-trained-model","link":"#_2-4-save-trained-model","children":[]}]},{"level":2,"title":"3. QLoRA Process Analysis","slug":"_3-qlora-process-analysis","link":"#_3-qlora-process-analysis","children":[{"level":3,"title":"3.1. Comparison of Original Model and Quantized Model","slug":"_3-1-comparison-of-original-model-and-quantized-model","link":"#_3-1-comparison-of-original-model-and-quantized-model","children":[]},{"level":3,"title":"3.2. Dataset Processing Flow","slug":"_3-2-dataset-processing-flow","link":"#_3-2-dataset-processing-flow","children":[]},{"level":3,"title":"3.3. Calculation of Trainable Parameters","slug":"_3-3-calculation-of-trainable-parameters","link":"#_3-3-calculation-of-trainable-parameters","children":[]}]},{"level":2,"title":"4. QLoRA Application Value","slug":"_4-qlora-application-value","link":"#_4-qlora-application-value","children":[{"level":3,"title":"4.1. Comparison of QLoRA and Full Parameter Fine-Tuning Methods","slug":"_4-1-comparison-of-qlora-and-full-parameter-fine-tuning-methods","link":"#_4-1-comparison-of-qlora-and-full-parameter-fine-tuning-methods","children":[]},{"level":3,"title":"4.2. Advantages and Potential Limitations of QLoRA in This Task","slug":"_4-2-advantages-and-potential-limitations-of-qlora-in-this-task","link":"#_4-2-advantages-and-potential-limitations-of-qlora-in-this-task","children":[]}]},{"level":2,"title":"5. QLoRA Questions and Thoughts","slug":"_5-qlora-questions-and-thoughts","link":"#_5-qlora-questions-and-thoughts","children":[{"level":3,"title":"5.1. Is the only difference between LoRA and QLoRA fine-tuning the quantization of the model before training?","slug":"_5-1-is-the-only-difference-between-lora-and-qlora-fine-tuning-the-quantization-of-the-model-before-training","link":"#_5-1-is-the-only-difference-between-lora-and-qlora-fine-tuning-the-quantization-of-the-model-before-training","children":[]},{"level":3,"title":"5.2. Why is quantization usually only applied to linear layers?","slug":"_5-2-why-is-quantization-usually-only-applied-to-linear-layers","link":"#_5-2-why-is-quantization-usually-only-applied-to-linear-layers","children":[]},{"level":3,"title":"5.3. Why is the lm_head layer not quantized, even though it is also a linear layer?","slug":"_5-3-why-is-the-lm-head-layer-not-quantized-even-though-it-is-also-a-linear-layer","link":"#_5-3-why-is-the-lm-head-layer-not-quantized-even-though-it-is-also-a-linear-layer","children":[]},{"level":3,"title":"5.4. Why do non-linear layers change dtype to float16 after quantization?","slug":"_5-4-why-do-non-linear-layers-change-dtype-to-float16-after-quantization","link":"#_5-4-why-do-non-linear-layers-change-dtype-to-float16-after-quantization","children":[]},{"level":3,"title":"5.5. What changes occurred in the precision of the quantized model?","slug":"_5-5-what-changes-occurred-in-the-precision-of-the-quantized-model","link":"#_5-5-what-changes-occurred-in-the-precision-of-the-quantized-model","children":[]},{"level":3,"title":"5.6. What is bnb_4bit_compute_dtype=torch.bfloat16 for, and how do computation precision and storage precision differ?","slug":"_5-6-what-is-bnb-4bit-compute-dtype-torch-bfloat16-for-and-how-do-computation-precision-and-storage-precision-differ","link":"#_5-6-what-is-bnb-4bit-compute-dtype-torch-bfloat16-for-and-how-do-computation-precision-and-storage-precision-differ","children":[]},{"level":3,"title":"5.7. Temporary Quantization vs. Permanent Storage?","slug":"_5-7-temporary-quantization-vs-permanent-storage","link":"#_5-7-temporary-quantization-vs-permanent-storage","children":[]},{"level":3,"title":"5.8. What precision does QLoRA use to store the trained model?","slug":"_5-8-what-precision-does-qlora-use-to-store-the-trained-model","link":"#_5-8-what-precision-does-qlora-use-to-store-the-trained-model","children":[]}]},{"level":2,"title":"6. QLoRA Details Supplement","slug":"_6-qlora-details-supplement","link":"#_6-qlora-details-supplement","children":[{"level":3,"title":"6.1. device_map","slug":"_6-1-device-map","link":"#_6-1-device-map","children":[]},{"level":3,"title":"6.2. seed=42 is a common \\"magic number\\"","slug":"_6-2-seed-42-is-a-common-magic-number","link":"#_6-2-seed-42-is-a-common-magic-number","children":[]}]},{"level":2,"title":"7. Reference","slug":"_7-reference","link":"#_7-reference","children":[]}],"git":{"createdTime":1744272080000,"updatedTime":1744272080000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro-4.local","commits":1}]},"readingTime":{"minutes":11.35,"words":3406},"filePathRelative":"posts/llm/031_qlora.md","localizedDate":"April 8, 2025","excerpt":"<h1> QLoRA Code Implementation and Process Analysis</h1>\\n<ul>\\n<li>Background Introduction: QLoRA / Base Model / Dataset</li>\\n<li>QLoRA Code Implementation</li>\\n<li>QLoRA Process Analysis</li>\\n<li>QLoRA Application Value</li>\\n<li>QLoRA Questions and Thoughts</li>\\n<li>QLoRA Details Supplement</li>\\n</ul>\\n","autoDesc":true}');export{e as data};
